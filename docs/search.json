[
  {
    "objectID": "misc/links.html",
    "href": "misc/links.html",
    "title": "Useful links",
    "section": "",
    "text": "Category\nLinks\n\n\n\n\nPersonal website\n🔗 GitHub site\n\n\nCoffee chat (with me)\n🔗 Appointment link\n\n\nComputing and replication resources\n🔗 Web-scraping 1  🔗 Web-scraping 2  🔗 Parallel programming  🔗 Docker  🔗 Google compute engine 1  🔗 Google compute engine 2  🔗 Databases  🔗 Spark  🔗 DuckDB and polars  🔗 NYU Course Notes & Resources  🔗 Quantitative dynamic model\n\n\nJulia\n🔗 Julia for data science  🔗 Computational economics for PhDs  🔗 Computational Methods in Macroeconomics  🔗 BLPDemand.jl  🔗 Guide to Efficient Computational Work in Economics  🔗 Econometrics with Julia  🔗 Advanced Dynamic Programming\n\n\nUseful packages\n🔗 New DiD methods\n\n\nGraduate trade and spatial\n🔗 Treb Allen",
    "crumbs": [
      "Misc.",
      "Useful links"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "tools",
    "section": "",
    "text": "This site is an archive that contains my tools.\n\nWho am I?\nI am a Ph.D student in Applied Economics at The Wharton School: [Website]"
  },
  {
    "objectID": "teaching/teaching.html",
    "href": "teaching/teaching.html",
    "title": "Class name",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the semester. Note that this schedule will be updated as the semester progresses and the timeline of topics and assignments might be updated throughout the semester. Syllabus\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nweek\ndow\ndate\nwhat\ntopic\nprepare\nslides\nhw\nhw_sol\nexam\nnotes\n\n\n\n\n1\nM\nAug 29\nLab\nNo lab\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nT\nAug 30\nLec 1\nWelcome to STA 199\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nSep 1\nLec 2\nMeet the toolkit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2\nM\nSep 5\nLab 0\nHello R!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelease: Lab 0\n\n\n\nT\nSep 6\nLec 3\nGrammar of graphics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nSep 8\nLec 4\nVisualizing various types of data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelease: HW 1\n\n\n\nF\nSep 9\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: Lab 0 + AE 1\n\n\n\nSu\nSep 11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: AE 2\n\n\n3\nM\nSep 12\nLab 1\nData visualization\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelease: Lab 1\n\n\n\nT\nSep 13\nLec 5\nGrammar of data wrangling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nSep 15\nLec 6\nWorking with multiple data frames\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: HW 1 / Release: HW 2\n\n\n\nF\nSep 16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: Lab 1\n\n\n4\nM\nSep 19\nLab 2\nData wrangling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelease: Lab 2\n\n\n\nT\nSep 20\nLec 7\nTidying data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nSep 22\nLec 8\nData types and classes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue HW 2\n\n\n\nF\nSep 23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: Lab 2\n\n\n5\nM\nSep 26\nLab 3\nData tidying\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelease: Lab 3\n\n\n\nT\nSep 27\nLec 9\nImporting and recoding data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nSep 29\nLec 10\nExam 1 Review\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelease: Exam 1 at 12pm\n\n\n\nF\nSep 30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: Lab 3\n\n\n6\nM\nOct 3\nLab\nNo lab - Work on Exam 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: Exam 1 at 2pm\n\n\n\nT\nOct 4\nLec 11\nData science ethics - Misrepresentation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nOct 6\nLec 12\nData science ethics - Algorithmic bias + data privacy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelease: HW 3\n\n\n7\nM\nOct 10\nLab\nNo lab - Fall break\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nT\nOct 11\nLecture\nNo Lec - Fall break\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nOct 13\nLec 13\nWeb scraping\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: HW 3\n\n\n8\nM\nOct 17\nLab\nWork on project proposal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nT\nOct 18\nLec 14\nFunctions + iteration\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nOct 20\nLec 15\nThe language of models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nF\nOct 21\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: Project proposal\n\n\n9\nM\nOct 24\nLab 4\nProbability + Simpson's Paradox\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelease: Lab 4\n\n\n\nT\nOct 25\nLec 16\nModels with a single predictor\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nOct 27\nLec 17\nModels with multiple predictors\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelease: HW 4\n\n\n\nF\nOct 28\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: Lab 4\n\n\n10\nM\nOct 31\nLab 5\nPredicting a numerical outcome\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelease: Lab 5\n\n\n\nT\nNov 1\nLec 18\nModels with multiple predictors + Overfitting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nNov 3\nLec 19\nLogistic regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: HW 4 / Release: HW 5\n\n\n\nF\nNov 4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: Lab 5 / Release: HW 6\n\n\n\nS\nNov 5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: Team peer evaluations 1\n\n\n11\nM\nNov 7\nLab\nWork on project draft\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nT\nNov 8\nLec 20\nQuantifying uncertainty with bootstrap intervals\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nNov 10\nLec 21\nHypothesis testing via simulation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: HW 5\n\n\n\nF\nNov 11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: Project draft 1\n\n\n12\nM\nNov 14\nLab 6\nPrediction + Bootstrapping\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelease: Lab 6\n\n\n\nT\nNov 15\nLec 22\nInference overview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nNov 17\nLec 23\nExam 2 Review\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelease: Exam 2 at 12pm\n\n\n\nF\nNov 18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: Lab 6\n\n\n13\nM\nNov 21\nLab\nNo lab - Work on Exam 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: Exam 2 at 2pm\n\n\n\nT\nNov 22\nLec 24\nNo lecture - Work on projects\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nNov 24\nLecture\nNo lecture - Thanksgiving\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nF\nNov 25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelease: Exam retake (optional)\n\n\n\nSu\nNov 27\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: Project draft 2 (optional)\n\n\n14\nM\nNov 28\nLab\nWork on project peer review\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: Project peer review\n\n\n\nT\nNov 29\nLec 25\nCommunicating data science results effectively\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: Team peer evaluations 2\n\n\n\nTh\nDec 1\nLec 26\nCustomizing Quarto reports and presentations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n15\nM\nDec 5\nLab\nProject presentations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nT\nDec 6\nLec 25\nLooking further: Text analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nDec 8\nLec 26\nLooking further: Interactive web applications with Shiny\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: Project everything\n\n\n\nF\nDec 9\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: HW 6 / Statistics experience\n\n\n16\nTh\nDec 15\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: Exam retake (optional)",
    "crumbs": [
      "Teaching",
      "Class-name",
      "Overview"
    ]
  },
  {
    "objectID": "rep/report.html",
    "href": "rep/report.html",
    "title": "Report",
    "section": "",
    "text": "This is my quarto template for reports and lecture notes.",
    "crumbs": [
      "Replication template",
      "report"
    ]
  },
  {
    "objectID": "rep/makefile.html",
    "href": "rep/makefile.html",
    "title": "Makefile",
    "section": "",
    "text": "This is my Makefile template.\n#| eval: false\n\n# Makefile for analysis report\n#\n\n## Directory vars (usually only these need changing)\nrawdir = data/raw/\ndatdir = data/\nrdir = R/\njuliadir = julia/\nstandir = stan/\nresdir = results/\nfigsdir = figs/\npapdir = paper/\nperfdir = performance/\n\n## See note about new grouped targets method, i.e. replacing \":\" with \"&:\"\n## https://stackoverflow.com/a/59877127/4115816\n\n## Headline build\nall: data stan main recursive evidence sensitivity paper\n\ndata: $(datdir)climate.csv $(datdir)priors.csv $(datdir)df18.fst\nstan: $(standir)mod-pred.stan $(standir)mod.stan $(standir)mod-anthro.stan \\\n $(standir)mod-me.stan\nmain: $(resdir)main/tcr.fst $(resdir)main/gmst2100.fst \\\n $(resdir)main/gmst-pred.csv $(resdir)main/params.csv $(resdir)main/gmst-sim.csv\nrecursive: $(resdir)recursive/tcr-rec.csv\nevidence: $(resdir)evidence/evid.csv\nsensitivity: $(resdir)sensitivity/params-alt-gmst.csv $(resdir)sensitivity/tcr-alt-gmst.fst \\\n $(resdir)sensitivity/params-me-gmst.csv $(resdir)sensitivity/tcr-me-gmst.fst \\\n $(resdir)sensitivity/tcr-me-forcings.fst \\\n $(resdir)sensitivity/tcr-eff1.fst $(resdir)sensitivity/tcr-eff2.fst \\\n $(resdir)sensitivity/params-anthro.csv $(resdir)sensitivity/tcr-anthro.fst\nscc: $(resdir)scc/scc.csv\npaper: $(papdir)sceptic/sceptic.pdf $(papdir)SM/sceptic-SM.pdf\n \nclean:\n    rm -f $(rawdir)* $(datdir)* $(resdir)* $(figsdir)* $(papdir)sceptic/* $(papdir)SM/* $(perfdir)*\n    \n    \n    \nfile_abbc: file_ab file_bc\n    copy file_ab+file_bc file_abbc\n\nfile_ab file_bc &: file_a file_b file_c\n    copy file_a+file_b file_ab\n    copy file_b+file_c file_bc\n\nfile_a file_b file_c &: content\n    copy content file_a\n    copy content file_b\n    copy content file_c\n\n## Draw the Makefile DAG\n## Requires: https://github.com/lindenb/makefile2graph\ndag: makefile-dag.png\nmakefile-dag.png: Makefile\n    make -Bnd all | make2graph | dot -Tpng -Gdpi=300 -o makefile-dag.png\n\n## Raw Data\nraw: $(rdir)00-data-raw.R\n    Rscript $&lt;\n    rm Rplots.pdf\n\n## Prep Data\n$(datdir)climate.csv: $(rdir)01-data-prep.R $(rawdir)*\n    Rscript $&lt;\n    rm Rplots.pdf\n\n$(datdir)priors.csv: $(rdir)01-data-prep.R $(datdir)climate.csv\n    Rscript $&lt;\n    rm Rplots.pdf\n\n$(datdir)df18.fst: $(rdir)01-data-prep.R $(rawdir)df18.idlsave\n    Rscript $&lt;\n    rm Rplots.pdf\n\n## Results\n\n### Main results\n#results_main = $(resdir)main/tcr.fst $(resdir)main/gmst2100.fst \\\n# $(resdir)main/gmst-pred.csv $(resdir)main/params.csv $(resdir)main/gmst-sim.csv\n#$(results_main) &: $(rdir)02-main.R $(standir)mod-pred.stan $(datdir)climate.csv\n#   Rscript $&lt;\n#   \n$(resdir)main/tcr.fst &: $(rdir)02-main.R $(standir)mod-pred.stan $(datdir)climate.csv\n    Rscript $&lt;\n$(resdir)main/gmst2100.fst &: $(rdir)02-main.R $(standir)mod-pred.stan $(datdir)climate.csv\n    Rscript $&lt;\n$(resdir)main/gmst-pred.csv &: $(rdir)02-main.R $(standir)mod-pred.stan $(datdir)climate.csv\n    Rscript $&lt;\n$(resdir)main/params.csv &: $(rdir)02-main.R $(standir)mod-pred.stan $(datdir)climate.csv\n    Rscript $&lt;\n$(resdir)main/gmst-sim.csv &: $(rdir)02-main.R $(standir)mod-pred.stan $(datdir)climate.csv\n    Rscript $&lt;\n\n### Recursive results\nresults_recursive = $(resdir)recursive/tcr-rec.csv\n$(results_recursive) &: $(rdir)03-recursive.R $(standir)mod.stan $(datdir)climate.csv\n    Rscript $&lt;\n    \n### Evidence results\nresults_evidence = $(resdir)evidence/evid.csv\n$(results_evidence) &: $(rdir)04-evidence.R $(standir)mod.stan $(datdir)climate.csv \\\n $(resdir)main/gmst-sim.csv\n    Rscript $&lt;\n\n### Sensitivity analysis results\n\n#### a) Alt GMST series\n#results_gmst_alt = $(resdir)sensitivity/params-alt-gmst.csv $(resdir)sensitivity/tcr-alt-gmst.fst\n#$(results_gmst_alt) &: $(rdir)05-sensitivity-alt-gmst.R $(standir)mod.stan $(datdir)climate.csv\n#   Rscript $&lt;\n$(resdir)sensitivity/params-alt-gmst.csv &: $(rdir)05-sensitivity-alt-gmst.R \\\n $(standir)mod.stan $(datdir)climate.csv\n    Rscript $&lt;\n$(resdir)sensitivity/tcr-alt-gmst.fst &: $(rdir)05-sensitivity-alt-gmst.R \\\n $(standir)mod.stan $(datdir)climate.csv\n    Rscript $&lt;\n\n#### b) Measurement error in GMST\n#results_me_gmst = $(resdir)sensitivity/params-me-gmst.csv $(resdir)sensitivity/tcr-me-gmst.fst\n#$(results_me_gmst) &: $(rdir)05-sensitivity-me-gmst.R $(standir)mod-me.stan $(datdir)climate.csv\n#   Rscript $&lt;\n$(resdir)sensitivity/params-me-gmst.csv &: $(rdir)05-sensitivity-me-gmst.R \\\n $(standir)mod-me.stan $(datdir)climate.csv\n    Rscript $&lt;\n$(resdir)sensitivity/tcr-me-gmst.fst &: $(rdir)05-sensitivity-me-gmst.R \\\n $(standir)mod-me.stan $(datdir)climate.csv\n    Rscript $&lt;\n\n#### c) Measurement error in forcings\nresults_me_gmst = $(resdir)sensitivity/tcr-me-forcings.fst\n$(results_me_gmst) &: $(rdir)05-sensitivity-me-forcings.R $(standir)mod.stan $(datdir)climate.csv \\\n $(datdir)df18.fst\n    Rscript $&lt;\n\n#### d) Adjust forcing efficacies (Marvel et. al, 2016)\n#results_eff = $(resdir)sensitivity/tcr-eff1.fst $(resdir)sensitivity/tcr-eff2.fst\n#$(results_eff) &: $(rdir)05-sensitivity-eff.R $(standir)mod.stan $(datdir)climate.csv \\\n# $(rawdir)rcps.csv\n#   Rscript $&lt;\n$(resdir)sensitivity/tcr-eff1.fst &: $(rdir)05-sensitivity-eff.R \\\n $(standir)mod.stan $(datdir)climate.csv $(rawdir)rcps.csv\n    Rscript $&lt;\n$(resdir)sensitivity/tcr-eff2.fst &: $(rdir)05-sensitivity-eff.R \\\n $(standir)mod.stan $(datdir)climate.csv $(rawdir)rcps.csv\n    Rscript $&lt;\n\n#### e) Separate out anthropogenic forcings\n#results_anthro = $(resdir)sensitivity/params-anthro.csv $(resdir)sensitivity/tcr-anthro.fst\n#$(results_anthro) &: $(rdir)05-sensitivity-anthro.R $(standir)mod-anthro.stan $(datdir)climate.csv\n#   Rscript $&lt;\n$(resdir)sensitivity/params-anthro.csv &: $(rdir)05-sensitivity-anthro.R \\\n $(standir)mod-anthro.stan $(datdir)climate.csv\n    Rscript $&lt;\n$(resdir)sensitivity/tcr-anthro.fst &: $(rdir)05-sensitivity-anthro.R \\\n $(standir)mod-anthro.stan $(datdir)climate.csv\n    Rscript $&lt;\n\n## Social cost of carbon\n$(resdir)scc/scc.csv &: $(juliadir)scc.jl $(resdir)main/tcr.fst\n    julia $&lt;\n\n## Paper\n$(papdir)sceptic/sceptic.pdf &: $(papdir)sceptic/sceptic.Rmd \\\n $(datdir)climate.csv $(datdir)priors.csv \\\n $(resdir)main/params.csv $(resdir)main/tcr.fst \\\n $(resdir)main/gmst-pred.csv $(resdir)main/gmst2100.fst \\\n $(resdir)recursive/tcr-rec.csv $(resdir)evidence/evid.csv \\\n $(resdir)scc/scc.csv\n    Rscript -e 'rmarkdown::render(\"$&lt;\")'\n$(papdir)SM/sceptic-SM.pdf: $(papdir)SM/sceptic-SM.Rmd $(resdir)scc/scc.csv\n    Rscript -e 'rmarkdown::render(\"$&lt;\")'\n\n## Helpers\n.PHONY: all clean dag data stan main recursive evidence sensitivity scc paper\n.DELETE_ON_ERROR:\n.SECONDARY:",
    "crumbs": [
      "Replication template",
      "makefile"
    ]
  },
  {
    "objectID": "rep/docker.html",
    "href": "rep/docker.html",
    "title": "Docker",
    "section": "",
    "text": "This is my Dockerfile template.\n#| eval: false\n\nFROM rocker/r-ver:4.4.0\n\nLABEL maintainer=\"Hyoungchul Kim &lt;hchul.kim96@gmail.com&gt;\"\n\n## Update and install system dependencies\nRUN apt-get update && apt-get install -y \\\n    libcurl4-openssl-dev \\\n    libssl-dev \\\n    libfontconfig1-dev \\\n    libharfbuzz-dev \\\n    libfribidi-dev \\\n    libfreetype6-dev \\\n    libpng-dev \\\n    libtiff5-dev \\\n    libjpeg-dev \\\n    libglpk-dev \\\n    libxml2-dev \\\n    libcairo2-dev \\\n    libgit2-dev \\\n    libpq-dev \\\n    libsasl2-dev \\\n    libsqlite3-dev \\\n    libssh2-1-dev \\\n    libxt-dev \\\n    libgdal-dev\n\n## Install Pandoc\nRUN /rocker_scripts/install_pandoc.sh\n\n## Install Python /reticulate\nRUN /rocker_scripts/install_python.sh\n\n## Install Python packages \nRUN pip3 install numpy\n\n## Install Julia. We'll use Abel Siqueira's handy JILL script to do this.\nRUN wget https://raw.githubusercontent.com/abelsiqueira/jill/master/jill.sh\nRUN bash jill.sh --no-confirm --version 1.5.0\n\n## Go to main project root\nWORKDIR /basic\n\n## Copy renv.lock file into the folder\nCOPY renv.lock .\n\n# Set environment variables for renv\nENV RENV_VERSION 1.0.7\nENV RENV_PATHS_CACHE /renv/cache\nENV RENV_CONFIG_REPOS_OVERRIDE https://cloud.r-project.org\nENV RENV_CONFIG_AUTOLOADER_ENABLED FALSE\nENV RENV_WATCHDOG_ENABLED FALSE\nRUN echo \"options(renv.consent = TRUE)\" &gt;&gt; .Rprofile\nRUN echo \"options(RETICULATE_MINICONDA_ENABLED = FALSE)\" &gt;&gt; .Rprofile\n\n\n# Install renv from CRAN (avoiding bootstrapping by specifying version)\nRUN R -e \"install.packages('renv', repos = c(CRAN = 'https://cloud.r-project.org'))\"\nRUN R -e \"renv::consent(provided = TRUE)\"\n\n# Run renv restore to restore the environment\nRUN R -e \"renv::restore(confirm = FALSE)\"\n\n## Copy over the rest of the data and scripts\nCOPY . .",
    "crumbs": [
      "Replication template",
      "docker"
    ]
  },
  {
    "objectID": "rep/code.html",
    "href": "rep/code.html",
    "title": "Code",
    "section": "",
    "text": "R\n\ndatasummary template\n\n\nlibrary(tidyverse)\nlibrary(modelsummary)\nlibrary(fixest)\n\n\n# define custom statistic\nMinMax &lt;- function(x) paste0('[', min(x, na.rm = TRUE), ', ', max(x, na.rm = TRUE), ']')\n\n\n# Generate LaTeX output\noutput_tex &lt;- datasummary(mpg + cyl ~ Factor(vs) * (mean + sd + MinMax), \n                          data = mtcars, \n                          align = 'lcccccc',\n                          output = \"latex\")\n\ndatasummary(mpg + cyl ~ Factor(vs) * (mean + sd + MinMax), \n            data = mtcars,\n            align = 'lcccccc')\n\n\n# Convert the output to a single character string\noutput_tex &lt;- paste(output_tex, collapse = \"\\n\")\n\n# Remove \\begin{table} and \\end{table}\noutput_tex_cleaned &lt;- str_replace(output_tex, \"(?s)\\\\\\\\begin\\\\{table\\\\}.*?\\\\\\\\begin\\\\{tblr\\\\}\", \"\\\\\\\\begin{tblr}\")\noutput_tex_cleaned &lt;- str_replace(output_tex_cleaned, \"(?s)\\\\\\\\end\\\\{tblr\\\\}.*?\\\\\\\\end\\\\{table\\\\}\", \"\\\\\\\\end{tblr}\")\n\n# Write to a file\nwriteLines(output_tex_cleaned, \"summary_output.tex\")\n\n\netable template\n\n\nlibrary(tidyverse)\nlibrary(modelsummary)\nlibrary(fixest)\n\n\nest1 = feols(Ozone ~ i(Month) / Wind + Temp, data = airquality)\n#&gt; NOTE: 37 observations removed because of NA values (LHS: 37).\nest2 = feols(Ozone ~ i(Month, Wind) + Temp | Month, data = airquality)\n\n# Assume the following dictionary:\ndict = c(\"Month::5\"=\"May\", \"Month::6\"=\"Jun\", \"Month::7\"=\"Jul\",\n         \"Month::8\"=\"Aug\", \"Month::9\"=\"Sep\")\n\n# set my custom latex style\nstyle_lines = style.tex(depvar.title = \"\",\n                        model.title=\"\", \n                        var.title=\"\\\\midrule\",\n                        line.top = \"\\\\toprule\", line.bottom = \"\\\\bottomrule\",\n                        fixef.title=\"\\\\midrule\", fixef.suffix=\" FE\",\n                        stats.title=\"\\\\midrule\",\n                        tablefoot.value=\"\")\n\n# save etable\netable(est1, est2, dict = dict,        \n       style.tex = style_lines,\n       digits = \"r2\", drop = \"Constant\", fitstat = ~ n + r2,\n       signif.code = c(\"***\"=0.01, \"**\"=0.05, \"*\"=0.1),\n       order = \"Temp\",\n       extralines = list(\"_Lorem\" = c(\"lorem1\", \"lorem2\")),\n       tex = TRUE,\n       replace = TRUE,\n       file= \"reg.tex\")\n\n\n\nJulia\n\n\nPython",
    "crumbs": [
      "Replication template",
      "code"
    ]
  },
  {
    "objectID": "teaching/topology.html",
    "href": "teaching/topology.html",
    "title": "Topology",
    "section": "",
    "text": "This pages contains log for my self-study on topology.\n\n\n\n\n\n\n\n\n\nweek\ndate\ntopic\nhw\nhw_sol\nexam\nnotes",
    "crumbs": [
      "Teaching",
      "Topology resources"
    ]
  },
  {
    "objectID": "teaching/metrics.html",
    "href": "teaching/metrics.html",
    "title": "Metric Theory",
    "section": "",
    "text": "This pages contains log for my self-study on metric theory.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nweek\ndate\ntopic\nhw\nhw_sol\nexam\nnotes\n\n\n\n\n1\nAug 29\nNo lab\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 30\nWelcome to STA 199\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 1\nMeet the toolkit\n\n\n\n\n\n\n\n\n\n\n\n\n\n2\nSep 5\nHello R!\n\n\n\n\n\n\n\n\n\nRelease: Lab 0\n\n\n\nSep 6\nGrammar of graphics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 8\nVisualizing various types of data\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelease: HW 1\n\n\n\nSep 9\n\n\n\n\n\n\n\n\n\n\n\nDue: Lab 0 + AE 1\n\n\n\nSep 11\n\n\n\n\n\n\n\n\n\n\n\nDue: AE 2\n\n\n3\nSep 12\nData visualization\n\n\n\n\n\n\n\n\n\nRelease: Lab 1\n\n\n\nSep 13\nGrammar of data wrangling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 15\nWorking with multiple data frames\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: HW 1 / Release: HW 2\n\n\n\nSep 16\n\n\n\n\n\n\n\n\n\n\n\nDue: Lab 1\n\n\n4\nSep 19\nData wrangling\n\n\n\n\n\n\n\n\n\nRelease: Lab 2\n\n\n\nSep 20\nTidying data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 22\nData types and classes\n\n\n\n\n\n\n\n\n\nDue HW 2\n\n\n\nSep 23\n\n\n\n\n\n\n\n\n\n\n\nDue: Lab 2\n\n\n5\nSep 26\nData tidying\n\n\n\n\n\n\n\n\n\nRelease: Lab 3\n\n\n\nSep 27\nImporting and recoding data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 29\nExam 1 Review\n\n\n\n\n\n\n\n\n\n\n\nRelease: Exam 1 at 12pm\n\n\n\nSep 30\n\n\n\n\n\n\n\n\n\n\n\nDue: Lab 3\n\n\n6\nOct 3\nNo lab - Work on Exam 1\n\n\n\n\n\n\n\n\n\nDue: Exam 1 at 2pm\n\n\n\nOct 4\nData science ethics - Misrepresentation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 6\nData science ethics - Algorithmic bias + data privacy\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelease: HW 3\n\n\n7\nOct 10\nNo lab - Fall break\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 11\nNo Lec - Fall break\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 13\nWeb scraping\n\n\n\n\n\n\n\n\n\nDue: HW 3\n\n\n8\nOct 17\nWork on project proposal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 18\nFunctions + iteration\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 20\nThe language of models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 21\n\n\n\n\n\n\n\n\n\n\n\nDue: Project proposal\n\n\n9\nOct 24\nProbability + Simpson's Paradox\n\n\n\n\n\n\n\n\n\nRelease: Lab 4\n\n\n\nOct 25\nModels with a single predictor\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 27\nModels with multiple predictors\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelease: HW 4\n\n\n\nOct 28\n\n\n\n\n\n\n\n\n\n\n\nDue: Lab 4\n\n\n10\nOct 31\nPredicting a numerical outcome\n\n\n\n\n\n\n\n\n\nRelease: Lab 5\n\n\n\nNov 1\nModels with multiple predictors + Overfitting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 3\nLogistic regression\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: HW 4 / Release: HW 5\n\n\n\nNov 4\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: Lab 5 / Release: HW 6\n\n\n\nNov 5\n\n\n\n\n\n\n\n\n\n\n\nDue: Team peer evaluations 1\n\n\n11\nNov 7\nWork on project draft\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 8\nQuantifying uncertainty with bootstrap intervals\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 10\nHypothesis testing via simulation\n\n\n\n\n\n\n\n\n\nDue: HW 5\n\n\n\nNov 11\n\n\n\n\n\n\n\n\n\n\n\nDue: Project draft 1\n\n\n12\nNov 14\nPrediction + Bootstrapping\n\n\n\n\n\n\n\n\n\nRelease: Lab 6\n\n\n\nNov 15\nInference overview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 17\nExam 2 Review\n\n\n\n\n\n\n\n\n\n\n\nRelease: Exam 2 at 12pm\n\n\n\nNov 18\n\n\n\n\n\n\n\n\n\n\n\nDue: Lab 6\n\n\n13\nNov 21\nNo lab - Work on Exam 2\n\n\n\n\n\n\n\n\n\nDue: Exam 2 at 2pm\n\n\n\nNov 22\nNo lecture - Work on projects\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 24\nNo lecture - Thanksgiving\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 25\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelease: Exam retake (optional)\n\n\n\nNov 27\n\n\n\n\n\n\n\n\n\n\n\nDue: Project draft 2 (optional)\n\n\n14\nNov 28\nWork on project peer review\n\n\n\n\n\n\n\n\n\n\n\nDue: Project peer review\n\n\n\nNov 29\nCommunicating data science results effectively\n\n\n\n\n\n\n\n\n\nDue: Team peer evaluations 2\n\n\n\nDec 1\nCustomizing Quarto reports and presentations\n\n\n\n\n\n\n\n\n\n\n\n\n\n15\nDec 5\nProject presentations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDec 6\nLooking further: Text analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDec 8\nLooking further: Interactive web applications with Shiny\n\n\n\n\n\n\n\n\n\nDue: Project everything\n\n\n\nDec 9\n\n\n\n\n\n\n\n\n\n\n\nDue: HW 6 / Statistics experience\n\n\n16\nDec 15\n\n\n\n\n\n\n\n\n\n\n\nDue: Exam retake (optional)",
    "crumbs": [
      "Teaching",
      "Metric theory resources"
    ]
  },
  {
    "objectID": "teaching/prob.html",
    "href": "teaching/prob.html",
    "title": "Probability theory",
    "section": "",
    "text": "This pages contains log for my self-study on probability theory.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nweek\ndate\ntopic\nhw\nhw_sol\nexam\nnotes\n\n\n\n\n1\nAug 29\nNo lab\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 30\nWelcome to STA 199\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 1\nMeet the toolkit\n\n\n\n\n\n\n\n\n\n\n\n\n\n2\nSep 5\nHello R!\n\n\n\n\n\n\n\n\n\nRelease: Lab 0\n\n\n\nSep 6\nGrammar of graphics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 8\nVisualizing various types of data\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelease: HW 1\n\n\n\nSep 9\n\n\n\n\n\n\n\n\n\n\n\nDue: Lab 0 + AE 1\n\n\n\nSep 11\n\n\n\n\n\n\n\n\n\n\n\nDue: AE 2\n\n\n3\nSep 12\nData visualization\n\n\n\n\n\n\n\n\n\nRelease: Lab 1\n\n\n\nSep 13\nGrammar of data wrangling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 15\nWorking with multiple data frames\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: HW 1 / Release: HW 2\n\n\n\nSep 16\n\n\n\n\n\n\n\n\n\n\n\nDue: Lab 1\n\n\n4\nSep 19\nData wrangling\n\n\n\n\n\n\n\n\n\nRelease: Lab 2\n\n\n\nSep 20\nTidying data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 22\nData types and classes\n\n\n\n\n\n\n\n\n\nDue HW 2\n\n\n\nSep 23\n\n\n\n\n\n\n\n\n\n\n\nDue: Lab 2\n\n\n5\nSep 26\nData tidying\n\n\n\n\n\n\n\n\n\nRelease: Lab 3\n\n\n\nSep 27\nImporting and recoding data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 29\nExam 1 Review\n\n\n\n\n\n\n\n\n\n\n\nRelease: Exam 1 at 12pm\n\n\n\nSep 30\n\n\n\n\n\n\n\n\n\n\n\nDue: Lab 3\n\n\n6\nOct 3\nNo lab - Work on Exam 1\n\n\n\n\n\n\n\n\n\nDue: Exam 1 at 2pm\n\n\n\nOct 4\nData science ethics - Misrepresentation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 6\nData science ethics - Algorithmic bias + data privacy\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelease: HW 3\n\n\n7\nOct 10\nNo lab - Fall break\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 11\nNo Lec - Fall break\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 13\nWeb scraping\n\n\n\n\n\n\n\n\n\nDue: HW 3\n\n\n8\nOct 17\nWork on project proposal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 18\nFunctions + iteration\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 20\nThe language of models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 21\n\n\n\n\n\n\n\n\n\n\n\nDue: Project proposal\n\n\n9\nOct 24\nProbability + Simpson's Paradox\n\n\n\n\n\n\n\n\n\nRelease: Lab 4\n\n\n\nOct 25\nModels with a single predictor\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 27\nModels with multiple predictors\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelease: HW 4\n\n\n\nOct 28\n\n\n\n\n\n\n\n\n\n\n\nDue: Lab 4\n\n\n10\nOct 31\nPredicting a numerical outcome\n\n\n\n\n\n\n\n\n\nRelease: Lab 5\n\n\n\nNov 1\nModels with multiple predictors + Overfitting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 3\nLogistic regression\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: HW 4 / Release: HW 5\n\n\n\nNov 4\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: Lab 5 / Release: HW 6\n\n\n\nNov 5\n\n\n\n\n\n\n\n\n\n\n\nDue: Team peer evaluations 1\n\n\n11\nNov 7\nWork on project draft\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 8\nQuantifying uncertainty with bootstrap intervals\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 10\nHypothesis testing via simulation\n\n\n\n\n\n\n\n\n\nDue: HW 5\n\n\n\nNov 11\n\n\n\n\n\n\n\n\n\n\n\nDue: Project draft 1\n\n\n12\nNov 14\nPrediction + Bootstrapping\n\n\n\n\n\n\n\n\n\nRelease: Lab 6\n\n\n\nNov 15\nInference overview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 17\nExam 2 Review\n\n\n\n\n\n\n\n\n\n\n\nRelease: Exam 2 at 12pm\n\n\n\nNov 18\n\n\n\n\n\n\n\n\n\n\n\nDue: Lab 6\n\n\n13\nNov 21\nNo lab - Work on Exam 2\n\n\n\n\n\n\n\n\n\nDue: Exam 2 at 2pm\n\n\n\nNov 22\nNo lecture - Work on projects\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 24\nNo lecture - Thanksgiving\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 25\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelease: Exam retake (optional)\n\n\n\nNov 27\n\n\n\n\n\n\n\n\n\n\n\nDue: Project draft 2 (optional)\n\n\n14\nNov 28\nWork on project peer review\n\n\n\n\n\n\n\n\n\n\n\nDue: Project peer review\n\n\n\nNov 29\nCommunicating data science results effectively\n\n\n\n\n\n\n\n\n\nDue: Team peer evaluations 2\n\n\n\nDec 1\nCustomizing Quarto reports and presentations\n\n\n\n\n\n\n\n\n\n\n\n\n\n15\nDec 5\nProject presentations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDec 6\nLooking further: Text analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDec 8\nLooking further: Interactive web applications with Shiny\n\n\n\n\n\n\n\n\n\nDue: Project everything\n\n\n\nDec 9\n\n\n\n\n\n\n\n\n\n\n\nDue: HW 6 / Statistics experience\n\n\n16\nDec 15\n\n\n\n\n\n\n\n\n\n\n\nDue: Exam retake (optional)",
    "crumbs": [
      "Teaching",
      "Probability theory resources"
    ]
  },
  {
    "objectID": "computing/docker.html",
    "href": "computing/docker.html",
    "title": "Dockerfile",
    "section": "",
    "text": "This is a Dockerfile template for my computing environment. If you want to know more about what this does, check out my blog post.\n\n# Use Rocker image as the base for R\nFROM rocker/r-ver:4.4.0\n\nLABEL maintainer=\"Hyoungchul Kim &lt;hchul.kim96@gmail.com&gt;\"\n\n## Update and install system dependencies\nRUN apt-get update && apt-get install -y \\\n    libcurl4-openssl-dev \\\n    libssl-dev \\\n    libfontconfig1-dev \\\n    libharfbuzz-dev \\\n    libfribidi-dev \\\n    libfreetype6-dev \\\n    libpng-dev \\\n    libtiff5-dev \\\n    libjpeg-dev \\\n    libglpk-dev \\\n    libxml2-dev \\\n    libcairo2-dev \\\n    libgit2-dev \\\n    libpq-dev \\\n    libsasl2-dev \\\n    libsqlite3-dev \\\n    libssh2-1-dev \\\n    libxt-dev \\\n    libgdal-dev \\\n    wget \\\n    curl \\\n    vim \\\n    git \n\n## Install Pandoc and Quarto (Required for RMarkdown, Quarto, etc.)\n# RUN /rocker_scripts/install_pandoc.sh\n# RUN /rocker_scripts/install_quarto.sh\n\n## Install Python & Poetry\nRUN /rocker_scripts/install_python.sh && \\\n    pip3 install --upgrade pip && \\\n    pip3 install poetry\n\n# Ensure Poetry installs dependencies in the system environment\nRUN poetry config virtualenvs.create false\n\n# Copy Poetry files and install dependencies\nCOPY pyproject.toml poetry.lock .\nRUN poetry install --no-interaction --no-root\n\n## Install Julia 1.11.3 (to match Manifest.toml)\nENV JULIA_VERSION=1.11.3\nRUN /rocker_scripts/install_julia.sh\n\n## Set working directory\nWORKDIR /project\n\n## Copy renv.lock file into the folder\nCOPY renv.lock .\n\n# Set environment variables for renv\nENV RENV_VERSION=1.0.7\nENV RENV_PATHS_CACHE=/renv/cache\nENV RENV_CONFIG_REPOS_OVERRIDE=https://cloud.r-project.org\nENV RENV_CONFIG_AUTOLOADER_ENABLED=FALSE\nENV RENV_WATCHDOG_ENABLED=FALSE\nRUN echo \"options(renv.consent = TRUE)\" &gt;&gt; .Rprofile\nRUN echo \"options(RETICULATE_MINICONDA_ENABLED = FALSE)\" &gt;&gt; .Rprofile\n\n# Install renv from CRAN (avoiding bootstrapping by specifying version)\nRUN R -e \"install.packages('renv', repos = c(CRAN = 'https://cloud.r-project.org'))\"\nRUN R -e \"renv::consent(provided = TRUE)\"\n\n# Run renv restore to restore the environment\nRUN R -e \"renv::restore(confirm = FALSE)\"\n\n# Install Julia packages and manage dependencies\nCOPY Manifest.toml Project.toml .\nENV JULIA_PROJECT=/project\nRUN julia -e \"import Pkg; Pkg.activate(\\\".\\\"); Pkg.instantiate()\"\n\n# Copy over the rest of the project files\nCOPY . .\n\n# Default command\nCMD [\"bash\"]",
    "crumbs": [
      "Computing",
      "Dockerfile"
    ]
  },
  {
    "objectID": "computing/computing.html",
    "href": "computing/computing.html",
    "title": "Overview",
    "section": "",
    "text": "This section is a set of templates related to computing.",
    "crumbs": [
      "Computing",
      "Overview"
    ]
  },
  {
    "objectID": "computing/duckdb-sql.html",
    "href": "computing/duckdb-sql.html",
    "title": "DuckDB SQL",
    "section": "",
    "text": "This is example templates that use DuckDB with SQL for R and Python. Note that these are short examples. If you want to know more about what they can do, check out this site.\n\nRPython\n\n\n\nlibrary(duckdb)\n\ncon = dbConnect(duckdb(), shutdown = TRUE)\n\n# uncomment and run the next line if you'd like to create a persistent, disk-based database instead. It is good for computation for bigger than RAM data.\n\n# con = dbConnect(duckdb(), dbdir = \"nyc.duck\")\n\n# SELECT\n#   passenger_count,\n#   AVG(tip_amount) AS mean_tip\n# FROM 'nyc-taxi/**/*.parquet'\n# GROUP BY passenger_count\n# ORDER BY passenger_count\n\ntic = Sys.time()\ndat1 = dbGetQuery(\n  con,\n  \"\n  FROM 'nyc-taxi/**/*.parquet'\n  SELECT\n    passenger_count,\n    AVG(tip_amount) AS mean_tip\n  GROUP BY ALL\n  ORDER BY ALL\n  \"\n)\ntoc = Sys.time()\n\ndat1\ntoc - tic\n\ntic = Sys.time()\ndat2 = dbGetQuery(\n  con,\n  \"\n  FROM 'nyc-taxi/**/*.parquet'\n  SELECT\n    month,\n    passenger_count,\n    AVG(tip_amount) AS mean_tip\n  WHERE month &lt;= 3\n  GROUP BY ALL\n  \"\n    )\ntoc = Sys.time()\n\nhead(dat2)\n\ndbDisconnect(con)\n\n\n\n\nimport duckdb\nimport time\n\ncon = duckdb.connect(database=':memory:', read_only=False)\n\n\n# uncomment and run the next line if you'd like to create a persistent, disk-based database instead. It is good for computation for bigger than RAM data.\n\ncon = duckdb.connect(database='nyc.duck', read_only=False)\n\ntic = time.time()\ndat1 = (\n  con.\n  query(\n    '''\n    FROM 'nyc-taxi/**/*.parquet'\n    SELECT\n      passenger_count,\n      AVG(tip_amount) AS mean_tip\n    GROUP BY ALL\n    ORDER BY ALL\n    '''\n    )\n)\ntoc = time.time()\n\ndat1\n\ntic = time.time()\ndat2 = (\n  con.\n  query(\n    '''\n    FROM 'nyc-taxi/**/*.parquet'\n    SELECT\n      month,\n      passenger_count,\n      AVG(tip_amount) AS mean_tip\n    WHERE month &lt;= 3\n    GROUP BY ALL\n    '''\n  )\n)\ntoc = time.time()\n\ndat2\n\ncon.close()",
    "crumbs": [
      "Computing",
      "DuckDB SQL"
    ]
  },
  {
    "objectID": "computing/duckdb-dplyr.html",
    "href": "computing/duckdb-dplyr.html",
    "title": "DuckDB + dplyr (R)",
    "section": "",
    "text": "This is just a direct copy of the resources from Grant McDermott. Thus, I do not have any credit for it. It is solely for the archive purpose.",
    "crumbs": [
      "Computing",
      "DuckDB + dplyr (R)"
    ]
  },
  {
    "objectID": "computing/duckdb-dplyr.html#load-libraries",
    "href": "computing/duckdb-dplyr.html#load-libraries",
    "title": "DuckDB + dplyr (R)",
    "section": "Load libraries",
    "text": "Load libraries\n\nlibrary(duckdb)\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(tidyr, warn.conflicts = FALSE)",
    "crumbs": [
      "Computing",
      "DuckDB + dplyr (R)"
    ]
  },
  {
    "objectID": "computing/duckdb-dplyr.html#create-a-database-connection",
    "href": "computing/duckdb-dplyr.html#create-a-database-connection",
    "title": "DuckDB + dplyr (R)",
    "section": "Create a database connection",
    "text": "Create a database connection\nFor the d(b)plyr workflow, the connection step is very similar to the pure SQL approach. The only difference is that, after instantiating the database connection, we need to register our parquet dataset as a table in our connection via the dplyr::tbl() function. Note that we also assign it to an object (here: nyc) that can be referenced from R.\n\n## Instantiate the in-memory DuckDB connection \ncon = dbConnect(duckdb(), shutdown = TRUE)\n\n## Register our parquet dataset as table in our connection (and that assign it\n## to an object that R understands)\n# nyc = tbl(con, \"nyc-taxi/**/*.parquet\") # works, but safer to use the read_parquet func)\nnyc = tbl(con, \"read_parquet('nyc-taxi/**/*.parquet', hive_partitioning = true)\")",
    "crumbs": [
      "Computing",
      "DuckDB + dplyr (R)"
    ]
  },
  {
    "objectID": "computing/duckdb-dplyr.html#first-example",
    "href": "computing/duckdb-dplyr.html#first-example",
    "title": "DuckDB + dplyr (R)",
    "section": "First example",
    "text": "First example\nThis next command runs instantly because all computation is deferred (i.e., lazy eval). In other words, it is just a query object.\n\nq1 = nyc |&gt;\n  summarize(\n    mean_tip = mean(tip_amount),\n    .by = passenger_count\n  )\n\n\n\n\n\n\n\n.by versus group_by\n\n\n\nIn case you weren’t aware: summarize(..., .by = x) is a shorthand (and non-persistent) version of group_by(x) |&gt; summarize(...). More details here.\n\n\nWe can see what DuckDB’s query tree looks like by asking it to explain the plan\n\nexplain(q1)\n\nSimilarly, to show the SQL translation that will be implemented on the backend, using show_query.\n\nshow_query(q1)\n\nNote that printing the query object actually does enforce some computation. OTOH it’s still just a preview of the data (we haven’t pulled everything into R’s memory).\n\nq1\n\nTo actually pull all of the result data into R, we must call collect() on the query object\n\ntic = Sys.time()\ndat1 = collect(q1)\ntoc = Sys.time()\n\ndat1\ntoc - tic",
    "crumbs": [
      "Computing",
      "DuckDB + dplyr (R)"
    ]
  },
  {
    "objectID": "computing/duckdb-dplyr.html#aggregation",
    "href": "computing/duckdb-dplyr.html#aggregation",
    "title": "DuckDB + dplyr (R)",
    "section": "Aggregation",
    "text": "Aggregation\nHere’s our earlier filtering example with multiple grouping + aggregation variables…\n\nq2 = nyc |&gt;\n  filter(month &lt;= 3) |&gt;\n  summarize(\n    across(c(tip_amount, fare_amount), mean),\n    .by = c(month, passenger_count)\n  )\nq2\n\nAside: note the optimised query includes hash groupings and projection (basically: fancy column subsetting, which is a suprisingly effective strategy in query optimization)\n\nexplain(q2)\n\nAnd our high-dimensional aggregation example. We’ll create a query for this first, since I’ll reuse it shortly again\n\nq3 = nyc |&gt;\n  group_by(passenger_count, trip_distance) |&gt;\n  summarize(\n    across(c(tip_amount, fare_amount), mean),\n  ) \ncollect(q3)",
    "crumbs": [
      "Computing",
      "DuckDB + dplyr (R)"
    ]
  },
  {
    "objectID": "computing/duckdb-dplyr.html#pivot-reshape",
    "href": "computing/duckdb-dplyr.html#pivot-reshape",
    "title": "DuckDB + dplyr (R)",
    "section": "Pivot (reshape)",
    "text": "Pivot (reshape)\n\n# library(tidyr) ## already loaded\n\nq3 |&gt;\n  pivot_longer(tip_amount:fare_amount) |&gt;\n  collect()",
    "crumbs": [
      "Computing",
      "DuckDB + dplyr (R)"
    ]
  },
  {
    "objectID": "computing/duckdb-dplyr.html#joins-merges",
    "href": "computing/duckdb-dplyr.html#joins-merges",
    "title": "DuckDB + dplyr (R)",
    "section": "Joins (merges)",
    "text": "Joins (merges)\n\nmean_tips  = nyc |&gt; summarise(mean_tips = mean(tip_amount), .by = month)\nmean_fares = nyc |&gt; summarise(mean_fares = mean(fare_amount), .by = month)\n\nAgain, these commands complete instantly because all computation has been deferred until absolutely necessary (i.e.,. lazy eval).\n\nleft_join(\n  mean_fares,\n  mean_tips\n  ) |&gt;\n  collect()",
    "crumbs": [
      "Computing",
      "DuckDB + dplyr (R)"
    ]
  },
  {
    "objectID": "computing/duckdb-dplyr.html#windowing",
    "href": "computing/duckdb-dplyr.html#windowing",
    "title": "DuckDB + dplyr (R)",
    "section": "Windowing",
    "text": "Windowing\nIf you recall from the native SQL API, we sampled 1 percent of the data before creating decile bins to reduce the computation burden of sorting the entire table. Unfortunately, this approach doesn’t work as well for the dplyr frontend because the underlying SQL translation uses a generic sampling approach (rather than DuckDB’s optimised USING SAMPLE statement.)",
    "crumbs": [
      "Computing",
      "DuckDB + dplyr (R)"
    ]
  },
  {
    "objectID": "computing/duckdb-dplyr.html#close-connection",
    "href": "computing/duckdb-dplyr.html#close-connection",
    "title": "DuckDB + dplyr (R)",
    "section": "Close connection",
    "text": "Close connection\n\ndbDisconnect(con)",
    "crumbs": [
      "Computing",
      "DuckDB + dplyr (R)"
    ]
  },
  {
    "objectID": "computing/duckdb-dplyr.html#appendix-related-interfaces",
    "href": "computing/duckdb-dplyr.html#appendix-related-interfaces",
    "title": "DuckDB + dplyr (R)",
    "section": "Appendix: Related interfaces",
    "text": "Appendix: Related interfaces\n\narrow+duckdb\n\nlibrary(arrow)\nlibrary(duckdb) ## Already loaded\nlibrary(dplyr)  ## Already loaded\nlibrary(tidyr)  ## Already loaded\n\nWhen going through the arrow intermediary, we don’t need to establish a database with DBI::dbConnect like we did above. Instead, we can create a link (pointers) to the dataset on disk directly via the arrow::open_dataset() convience function. Here I’ll assign it to a new R object called nyc2.\n\nnyc2 = open_dataset(\"nyc-taxi\")\n\n\n\n\n\n\n\nopen_dataset() versus read_parquet()\n\n\n\n(For individual parquet files, we could just read then via arrow::read_parquet(), perhaps efficiently subsetting columns at the same time. But I find the open_dataset is generally what I’m looking for.)\n\n\nNote that printing our nyc2 dataset to the R console will just display the data schema. This is a cheap and convenient way to quickly interrogate the basic structure of your data, including column types, etc.\n\nnyc2\n\nThe key step for this “arrow + duckdb” dplyr workflow is to pass our arrow dataset to DuckDB via the to_duckdb() function.\n\nto_duckdb(nyc2)\n\nNote that this transfer from Arrow to DuckDB is very quick (and memory cheap) because it is a zero copy. We are just passing around pointers instead of actually moving any data. See this blog post for more details, but the high-level take away is that we are benefitting from the tightly integrated architectures of these two libraries.1\nAt this, point all of the regular dplyr workflow logic from above should carry over. Just remember to first pass the arrow dataset via the to_duckdb() funciton. For example, here’s our initial aggregation query again:\n\nnyc2 |&gt;\n  to_duckdb() |&gt; ## &lt;= key step\n  summarise(\n    mean_tip = mean(tip_amount),\n    .by = passenger_count\n  ) |&gt;\n  collect()\n\n\n\n\n\n\n\nArrow’s native acero engine\n\n\n\nSome of you may be used to performing computation with the arrow package without going through DuckDB. What’s happening here is that arrow provides its own computation engine called “acero”. This Arrow-native engine is actually pretty performant… albeit not a fast as DuckDB, nor as feature rich. So I personally recommend always passing to DuckDB if you can. Still, if you’re curious then you can test yourself by re-trying the code chunk, but commenting out the to_duckdb() line. For more details, see here.\n\n\n\n\nduckplyr\nThe new kid on the block is duckplyr (announcement / homepage). Without going into too much depth, the promise of duckplyr is that it can provide a “fully native” dplyr experience that is directly coupled to DuckDB’s query engine. So, for example, it won’t have to rely on DBI’s generic’ SQL translations. Instead, the relevant dplyr “verbs” are being directly translated to DuckDB’s relational API to construct logical query plans. If that’s too much jargon, just know that it should involve less overhead, fewer translation errors, and better optimization. Moreover, a goal of duckplyr is for it to be a drop-in replace for dplyr in general. In other words, you could just swap out library(dplyr) for library(duckplyr) and all of your data wrangling operations will come backed by the power of DuckDB. This includes for working on “regular” R data frames in memory.\nAll of this is exciting and I would urge you stay tuned. Right now, duckplyr is still marked as experimental and has a few rough edges. But the basics are there. For example:\n\nlibrary(duckplyr, warn.conflicts = FALSE)\n\nduckplyr_df_from_parquet(\"nyc-taxi/**/*.parquet\") |&gt;\n  summarise(\n    mean_tip = mean(tip_amount),\n    .by = passenger_count\n  )",
    "crumbs": [
      "Computing",
      "DuckDB + dplyr (R)"
    ]
  },
  {
    "objectID": "computing/duckdb-dplyr.html#footnotes",
    "href": "computing/duckdb-dplyr.html#footnotes",
    "title": "DuckDB + dplyr (R)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“Similar” might be a better description than “integrated”, since DuckdB does not use the Arrow memory model. But they are both columnar-orientated (among other things) and so the end result is pretty seamless integration.↩︎",
    "crumbs": [
      "Computing",
      "DuckDB + dplyr (R)"
    ]
  },
  {
    "objectID": "computing/polars.html",
    "href": "computing/polars.html",
    "title": "Polars from Python and R",
    "section": "",
    "text": "This is example templates that use Polars for R and Python. Note that these are short examples. If you want to know more about what they can do, check out this site.\n\nRPython\n\n\n\nlibrary(polars)\n\nnyc = pl$scan_parquet(\"nyc-taxi/**/*.parquet\", hive_partitioning = TRUE)\nnyc\n\nq1 = (\n    nyc\n    $group_by(\"passenger_count\")\n    $agg(\n        pl$mean(\"tip_amount\")#$alias(\"mean_tip\") ## alias is optional\n    )\n    $sort(\"passenger_count\")\n)\nq1 \n\ntic = Sys.time()\ndat1 = q1$collect()\ntoc = Sys.time()\n\ndat1\n\nq2 = (\n    nyc\n    $filter(pl$col(\"month\") &lt;= 3)\n    $group_by(\"month\", \"passenger_count\")\n    $agg(pl$mean(\"tip_amount\")$alias(\"mean_tip\"))\n    $sort(\"passenger_count\")\n) \n\n# q2              # naive\ncat(q2$explain()) # optimized\n\ntic = Sys.time()\ndat2 = q2$collect()\ntoc = Sys.time()\n\ndat2\n\nq3 = (\n    nyc\n    $group_by(\"passenger_count\", \"trip_distance\")\n    $agg(\n        pl$mean(\"tip_amount\")$alias(\"mean_tip\"),\n        pl$mean(\"fare_amount\")$alias(\"mean_fare\")\n        )\n    $sort(\"passenger_count\", \"trip_distance\")\n)\n\ntic = Sys.time()\ndat3 = q3$collect()\ntoc = Sys.time()\n \ndat3\n\ndat3$unpivot(index = c(\"passenger_count\", \"trip_distance\"))\n\nmean_tips  = nyc$group_by(\"month\")$agg(pl$col(\"tip_amount\")$mean())\nmean_fares = nyc$group_by(\"month\")$agg(pl$col(\"fare_amount\")$mean())\n\n(\n    mean_tips\n    $join(\n        mean_fares,\n        on = \"month\",\n        how = \"left\"  # default is inner join\n    )\n    $collect()\n)\n\n# You can also try tidypolars\n\nlibrary(polars) ## Already loaded\nlibrary(tidypolars)\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(tidyr, warn.conflicts = FALSE)\n\nnyc = scan_parquet_polars(\"nyc-taxi/**/*.parquet\")\n\nnyc |&gt; \n    summarise(mean_tip = mean(tip_amount), .by = passenger_count) |&gt;\n    compute()\n\n# Aside: Use collect() instead of compute() at the end if you would prefer to return a standard R data.frame instead of a Polars DataFrame.\n\n\n\n\nimport polars as pl\nimport time\nimport matplotlib\n\nnyc = pl.scan_parquet(\"nyc-taxi/**/*.parquet\", hive_partitioning=True)\nnyc\n\nq1 = (\n    nyc\n    .group_by([\"passenger_count\"])\n    .agg([\n            pl.mean(\"tip_amount\")#.alias(\"mean_tip\") ## alias is optional\n        ])\n    .sort(\"passenger_count\")\n)\nq1\n\ntic = time.time()\ndat1 = q1.collect()\ntoc = time.time()\n\ndat1\n\nq2 = (\n    nyc\n    .filter(pl.col(\"month\") &lt;= 3)\n    .group_by([\"month\", \"passenger_count\"])\n    .agg([pl.mean(\"tip_amount\").alias(\"mean_tip\")])\n    .sort(\"passenger_count\")\n)\n\n# q2             # naive\nq2.show_graph()  # optimized\n\ntic = time.time()\ndat2 = q2.collect()\ntoc = time.time()\n\ndat2\n\nq3 = (\n    nyc\n    .group_by([\"passenger_count\", \"trip_distance\"])\n    .agg([\n        pl.mean(\"tip_amount\").alias(\"mean_tip\"),\n        pl.mean(\"fare_amount\").alias(\"mean_fare\"),\n        ])\n    .sort([\"passenger_count\", \"trip_distance\"])\n)\n\ntic = time.time()\ndat3 = q3.collect()\ntoc = time.time()\n\ndat3\n\ndat3.unpivot(index = [\"passenger_count\", \"trip_distance\"])\n\nmean_tips  = nyc.group_by(\"month\").agg(pl.col(\"tip_amount\").mean())\nmean_fares = nyc.group_by(\"month\").agg(pl.col(\"fare_amount\").mean())\n\n(\n    mean_tips\n    .join(\n        mean_fares,\n        on = \"month\",\n        how = \"left\" # default is inner join\n    )\n    .collect()\n)",
    "crumbs": [
      "Computing",
      "Polars (R + Python)"
    ]
  },
  {
    "objectID": "computing/blp.html",
    "href": "computing/blp.html",
    "title": "BLP Demystified: From Basics to Brain-Busting Models!",
    "section": "",
    "text": "Install necessary julia packages for later computation:\n# You might need these commented codes to install packages\n# using Pkg\n# Pkg.add([\"DataFrames\", \"CSV\", \"GLM\", \"Statistics\", \"LinearAlgebra\", \"Distributions\", \"NLopt\", \"FixedEffectModels\", \"RegressionTables\"])",
    "crumbs": [
      "Computing",
      "BLP code (Julia)"
    ]
  },
  {
    "objectID": "computing/blp.html#motivation",
    "href": "computing/blp.html#motivation",
    "title": "BLP Demystified: From Basics to Brain-Busting Models!",
    "section": "1.1 Motivation",
    "text": "1.1 Motivation\nWhy do this? Demand estimation is very important in IO literature because measuring market power is important in IO. How do we quantify market power? Usually we use markup as the measure. But it is hard to directly calculate markup because it depends on the cost function of the firm which is not observed. But IO theory shows that we can actually get the markup using demand elasticity. Thus estimating demand is important.",
    "crumbs": [
      "Computing",
      "BLP code (Julia)"
    ]
  },
  {
    "objectID": "computing/blp.html#basic-mcfadden-1973-style-logit-model",
    "href": "computing/blp.html#basic-mcfadden-1973-style-logit-model",
    "title": "BLP Demystified: From Basics to Brain-Busting Models!",
    "section": "1.2 Basic: McFadden (1973) style logit model",
    "text": "1.2 Basic: McFadden (1973) style logit model\n\n1.2.1 Model setup\nWe will first estimate a basic logit model with no unobserved demand shifters and no random coefficents. But let’s just talk bit about the background of this discrete choice model. Note that most of it is from Train (2009).\nEven before McFadden (1974), there has been a long history of the development of the logit model. But McFadden (1974) provides a complete, well-defined econometric model that is consistent with the utility maximization behavior of individuals.\nIndividual’s (\\(i\\)) utility maximizing behavior (indirect utility) can be specified as follows:\n\\[\nu_{ij} = \\underbrace{x_j \\beta + \\alpha p_j}_{\\delta_j} + \\varepsilon_{ij}\n\\]\nwhere mean utility of outside option is normalized to zero. Also, idiosyncratic shock (i.i.d) follows Type 1 Extreme Value distribution (T1EV). We also assume there are \\(0, \\ldots, J\\) products (denote \\(0\\) as the outside option) where one option is outside option. We can think of \\(\\delta_j\\) as the mean utility from the product \\(j\\). This is because in this parameterization, \\(\\delta_j\\) does not depend on \\(i\\).\nNow let’s do some math to derive the logit choice probabilities. One benefit about logit model is that we can get a close-form solution. We are going to compute the probability of individuals choosing product \\(j\\) given \\(p_j\\), and \\(x_j\\).\n\\[\\begin{align}\n  P (u_{ij} \\geq \\forall_{j' \\neq j} u_{ij'} \\mid x_j, p_j) &= P (x_j \\beta + \\alpha p_j + \\varepsilon_{ij} \\geq \\forall_{j' \\neq j} x_{j'}\\beta + \\alpha p_{j'} + \\varepsilon_{ij'} \\mid x_j, p_j) \\\\\n  &= P ( \\varepsilon_{ij'} \\leq \\varepsilon_{ij} + \\delta_j - \\delta_{j'} \\, \\forall j' \\neq j).\n\\end{align}\\]\nIf we assume that \\(\\varepsilon_{ij}\\) is given, we can think of the last term as the cumulative distribution of the T1EV where \\(F(\\varepsilon_{ij}) = e^{-e^{- \\varepsilon_{ij}}}\\). Since we assumed i.i.d., we can express the last term as the product of the individual cumulative distributions (For brevity, we will now denote the conditional logit choice probability as \\(P_{ij}\\)):\n\\[\n  P_{ij} \\mid \\varepsilon_{ij} = \\prod_{j' \\neq j} e^{ - e^{-(\\varepsilon_{ij} + \\delta_j - \\delta_{j'})}}.\n\\]\nSince \\(\\varepsilon_{ij}\\) is not given, we need o integrate it over density of \\(\\varepsilon_{ij}\\):\n\\[\n  P_{ij} = \\int \\left(\\prod_{j' \\neq j} e^{ - e^{-(\\varepsilon_{ij} + \\delta_j - \\delta_{j'})}} \\right) e^{- \\varepsilon_{ij}} e^{-e^{\\varepsilon_{ij}}} d \\varepsilon_{ij}.\n\\]\nNow let’s get this into a closed-form expression:\nAs a result, we can get the closed-form expression:\n\\[\n  P_{ij} = \\frac{e^{\\delta_{ij}}}{\\sum_{j'} e^{\\delta_{ij'}}}\n\\]\nThis could be understood as the predicted share function given the fixed values of the parameters.\nNote that this is a very simple model because we are not assuming any unobserved product demand shifters that could be affected the utility gained from the product. In fact, we are assuming that econometricians can fully observe all the necessary variables that constructs the mean utility. Thus there is not much econometrics involved. You can just get the parameters as follows:\n\nAssuming you have the data on market share, you can use it to match it to \\(P_{ij} \\cdot M\\) where \\(M\\)is the total market size.\nThen since we will get \\(J\\) equations using \\(J\\) market share, we can do simple algebra to get the mean utility \\(\\delta_j\\).\nThen you can do some nonlinear least squares that minimize the sum of the differences between oberved and predicted shares of all products. This will get you the parameters that best fit the data.\n\n\n\n1.2.2 Adding unobserved demand shifters\nWe can add the additional unobserved variables \\(\\xi_j\\) which can be thought of as some demand shifter for product \\(j\\). This allows the model to be more flexible to incorporate the realistic situation where econometrician might not be able to observe some components that might be affecting the utility of getting some product. Thus most of what we did above does not change much. The only problem would be understanding the nature of this unobserved terms with other main parameters of interest. If there is endogeneity, we would need some IV to estimate the parameter. In this section, we will do both cases (OLS, IV).\n\n\n1.2.3 Computation (Following Berry (1994))\nSo how can we retrieve the parameters of interest? Naive way to think about it would be doing some nonlinear least squares where you minimize the sum of differences between predicted share and observed shares of all products. The problem is that this directy way is implausible: You would need to know the \\(\\xi_j\\). Since this is unobservable, it is problematic.\nThis is where Berry (1994) comes in. He introduces this clever two steps estimation process.\nStep 1: Inversion\nNotation: Let \\(\\hat{s}_j (\\delta)\\) be predicted shares and let \\(s_j\\) be observed shares.1\nThen you can use the system of equations from matching actual to predicted shares and invert them to get the mean utility. For this simple case, we can get the following equations:\n\\[\n  \\delta_j = \\log s_j - \\log \\hat{s}_0, \\quad j = 1, \\ldots, J.\n\\]\nSo this inversion gets us the value of the mean utility. Then we have the second step.\nStep 2: IV estimation\nBy definition, we have \\(\\delta_j = x_j \\beta + \\alpha p_j + \\xi_j\\). So we can do the regression to retrieve the parameters. I put IV, but this could be just OLS if you can assume the unobserved term is uncorrelated with the price.\n\n\n1.2.4 Coding (with Julia)\nFinally we will do some coding to get the result we just talked about.\n\nusing FixedEffectModels, DataFrames, CSV, RegressionTables \n\n# read in the data\notc = CSV.read(\"data/otc.csv\", DataFrame)\n\n# Run regressions\nols1 = reg(otc, @formula(ln_mkt_share_diff ~ price + promotion)) \nols2 = reg(otc, @formula(ln_mkt_share_diff ~ price + promotion + fe(product)))\nols3 = reg(otc, @formula(ln_mkt_share_diff ~ price + promotion + fe(product) + fe(store)))\niv1 = reg(otc, @formula(ln_mkt_share_diff ~ (price ~ cost) + promotion))\niv2 = reg(otc, @formula(ln_mkt_share_diff ~ (price ~ cost) + promotion + fe(product)))\n\nregtable(ols1, ols2, ols3, iv1, iv2, order = [\"price\"], drop = [\"(Intercept)\"], regression_statistics = [FStatIV, Nobs, R2],\n  labels = Dict(\n    \"price\" =&gt; \"Price\",\n    \"promotion\" =&gt; \"Promotion\",\n    \"ln_mkt_share_diff\" =&gt; \"Log Mkt share difference\"\n  ))\n## Some R codes that I followed\n\n# m1 &lt;- lm(ln_mkt_share_diff ~ price + promotion , data = otc)\n# m2 &lt;- lm(ln_mkt_share_diff ~ price + promotion + factor(product), data = otc)\n# m3 &lt;- lm(ln_mkt_share_diff ~ price + promotion + factor(product) + factor(store), data = otc)\n# m4 &lt;- ivreg(ln_mkt_share_diff ~ price + promotion | . - price + cost, data = otc)\n# m5 &lt;- ivreg(ln_mkt_share_diff ~ price + promotion + factor(product) | . - price + cost, data = otc)\n# stargazer(m1, m2, m3, m4, m5, \n#           omit = c(\"product\", \"store\"),\n#           type = \"text\")\n\n\n-----------------------------------------------------------------------------\n                                        Log Mkt share difference             \n                          ---------------------------------------------------\n                              (1)        (2)       (3)         (4)        (5)\n-----------------------------------------------------------------------------\nPrice                       0.020   -0.189**   -0.145*    0.069***      0.169\n                          (0.014)    (0.059)   (0.059)     (0.015)    (0.115)\nPromotion                   0.121     0.187*   0.201**       0.149   0.308***\n                          (0.093)    (0.074)   (0.073)     (0.093)    (0.082)\n-----------------------------------------------------------------------------\nproduct Fixed Effects                    Yes       Yes                    Yes\nstore Fixed Effects                                Yes                       \n-----------------------------------------------------------------------------\nEstimator                     OLS        OLS       OLS          IV         IV\n-----------------------------------------------------------------------------\nControls                      Yes                              Yes           \n-----------------------------------------------------------------------------\nFirst-stage F statistic                                  8,147.921    394.113\nN                           1,056      1,056     1,056       1,056      1,056\nR2                          0.003      0.440     0.456      -0.008      0.420\n-----------------------------------------------------------------------------\n\n\n\n\n\n1.2.5 Caveats\nBut we don’t usually use this basic setup in IO. This is because the model is bit too simple to fully capture the reality. One of the well known problem is the Independence of irrelevant alternatives (IIA). Basically what this means is that we don’t get a realistic demand elasticities. If you want to know more about it, google the famouse Red bus, blue bus story.\n\n\n1.2.6 Solutions?\nThere are some ways to alleviate this problem. One of them (which we will not discuss), is using nested logit. Basically we are defining certain group of products where IIA holds within the group but may not hold across the group. So for the case of red bus, blue bus, they would be in a same group.\nAnother way is to do enhance the random utility model into logit model with random coefficients. In essence, this is sort of introducing preference heterogeneity of consumers into the model. This is done by interacting consumer preferences with product characteristics. The nuisance with this case is that now closed-form expression for choice probability is not obtainable. We need to do some numerical computation.",
    "crumbs": [
      "Computing",
      "BLP code (Julia)"
    ]
  },
  {
    "objectID": "computing/blp.html#advanced-blp-random-coefficients-logit-model",
    "href": "computing/blp.html#advanced-blp-random-coefficients-logit-model",
    "title": "BLP Demystified: From Basics to Brain-Busting Models!",
    "section": "1.3 Advanced: Berry, Levinsohn, and Pakes (1995) (Random coefficients logit model)",
    "text": "1.3 Advanced: Berry, Levinsohn, and Pakes (1995) (Random coefficients logit model)\nWe again start with the individual utility function. But now something is added (we will now also explicitly denote markets as \\(t\\)):\n\\[\nu_{ijt} = x_{jt} \\beta_i + \\alpha p_{jt} + \\xi_jt + \\varepsilon_{ijt}\n\\]\nThe difference is that slope coefficients can now vary across individuals \\(i\\). usually, BLP assumes that \\(beta_i \\sim N(\\beta, \\Sigma)\\). Note that we are assuming that the variance is 1. Also, we will denote \\(k\\) as the product characteristics which sums up to \\(K\\).\nNow we can expand the model:\n\\[\n  u_{ijt} = (x_{j1}, \\ldots, x_{jK}) \\cdot  \n\\]",
    "crumbs": [
      "Computing",
      "BLP code (Julia)"
    ]
  },
  {
    "objectID": "computing/blp.html#references",
    "href": "computing/blp.html#references",
    "title": "BLP Demystified: From Basics to Brain-Busting Models!",
    "section": "References",
    "text": "References\n\n\nBerry, Steven. 1994. “Estimating Discrete-Choice Models of Product Differentiation.” The RAND Journal of Economics 25 (2): 242–62. http://www.jstor.org/stable/2555829.\n\n\nBerry, Steven, James Levinsohn, and Ariel Pakes. 1995. “Automobile Prices in Market Equilibrium.” Econometrica 63 (4): 841–90. http://www.jstor.org/stable/2171802.\n\n\nMcFadden, Daniel. 1974. “Conditional Logit Analysis of Qualitative Choice Behavior.” In Fontiers in Econometrics, edited by Paul Zarembka, 105–42. New York: Academic press.\n\n\nTrain, Kenneth E. 2009. Discrete Choice Methods with Simulation. 2nd ed. Cambridge University Press.",
    "crumbs": [
      "Computing",
      "BLP code (Julia)"
    ]
  },
  {
    "objectID": "computing/blp.html#footnotes",
    "href": "computing/blp.html#footnotes",
    "title": "BLP Demystified: From Basics to Brain-Busting Models!",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou might have already noticed, but I kind of use variables without subscript as the vector of the variables. For example, \\(\\delta\\) is just \\((\\delta_1, \\ldots, \\delta_J).\\)↩︎",
    "crumbs": [
      "Computing",
      "BLP code (Julia)"
    ]
  },
  {
    "objectID": "computing/blp.html#basic-mcfadden74-style-logit-model",
    "href": "computing/blp.html#basic-mcfadden74-style-logit-model",
    "title": "BLP Demystified: From Basics to Brain-Busting Models!",
    "section": "1.2 Basic: McFadden (1974) style logit model",
    "text": "1.2 Basic: McFadden (1974) style logit model\n\n1.2.1 Model setup\nWe will first estimate a basic logit model with no unobserved demand shifters and no random coefficents. But let’s just talk bit about the background of this discrete choice model. Note that most of it is from Train (2009).\nEven before McFadden (1974), there has been a long history of the development of the logit model. But McFadden (1974) provides a complete, well-defined econometric model that is consistent with the utility maximization behavior of individuals.\nIndividual’s (\\(i\\)) utility maximizing behavior (indirect utility) can be specified as follows:\n\\[\nu_{ij} = \\underbrace{x_j \\beta + \\alpha p_j}_{\\delta_j} + \\varepsilon_{ij}\n\\]\nwhere mean utility of outside option is normalized to zero. Also, idiosyncratic shock (i.i.d) follows Type 1 Extreme Value distribution (T1EV). We also assume there are \\(0, \\ldots, J\\) products (denote \\(0\\) as the outside option) where one option is outside option. We can think of \\(\\delta_j\\) as the mean utility from the product \\(j\\). This is because in this parameterization, \\(\\delta_j\\) does not depend on \\(i\\).\nNow let’s do some math to derive the logit choice probabilities. One benefit about logit model is that we can get a close-form solution. We are going to compute the probability of individuals choosing product \\(j\\) given \\(p_j\\), and \\(x_j\\).\n\\[\\begin{align}\n  P (u_{ij} \\geq \\forall_{j' \\neq j} u_{ij'} \\mid x_j, p_j) &= P (x_j \\beta + \\alpha p_j + \\varepsilon_{ij} \\geq \\forall_{j' \\neq j} x_{j'}\\beta + \\alpha p_{j'} + \\varepsilon_{ij'} \\mid x_j, p_j) \\\\\n  &= P ( \\varepsilon_{ij'} \\leq \\varepsilon_{ij} + \\delta_j - \\delta_{j'} \\, \\forall j' \\neq j).\n\\end{align}\\]\nIf we assume that \\(\\varepsilon_{ij}\\) is given, we can think of the last term as the cumulative distribution of the T1EV where \\(F(\\varepsilon_{ij}) = e^{-e^{- \\varepsilon_{ij}}}\\). Since we assumed i.i.d., we can express the last term as the product of the individual cumulative distributions (For brevity, we will now denote the conditional logit choice probability as \\(P_{ij}\\)):\n\\[\n  P_{ij} \\mid \\varepsilon_{ij} = \\prod_{j' \\neq j} e^{ - e^{-(\\varepsilon_{ij} + \\delta_j - \\delta_{j'})}}.\n\\]\nSince \\(\\varepsilon_{ij}\\) is not given, we need o integrate it over density of \\(\\varepsilon_{ij}\\):\n\\[\n  P_{ij} = \\int \\left(\\prod_{j' \\neq j} e^{ - e^{-(\\varepsilon_{ij} + \\delta_j - \\delta_{j'})}} \\right) e^{- \\varepsilon_{ij}} e^{-e^{\\varepsilon_{ij}}} d \\varepsilon_{ij}.\n\\]\nNow let’s get this into a closed-form expression:\nAs a result, we can get the closed-form expression:\n\\[\n  P_{ij} = \\frac{e^{\\delta_{ij}}}{\\sum_{j'} e^{\\delta_{ij'}}}\n\\]\nThis could be understood as the predicted share function given the fixed values of the parameters.\nNote that this is a very simple model because we are not assuming any unobserved product demand shifters that could be affected the utility gained from the product. In fact, we are assuming that econometricians can fully observe all the necessary variables that constructs the mean utility. Thus there is not much econometrics involved. You can just get the parameters as follows:\n\nAssuming you have the data on market share, you can use it to match it to \\(P_{ij} \\cdot M\\) where \\(M\\)is the total market size.\nThen since we will get \\(J\\) equations using \\(J\\) market share, we can do simple algebra to get the mean utility \\(\\delta_j\\).\nThen you can do some nonlinear least squares that minimize the sum of the differences between oberved and predicted shares of all products. This will get you the parameters that best fit the data.\n\n\n\n1.2.2 Adding unobserved demand shifters\nWe can add the additional unobserved variables \\(\\xi_j\\) which can be thought of as some demand shifter for product \\(j\\). This allows the model to be more flexible to incorporate the realistic situation where econometrician might not be able to observe some components that might be affecting the utility of getting some product. Thus most of what we did above does not change much. The only problem would be understanding the nature of this unobserved terms with other main parameters of interest. If there is endogeneity, we would need some IV to estimate the parameter. In this section, we will do both cases (OLS, IV).\n\n\n1.2.3 Computation (Following Berry (1994))\nSo how can we retrieve the parameters of interest? Naive way to think about it would be doing some nonlinear least squares where you minimize the sum of differences between predicted share and observed shares of all products. The problem is that this directy way is implausible: You would need to know the \\(\\xi_j\\). Since this is unobservable, it is problematic.\nThis is where Berry (1994) comes in. He introduces this clever two steps estimation process.\nStep 1: Inversion\nNotation: Let \\(\\hat{s}_j (\\delta)\\) be predicted shares and let \\(s_j\\) be observed shares.1\nThen you can use the system of equations from matching actual to predicted shares and invert them to get the mean utility. For this simple case, we can get the following equations:\n\\[\n  \\delta_j = \\log s_j - \\log \\hat{s}_0, \\quad j = 1, \\ldots, J.\n\\]\nSo this inversion gets us the value of the mean utility. Then we have the second step.\nStep 2: IV estimation\nBy definition, we have \\(\\delta_j = x_j \\beta + \\alpha p_j + \\xi_j\\). So we can do the regression to retrieve the parameters. I put IV, but this could be just OLS if you can assume the unobserved term is uncorrelated with the price.\n\n\n1.2.4 Coding (with Julia)\nFinally we will do some coding to get the result we just talked about.\n\nusing FixedEffectModels, DataFrames, CSV, RegressionTables \n\n# read in the data\notc = CSV.read(\"data/otc.csv\", DataFrame)\n\n# Run regressions\nols1 = reg(otc, @formula(ln_mkt_share_diff ~ price + promotion)) \nols2 = reg(otc, @formula(ln_mkt_share_diff ~ price + promotion + fe(product)))\nols3 = reg(otc, @formula(ln_mkt_share_diff ~ price + promotion + fe(product) + fe(store)))\niv1 = reg(otc, @formula(ln_mkt_share_diff ~ (price ~ cost) + promotion))\niv2 = reg(otc, @formula(ln_mkt_share_diff ~ (price ~ cost) + promotion + fe(product)))\n\nregtable(ols1, ols2, ols3, iv1, iv2, order = [\"price\"], drop = [\"(Intercept)\"], regression_statistics = [FStatIV, Nobs, R2],\n  labels = Dict(\n    \"price\" =&gt; \"Price\",\n    \"promotion\" =&gt; \"Promotion\",\n    \"ln_mkt_share_diff\" =&gt; \"Log Mkt share difference\"\n  ))\n## Some R codes that I followed\n\n# m1 &lt;- lm(ln_mkt_share_diff ~ price + promotion , data = otc)\n# m2 &lt;- lm(ln_mkt_share_diff ~ price + promotion + factor(product), data = otc)\n# m3 &lt;- lm(ln_mkt_share_diff ~ price + promotion + factor(product) + factor(store), data = otc)\n# m4 &lt;- ivreg(ln_mkt_share_diff ~ price + promotion | . - price + cost, data = otc)\n# m5 &lt;- ivreg(ln_mkt_share_diff ~ price + promotion + factor(product) | . - price + cost, data = otc)\n# stargazer(m1, m2, m3, m4, m5, \n#           omit = c(\"product\", \"store\"),\n#           type = \"text\")\n\n\n-----------------------------------------------------------------------------\n                                        Log Mkt share difference             \n                          ---------------------------------------------------\n                              (1)        (2)       (3)         (4)        (5)\n-----------------------------------------------------------------------------\nPrice                       0.020   -0.189**   -0.145*    0.069***      0.169\n                          (0.014)    (0.059)   (0.059)     (0.015)    (0.115)\nPromotion                   0.121     0.187*   0.201**       0.149   0.308***\n                          (0.093)    (0.074)   (0.073)     (0.093)    (0.082)\n-----------------------------------------------------------------------------\nproduct Fixed Effects                    Yes       Yes                    Yes\nstore Fixed Effects                                Yes                       \n-----------------------------------------------------------------------------\nEstimator                     OLS        OLS       OLS          IV         IV\n-----------------------------------------------------------------------------\nControls                      Yes                              Yes           \n-----------------------------------------------------------------------------\nFirst-stage F statistic                                  8,147.921    394.113\nN                           1,056      1,056     1,056       1,056      1,056\nR2                          0.003      0.440     0.456      -0.008      0.420\n-----------------------------------------------------------------------------\n\n\n\n\n\n1.2.5 Caveats\nBut we don’t usually use this basic setup in IO. This is because the model is bit too simple to fully capture the reality. One of the well known problem is the Independence of irrelevant alternatives (IIA). Basically what this means is that we don’t get a realistic demand elasticities. If you want to know more about it, google the famouse Red bus, blue bus story.\n\n\n1.2.6 Solutions?\nThere are some ways to alleviate this problem. One of them (which we will not discuss), is using nested logit. Basically we are defining certain group of products where IIA holds within the group but may not hold across the group. So for the case of red bus, blue bus, they would be in a same group.\nAnother way is to do enhance the random utility model into logit model with random coefficients. In essence, this is sort of introducing preference heterogeneity of consumers into the model. This is done by interacting consumer preferences with product characteristics. The nuisance with this case is that now closed-form expression for choice probability is not obtainable. We need to do some numerical computation.",
    "crumbs": [
      "Computing",
      "BLP code (Julia)"
    ]
  }
]