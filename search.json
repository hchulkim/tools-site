[
  {
    "objectID": "teaching/office-hours.html",
    "href": "teaching/office-hours.html",
    "title": "Overview on OH",
    "section": "",
    "text": "TBD",
    "crumbs": [
      "Teaching",
      "Class-name",
      "Office hours"
    ]
  },
  {
    "objectID": "teaching/prob.html",
    "href": "teaching/prob.html",
    "title": "Probability theory",
    "section": "",
    "text": "This pages contains log for my self-study on probability theory.",
    "crumbs": [
      "Teaching",
      "Mathematics",
      "Probability theory resources"
    ]
  },
  {
    "objectID": "teaching/assignments.html",
    "href": "teaching/assignments.html",
    "title": "General logistics on assignments",
    "section": "",
    "text": "TBD",
    "crumbs": [
      "Teaching",
      "Class-name",
      "Assignments"
    ]
  },
  {
    "objectID": "teaching/faq.html",
    "href": "teaching/faq.html",
    "title": "FAQ",
    "section": "",
    "text": "TBD",
    "crumbs": [
      "Teaching",
      "Class-name",
      "FAQ"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "tools",
    "section": "",
    "text": "This site is an archive that contains my tools.\n\nWho am I?\nI am a Ph.D student in Applied Economics at The Wharton School: [Website]"
  },
  {
    "objectID": "prepare/notes-io.html",
    "href": "prepare/notes-io.html",
    "title": "Notes",
    "section": "",
    "text": "Before the modern empirical IO, demand system was usually based on “representative agent models” in a “product space.” This lead to various problems:\nProblms of representative agent models\nSince lot of the demand systemts were estimated from aggregate market level data, it was hard to generalize the analysis of particular market into different settings. This is because of heterogenous agents in different markets. In order to alleviate this, researchers tried methods such as imposing some a priori distribution of consumer characteristics and aggregate it to the market level. The problem of this approach was that this distribution was very ad-hoc and unrealistic.\nSolutions of representative agent problem\nOne solution to this problem was the use of simulation methods. Instead of assuming certain distribution, researchers would draw a vector of consumer characteristics from some observed population data (e.g. CPS) in that market. Then they would determine the choices made by individuals for some given parameters and aggregate the choices to get the predicted demand. After that, they would apply some algorithm to find the parameter values that get these predicted demand match the observed demand.\n\nProblems of product space\n\n“Too many parameters problems”: If we model the demand system in terms of the product space, we would need to estimate large number of parameters. This could easily become impossible with the data we have.\n“New goods problem”: Since we are in product space, we can only think about goods that were already there. We cannot analyze the demand for new goods.\n\nSolutions of product space problem\nOne solution was to transition from product space to characteristic space. Now a product is a bundle of characteristics and individuals’ preferences are defined on those characteristics.\n\nBackground on characteristics space\nIn fact, papers such as Mcfadden (1974, 1981) and others already provided a well-defined econometrics models that is consistent with this setup. But it was not widely used in IO due to two reasons:\n\nLogit model’s tendency to give arise to IIA problem.\nEarly models did not account for unobserved product characteristics.\n\n\nIIA problem\nSolutions to IIA\n\nUnobserved product characteristics problem\nThis is similar to the traditional simultaneous equation problem in demand/supply function. Basically, price is correlated with the error term. Simple solution would be to apply IV using supply shifters. But this is not an easy problem for the demand system as equations are embedded in a complex non-linear functional forms.\nSolutions to unobserved product characteristics problem\nThis was solved by Berry (1994) and BLP (1995). By applying non-linear change of variables or contraction mapping, we can retrieve the equation that is linear in unobservables. Then we can apply the traditional IV estimation to overcome the endogeneity problem.\n\nSimple model\n\\[\nu_{ijt} = U(\\tilde{x}_{jt}, \\xi_{jt}, z_{it}, v_{it}, y_{it} - p_{jt}, \\theta).\n\\]\nUsually we will just drop the index \\(t\\) which stands for market. We asume there are \\(k\\) dimension of product characteristics. In practice, we do not explicitly model the expenditure in other markets. Instead, income is subsumed into either \\(v_i\\) or \\(z_i\\) and utility is modelled as depending explicitly on price.\n\\[\nu_{ij} = U(\\tilde{x}_j, \\xi_j, z_i, v_i, p_j, \\theta).\n\\]\nWe then parameterize the model in linear fashion and let\n\\[\nU_{ij} = \\sum_k x_{jk}\\theta_{ik} + \\xi_j + \\varepsilon_{ij},\n\\]\nwhere \\(\\theta_{ik} = \\overline{\\theta_k} + \\theta_k^{o}` z_i + \\theta_k^{u}` v_i\\). We normalize outside utility as \\(U_{i,0} = 0\\).\nWe can then fully write the model as\n\\[\nU_{ij} = \\overbrace{\\delta_j}^{\\sum_k x_{jk} \\overline{\\theta_k} + \\xi_j} + \\sum_{kr} x_{jk} z_{ir} \\theta_{rk}^o + \\sum_{kl} x_{jk} v_{il} \\theta_{kl}^u + \\varepsilon_{ij}.\n\\]\nSteps in estimation\nStep I\nApproximation to the aggregate shares conditional on a partiuclar value of \\((\\delta, \\theta)\\). McFadden (1974) showed that in logit assumption, we can find the choice probabilities implied by the model analytically condition on the \\(v_i\\).\n\\[\n\\sigma_j(\\theta, \\delta) = \\int \\overbrace{\\frac{\\exp [ \\delta_j + \\sum_{kl} x_{jk} v_{il} \\theta_{kl}^u ]}{1 + \\sum_q \\exp[ \\delta_q + \\sum_{kl} x_{qk} v_{il} \\theta_{kl}^u ]}}^{\\text{by McFadden}} f(v) d(v).\n\\]\nThis integral is intractable. We then use simulation to obtain an approximation of it following Pakes (1986).\n\\[\n\\sigma_j(\\theta, \\delta, P^{ns}) = \\sum_{r=1}^{ns} \\ldots\n\\]\nStep II\nThen contraction mapping by BLP (1995):\n\\[\n\\delta_j^k(\\theta) = \\delta_j^{k-1} (\\theta) + \\log[s_j^n] - \\log[\\sigma_j(\\theta, \\delta^{k-1}, P^{ns})].\n\\]\nThen we can get the unobservables in linear form:\n\\[\n\\xi_j(\\theta, s^n, P^{ns}) = \\delta(\\cdot) - \\sum_k x_{jk} \\overline{\\theta_k}.\n\\]\nStep III\nGood ol’ GMM.\n\\[\nG_{J, n, ns} (\\theta) = \\sum_j \\xi_j(\\theta, s^n, P^{ns}) f_j(w).\n\\]\nAdditional sources of info on demand parameters\n\nAdding equilibrium assumption (e.g. supply equation, pricing equation) could make the estimation more precise.\nAdding Micro data: Sometimes we can get a more precise estimate if we have individual level data on consumer’s characteristics. This is become more relevant as now there are many microdata that is available to researchers.\n\n\n\n\nResearch which uses dynamic games settings are relatively rare and new. This is not because dynamic setting was not considered important to IO folks. It is just that it is very hard to estimate parameters in dynamic setup. But luckily, there has been some recent breakthroughs."
  },
  {
    "objectID": "prepare/notes-io.html#ackerberg-et-al.-2006",
    "href": "prepare/notes-io.html#ackerberg-et-al.-2006",
    "title": "Notes",
    "section": "",
    "text": "Before the modern empirical IO, demand system was usually based on “representative agent models” in a “product space.” This lead to various problems:\nProblms of representative agent models\nSince lot of the demand systemts were estimated from aggregate market level data, it was hard to generalize the analysis of particular market into different settings. This is because of heterogenous agents in different markets. In order to alleviate this, researchers tried methods such as imposing some a priori distribution of consumer characteristics and aggregate it to the market level. The problem of this approach was that this distribution was very ad-hoc and unrealistic.\nSolutions of representative agent problem\nOne solution to this problem was the use of simulation methods. Instead of assuming certain distribution, researchers would draw a vector of consumer characteristics from some observed population data (e.g. CPS) in that market. Then they would determine the choices made by individuals for some given parameters and aggregate the choices to get the predicted demand. After that, they would apply some algorithm to find the parameter values that get these predicted demand match the observed demand.\n\nProblems of product space\n\n“Too many parameters problems”: If we model the demand system in terms of the product space, we would need to estimate large number of parameters. This could easily become impossible with the data we have.\n“New goods problem”: Since we are in product space, we can only think about goods that were already there. We cannot analyze the demand for new goods.\n\nSolutions of product space problem\nOne solution was to transition from product space to characteristic space. Now a product is a bundle of characteristics and individuals’ preferences are defined on those characteristics.\n\nBackground on characteristics space\nIn fact, papers such as Mcfadden (1974, 1981) and others already provided a well-defined econometrics models that is consistent with this setup. But it was not widely used in IO due to two reasons:\n\nLogit model’s tendency to give arise to IIA problem.\nEarly models did not account for unobserved product characteristics.\n\n\nIIA problem\nSolutions to IIA\n\nUnobserved product characteristics problem\nThis is similar to the traditional simultaneous equation problem in demand/supply function. Basically, price is correlated with the error term. Simple solution would be to apply IV using supply shifters. But this is not an easy problem for the demand system as equations are embedded in a complex non-linear functional forms.\nSolutions to unobserved product characteristics problem\nThis was solved by Berry (1994) and BLP (1995). By applying non-linear change of variables or contraction mapping, we can retrieve the equation that is linear in unobservables. Then we can apply the traditional IV estimation to overcome the endogeneity problem.\n\nSimple model\n\\[\nu_{ijt} = U(\\tilde{x}_{jt}, \\xi_{jt}, z_{it}, v_{it}, y_{it} - p_{jt}, \\theta).\n\\]\nUsually we will just drop the index \\(t\\) which stands for market. We asume there are \\(k\\) dimension of product characteristics. In practice, we do not explicitly model the expenditure in other markets. Instead, income is subsumed into either \\(v_i\\) or \\(z_i\\) and utility is modelled as depending explicitly on price.\n\\[\nu_{ij} = U(\\tilde{x}_j, \\xi_j, z_i, v_i, p_j, \\theta).\n\\]\nWe then parameterize the model in linear fashion and let\n\\[\nU_{ij} = \\sum_k x_{jk}\\theta_{ik} + \\xi_j + \\varepsilon_{ij},\n\\]\nwhere \\(\\theta_{ik} = \\overline{\\theta_k} + \\theta_k^{o}` z_i + \\theta_k^{u}` v_i\\). We normalize outside utility as \\(U_{i,0} = 0\\).\nWe can then fully write the model as\n\\[\nU_{ij} = \\overbrace{\\delta_j}^{\\sum_k x_{jk} \\overline{\\theta_k} + \\xi_j} + \\sum_{kr} x_{jk} z_{ir} \\theta_{rk}^o + \\sum_{kl} x_{jk} v_{il} \\theta_{kl}^u + \\varepsilon_{ij}.\n\\]\nSteps in estimation\nStep I\nApproximation to the aggregate shares conditional on a partiuclar value of \\((\\delta, \\theta)\\). McFadden (1974) showed that in logit assumption, we can find the choice probabilities implied by the model analytically condition on the \\(v_i\\).\n\\[\n\\sigma_j(\\theta, \\delta) = \\int \\overbrace{\\frac{\\exp [ \\delta_j + \\sum_{kl} x_{jk} v_{il} \\theta_{kl}^u ]}{1 + \\sum_q \\exp[ \\delta_q + \\sum_{kl} x_{qk} v_{il} \\theta_{kl}^u ]}}^{\\text{by McFadden}} f(v) d(v).\n\\]\nThis integral is intractable. We then use simulation to obtain an approximation of it following Pakes (1986).\n\\[\n\\sigma_j(\\theta, \\delta, P^{ns}) = \\sum_{r=1}^{ns} \\ldots\n\\]\nStep II\nThen contraction mapping by BLP (1995):\n\\[\n\\delta_j^k(\\theta) = \\delta_j^{k-1} (\\theta) + \\log[s_j^n] - \\log[\\sigma_j(\\theta, \\delta^{k-1}, P^{ns})].\n\\]\nThen we can get the unobservables in linear form:\n\\[\n\\xi_j(\\theta, s^n, P^{ns}) = \\delta(\\cdot) - \\sum_k x_{jk} \\overline{\\theta_k}.\n\\]\nStep III\nGood ol’ GMM.\n\\[\nG_{J, n, ns} (\\theta) = \\sum_j \\xi_j(\\theta, s^n, P^{ns}) f_j(w).\n\\]\nAdditional sources of info on demand parameters\n\nAdding equilibrium assumption (e.g. supply equation, pricing equation) could make the estimation more precise.\nAdding Micro data: Sometimes we can get a more precise estimate if we have individual level data on consumer’s characteristics. This is become more relevant as now there are many microdata that is available to researchers.\n\n\n\n\nResearch which uses dynamic games settings are relatively rare and new. This is not because dynamic setting was not considered important to IO folks. It is just that it is very hard to estimate parameters in dynamic setup. But luckily, there has been some recent breakthroughs."
  },
  {
    "objectID": "study/julia.html",
    "href": "study/julia.html",
    "title": "Julia study",
    "section": "",
    "text": "This page contains an outline of the topics, contents, and works I did for Julia study group.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nweek\ndow\ndate\nwhat\ntopic\nlink\nhw\nhw_sol\nnotes\n\n\n\n\n1\nFri\nMay 9\nmeeting\nlorem\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlorem\n\n\n2\nFri\nMay 16\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3\nSat\nMay 10\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4\nSat\nMay 17\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5\nSun\nMay 11\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6\nSun\nMay 18\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7\nMon\nMay 12\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8\nMon\nMay 19\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9\nTue\nMay 13\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10\nTue\nMay 20\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n11\nWed\nMay 14\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n12\nWed\nMay 21\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n13\nThu\nMay 15\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n14\nThu\nMay 22\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n15\nFri\nMay 16\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n16\nFri\nMay 23\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n17\nSat\nMay 17\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n18\nSat\nMay 24\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n19\nSun\nMay 18\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n20\nSun\nMay 25\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n21\nMon\nMay 19\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n22\nMon\nMay 26\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n23\nTue\nMay 20\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n24\nTue\nMay 27\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n25\nWed\nMay 21\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n26\nWed\nMay 28\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n27\nThu\nMay 22\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n28\nThu\nMay 29\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n29\nFri\nMay 23\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n30\nFri\nMay 30\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n31\nSat\nMay 24\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n32\nSat\nMay 31\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n33\nSun\nMay 25\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n34\nSun\nJun 1\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n35\nMon\nMay 26\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n36\nMon\nJun 2\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n37\nTue\nMay 27\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n38\nTue\nJun 3\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n39\nWed\nMay 28\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n40\nWed\nJun 4\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n41\nThu\nMay 29\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n42\nThu\nJun 5\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n43\nFri\nMay 30\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n44\nFri\nJun 6\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n45\nSat\nMay 31\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n46\nSat\nJun 7\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n47\nSun\nJun 1\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n48\nSun\nJun 8\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n49\nMon\nJun 2\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n50\nMon\nJun 9\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n51\nTue\nJun 3\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n52\nTue\nJun 10\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n53\nWed\nJun 4\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n54\nWed\nJun 11\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n55\nThu\nJun 5\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n56\nThu\nJun 12\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n57\nFri\nJun 6\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n58\nFri\nJun 13\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n59\nSat\nJun 7\nmeeting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n60\nSat\nJun 14\nmeeting",
    "crumbs": [
      "Study",
      "Julia"
    ]
  },
  {
    "objectID": "study/spatial.html",
    "href": "study/spatial.html",
    "title": "Spatial study",
    "section": "",
    "text": "This page contains an outline of the topics, contents, and works I did for Spatial economics self-study.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nweek\ndow\ndate\nwhat\ntopic\nlink\nmaterial\nnotes\n\n\n\n\n1\nFri\nMay 9\ndue-date\nintro\n\n\n\n\n\n\n\n\n\n\nintro_phd (holger)\n\n\n\nFri\n\n\n\n\nMTO (holger)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFri\n\n\n\n\nagglomeration externalities\n\n\n\n\n\n\n\n\nslides_ghm_10 (holger)\n\n\n2\nFri\nMay 16\ndue-date\ndiscrete choice and continuous-discrete choice\n\n\n\n\n\n\n\n\n\n\nblp1995; epple and sieg1999; blp2004; sieg, smigh, banzhaf and walsh2004;\n\n\n3\nFri\nMay 23\ndue-date\nstructural io extra\n\n\n\n\n\n\nackerberg, benkard, berry, pakes2007\n\n\n4\nFri\nMay 30\ndue-date\nspatial rdd\n\n\n\n\n\n\n\n\n\n\nslides_srdd (holger)\n\n\n5\nFri\nJun 6\ndue-date\nsorting across cities\n\n\n\n\n\n\n\n\ndiamond2016\n\n\n\nFri\n\n\n\n\nsorting across cities2\n\n\n\n\n\n\n\n\n\n\n\n\n6\nFri\nJun 13\ndue-date\ntrade (ricardian model) - frechet\n\n\n\n\n\n\n\n\n\n\nEK\n\n\n\nFri\n\n\n\n\ntrade (ricardian model) - frechet2\n\n\n\n\n\n\n\n\nEK\n\n\n\nFri\n\n\n\n\ntrade (ricardian model) - frechet3\n\n\n\n\n\n\n\n\nEK\n\n\n7\nFri\nJun 20\ndue-date\nsorting within cities and agglomeration externalities\n\n\n\n\n\n\n\n\nahlfeldt, redding, sturm and wolf2015\n\n\n\nFri\n\n\n\n\nsorting within cities and agglomeration externalities\n\n\n\n\n\n\n\n\n\n\n\n\n8\nFri\nJun 27\ndue-date\neducation\n\n\n\n\n\n\n\n\nepple, romano and sieg2006; fu2014\n\n\n9\nFri\nJul 4\ndue-date\ndynamic discrete choice review\n\n\n\n\n\n\n\n\ncaliendo-china-shock, balboni-vietnam\n\n\n\nFri\n\n\n\n\ndynamic discrete choice review\n\n\n\n\n\n\n\n\n\n\n\n\n10\nFri\nJul 11\ndue-date\nestimation of stationary spatial models\n\n\n\n\n\n\n\n\nalmagro and dominguez2022, amsterdam\n\n\n\nFri\n\n\n\n\nestimation of stationary spatial models\n\n\n\n\n\n\n\n\n\n\n\n\n11\nFri\nJul 18\ndue-date\nstationary housing models: rental markets, public and affordable housing\n\n\n\n\n\n\n\n\n\n\n\n\n12\nFri\nJul 25\ndue-date\nstationary housing models: owner occupied housing markets\n\n\n\n\n\n\n\n\nhurst, keys, seru and vavra2016\n\n\n\nFri\n\n\n\n\nstationary housing models: owner occupied housing markets\n\n\n\n\n\n\n\n\n\n\n\n\n13\nFri\nAug 1\ndue-date\nstatic hedonic models\n\n\n\n\n\n\n\n\nrosen, sherwin1974; epple1987; seig, smith, banzhaf and walsh2002; epple, quintero, and sieg2020\n\n\n14\nFri\nAug 8\ndue-date\nnonstationary dynamic hedonic\n\n\n\n\n\n\n\n\nlandvoigt, piazzesi, and schneider2015\n\n\n15\nFri\nAug 15\ndue-date\nnonstationary spatial models\n\n\n\n\n\n\n\n\nkleinman, liu, redding2023\n\n\n\nFri\n\n\n\n\nnonstationary spatial models\n\n\n\n\n\n\n\n\n\n\n\n\n\nFri\n\n\n\n\nnonstationary spatial models\n\n\n\n\n\n\n\n\ncaliendo, dvorkin, and parro2019; balboni2021; bilal and rossi-hansberg2023\n\n\n\nFri\n\n\n\n\nnonstationary spatial models\n\n\n\n\n\n\n\n\n\n\n\n\n16\nFri\nAug 22\ndue-date\nnonstationary spatial models: firm location and density\n\n\n\n\n\n\n\n\nholmes2011\n\n\n0\nNA\n\n\nNOTES\nsummary",
    "crumbs": [
      "Study",
      "Spatial"
    ]
  },
  {
    "objectID": "computing/terra-raster-data.html",
    "href": "computing/terra-raster-data.html",
    "title": "Geospatial with R",
    "section": "",
    "text": "This is my practice sections following R as GIS for Economists.\n\nBasics\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\n\nLoading required package: pacman\n\npacman::p_load(terra, raster, mapview, dplyr, sf, lubridate, downloader)\n\n\n(\n  IA_cdl_2015 &lt;- raster::raster(\"Data/IA_cdl_2015.tif\")\n)\n\nclass      : RasterLayer \ndimensions : 11671, 17795, 207685445  (nrow, ncol, ncell)\nresolution : 30, 30  (x, y)\nextent     : -52095, 481755, 1938165, 2288295  (xmin, xmax, ymin, ymax)\ncrs        : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nsource     : IA_cdl_2015.tif \nnames      : Layer_1 \nvalues     : 0, 229  (min, max)\n\n\n\nIA_cdl_2016 &lt;- raster::raster(\"Data/IA_CDL_2016.tif\")\n\n#--- stack the two ---#\n(\n  IA_cdl_stack &lt;- raster::stack(IA_cdl_2015, IA_cdl_2016)\n)\n\nclass      : RasterStack \ndimensions : 11671, 17795, 207685445, 2  (nrow, ncol, ncell, nlayers)\nresolution : 30, 30  (x, y)\nextent     : -52095, 481755, 1938165, 2288295  (xmin, xmax, ymin, ymax)\ncrs        : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nnames      : Layer_1, IA_CDL_2016 \nmin values :       0,           0 \nmax values :     229,         241 \n\n\n\n# I am not evaluating this cell because it takes quite some time to execute.\n#--- stack the two ---#\nIA_cdl_brick &lt;- brick(IA_cdl_stack)\n\n#--- or this works as well ---#\n# IA_cdl_brick &lt;- brick(IA_cdl_2015, IA_cdl_2016)\n\n#--- take a look ---#\nIA_cdl_brick\n\n\n#--- convert to a SpatRaster ---#\nIA_cdl_2015_sr &lt;- terra::rast(IA_cdl_2015)\n\n#--- convert to a SpatRaster ---#\nIA_cdl_stack_sr &lt;- terra::rast(IA_cdl_stack)\n\nWarning: [rast] CRS do not match\n\n#--- take a look ---#\nIA_cdl_2015_sr\n\nclass       : SpatRaster \ndimensions  : 11671, 17795, 1  (nrow, ncol, nlyr)\nresolution  : 30, 30  (x, y)\nextent      : -52095, 481755, 1938165, 2288295  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nsource      : IA_cdl_2015.tif \nname        : Layer_1 \nmin value   :       0 \nmax value   :     229 \n\n\n\n# create a single-layer from multiple single-layer\nIA_cdl_2016_sr &lt;- terra::rast(IA_cdl_2016)\n\n# concatenate\n(\n    IA_cdl_ml_sr &lt;- c(IA_cdl_2015_sr, IA_cdl_2016_sr)\n)\n\nWarning: [rast] CRS do not match\n\n\nclass       : SpatRaster \ndimensions  : 11671, 17795, 2  (nrow, ncol, nlyr)\nresolution  : 30, 30  (x, y)\nextent      : -52095, 481755, 1938165, 2288295  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nsources     : IA_cdl_2015.tif  \n              IA_CDL_2016.tif  \ncolor table : 2 \nnames       : Layer_1, IA_CDL_2016 \nmin values  :       0,           0 \nmax values  :     229,         241 \n\n\n\nIA_cdl_stack_sr %&gt;% raster::raster()\n\nclass      : RasterLayer \ndimensions : 11671, 17795, 207685445  (nrow, ncol, ncell)\nresolution : 30, 30  (x, y)\nextent     : -52095, 481755, 1938165, 2288295  (xmin, xmax, ymin, ymax)\ncrs        : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nsource     : IA_cdl_2015.tif \nnames      : Layer_1 \nvalues     : 0, 229  (min, max)\n\n#  %&gt;% raster::stack()\n#  %&gt;% raster::brick() \n\n\n#--- Illinois county boundary ---#\n(\n  IL_county &lt;- \n    tigris::counties(\n      state = \"Illinois\", \n      progress_bar = FALSE\n    ) %&gt;%\n    dplyr::select(STATEFP, COUNTYFP)\n)\n\nRetrieving data for the year 2022\n\n\nSimple feature collection with 102 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -91.51308 ymin: 36.9703 xmax: -87.01994 ymax: 42.50848\nGeodetic CRS:  NAD83\nFirst 10 features:\n    STATEFP COUNTYFP                       geometry\n86       17      067 MULTIPOLYGON (((-90.90609 4...\n92       17      025 MULTIPOLYGON (((-88.69516 3...\n131      17      185 MULTIPOLYGON (((-87.89243 3...\n148      17      113 MULTIPOLYGON (((-88.91954 4...\n158      17      005 MULTIPOLYGON (((-89.37207 3...\n159      17      009 MULTIPOLYGON (((-90.53674 3...\n213      17      083 MULTIPOLYGON (((-90.1459 39...\n254      17      147 MULTIPOLYGON (((-88.46335 4...\n266      17      151 MULTIPOLYGON (((-88.48289 3...\n303      17      011 MULTIPOLYGON (((-89.16654 4...\n\n\n\n(\n    IL_county_sv &lt;- terra::vect(IL_county)\n)\n\n class       : SpatVector \n geometry    : polygons \n dimensions  : 102, 2  (geometries, attributes)\n extent      : -91.51308, -87.01994, 36.9703, 42.50848  (xmin, xmax, ymin, ymax)\n coord. ref. : lon/lat NAD83 (EPSG:4269) \n names       : STATEFP COUNTYFP\n type        :   &lt;chr&gt;    &lt;chr&gt;\n values      :      17      067\n                    17      025\n                    17      185\n\n\n\n(\n    IA_cdl_2015_sr &lt;- terra::rast(\"Data/IA_cdl_2015.tif\")\n)\n\nclass       : SpatRaster \ndimensions  : 11671, 17795, 1  (nrow, ncol, nlyr)\nresolution  : 30, 30  (x, y)\nextent      : -52095, 481755, 1938165, 2288295  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nsource      : IA_cdl_2015.tif \nname        : Layer_1 \nmin value   :       0 \nmax value   :     229 \n\n\n\n#--- the list of path to the files ---#\nfiles_list &lt;- c(\"Data/IA_cdl_2015.tif\", \"Data/IA_CDL_2016.tif\")\n\n#--- read the two at the same time ---#\n(\n  multi_layer_sr &lt;- terra::rast(files_list)\n)\n\nWarning: [rast] CRS do not match\n\n\nclass       : SpatRaster \ndimensions  : 11671, 17795, 2  (nrow, ncol, nlyr)\nresolution  : 30, 30  (x, y)\nextent      : -52095, 481755, 1938165, 2288295  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nsources     : IA_cdl_2015.tif  \n              IA_CDL_2016.tif  \ncolor table : 2 \nnames       : Layer_1, IA_CDL_2016 \nmin values  :       0,           0 \nmax values  :     229,         241 \n\n\n\nterra::crs(IA_cdl_2015_sr)\n\n[1] \"BOUNDCRS[\\n    SOURCECRS[\\n        PROJCRS[\\\"unnamed\\\",\\n            BASEGEOGCRS[\\\"GRS 1980(IUGG, 1980)\\\",\\n                DATUM[\\\"unknown\\\",\\n                    ELLIPSOID[\\\"GRS80\\\",6378137,298.257222101,\\n                        LENGTHUNIT[\\\"metre\\\",1,\\n                            ID[\\\"EPSG\\\",9001]]]],\\n                PRIMEM[\\\"Greenwich\\\",0,\\n                    ANGLEUNIT[\\\"degree\\\",0.0174532925199433,\\n                        ID[\\\"EPSG\\\",9122]]]],\\n            CONVERSION[\\\"Albers Equal Area\\\",\\n                METHOD[\\\"Albers Equal Area\\\",\\n                    ID[\\\"EPSG\\\",9822]],\\n                PARAMETER[\\\"Latitude of false origin\\\",23,\\n                    ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n                    ID[\\\"EPSG\\\",8821]],\\n                PARAMETER[\\\"Longitude of false origin\\\",-96,\\n                    ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n                    ID[\\\"EPSG\\\",8822]],\\n                PARAMETER[\\\"Latitude of 1st standard parallel\\\",29.5,\\n                    ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n                    ID[\\\"EPSG\\\",8823]],\\n                PARAMETER[\\\"Latitude of 2nd standard parallel\\\",45.5,\\n                    ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n                    ID[\\\"EPSG\\\",8824]],\\n                PARAMETER[\\\"Easting at false origin\\\",0,\\n                    LENGTHUNIT[\\\"metre\\\",1],\\n                    ID[\\\"EPSG\\\",8826]],\\n                PARAMETER[\\\"Northing at false origin\\\",0,\\n                    LENGTHUNIT[\\\"metre\\\",1],\\n                    ID[\\\"EPSG\\\",8827]]],\\n            CS[Cartesian,2],\\n                AXIS[\\\"easting\\\",east,\\n                    ORDER[1],\\n                    LENGTHUNIT[\\\"metre\\\",1,\\n                        ID[\\\"EPSG\\\",9001]]],\\n                AXIS[\\\"northing\\\",north,\\n                    ORDER[2],\\n                    LENGTHUNIT[\\\"metre\\\",1,\\n                        ID[\\\"EPSG\\\",9001]]]]],\\n    TARGETCRS[\\n        GEOGCRS[\\\"WGS 84\\\",\\n            DATUM[\\\"World Geodetic System 1984\\\",\\n                ELLIPSOID[\\\"WGS 84\\\",6378137,298.257223563,\\n                    LENGTHUNIT[\\\"metre\\\",1]]],\\n            PRIMEM[\\\"Greenwich\\\",0,\\n                ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n            CS[ellipsoidal,2],\\n                AXIS[\\\"geodetic latitude (Lat)\\\",north,\\n                    ORDER[1],\\n                    ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n                AXIS[\\\"geodetic longitude (Lon)\\\",east,\\n                    ORDER[2],\\n                    ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n            USAGE[\\n                SCOPE[\\\"Horizontal component of 3D system.\\\"],\\n                AREA[\\\"World.\\\"],\\n                BBOX[-90,-180,90,180]],\\n            ID[\\\"EPSG\\\",4326]]],\\n    ABRIDGEDTRANSFORMATION[\\\"Transformation to WGS84\\\",\\n        METHOD[\\\"Position Vector transformation (geog2D domain)\\\",\\n            ID[\\\"EPSG\\\",9606]],\\n        PARAMETER[\\\"X-axis translation\\\",0,\\n            ID[\\\"EPSG\\\",8605]],\\n        PARAMETER[\\\"Y-axis translation\\\",0,\\n            ID[\\\"EPSG\\\",8606]],\\n        PARAMETER[\\\"Z-axis translation\\\",0,\\n            ID[\\\"EPSG\\\",8607]],\\n        PARAMETER[\\\"X-axis rotation\\\",0,\\n            ID[\\\"EPSG\\\",8608]],\\n        PARAMETER[\\\"Y-axis rotation\\\",0,\\n            ID[\\\"EPSG\\\",8609]],\\n        PARAMETER[\\\"Z-axis rotation\\\",0,\\n            ID[\\\"EPSG\\\",8610]],\\n        PARAMETER[\\\"Scale difference\\\",1,\\n            ID[\\\"EPSG\\\",8611]]]]\"\n\n\n\n# index\nIA_cdl_stack_sr[[2]]\n\nclass       : SpatRaster \ndimensions  : 11671, 17795, 1  (nrow, ncol, nlyr)\nresolution  : 30, 30  (x, y)\nextent      : -52095, 481755, 1938165, 2288295  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nsource      : IA_CDL_2016.tif \ncolor table : 1 \nname        : IA_CDL_2016 \nmin value   :           0 \nmax value   :         241 \n\n\n\nvalues_from_rs &lt;- terra::values(IA_cdl_stack_sr)\n\nhead(values_from_rs)\n\n     Layer_1 IA_CDL_2016\n[1,]       0           0\n[2,]       0           0\n[3,]       0           0\n[4,]       0           0\n[5,]       0           0\n[6,]       0           0\n\n\n\nplot(IA_cdl_2015_sr)\n\n\n\n\n\n\n\n\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(terra, tidyterra, raster, exactextractr, sf, dplyr, tidyr, data.table, prism, tictoc, tigris)\n\n\n#--- set the path to the folder to which you save the downloaded PRISM data ---#\n# This code sets the current working directory as the designated folder\noptions(prism.path = \"Data\")\n\n#--- download PRISM precipitation data ---#\nprism::get_prism_dailys(\n  type = \"tmax\",\n  date = \"2018-07-01\",\n  keepZip = FALSE\n)\n\n\n  |                                                                            \n  |                                                                      |   0%\n\n\n\nPRISM_tmax_stable_4kmD2_20180701_bil.zip already exists. Skipping downloading.\n\n\n\n  |                                                                            \n  |======================================================================| 100%\n\n#--- the file name of the PRISM data just downloaded ---#\nprism_file &lt;- \"Data/PRISM_tmax_stable_4kmD2_20180701_bil/PRISM_tmax_stable_4kmD2_20180701_bil.bil\"\n\n#--- read in the prism data ---#\nprism_tmax_0701_sr &lt;- terra::rast(prism_file)\n\n\n#--- Kansas boundary (sf) ---#\nKS_county_sf &lt;-\n  #--- get Kansas county boundary ---\n  tigris::counties(state = \"Kansas\", cb = TRUE) %&gt;%\n  #--- sp to sf ---#\n  sf::st_as_sf() %&gt;%\n  #--- transform using the CRS of the PRISM tmax data  ---#\n  sf::st_transform(terra::crs(prism_tmax_0701_sr))\n\nRetrieving data for the year 2022\n\n\n\nDownloading: 48 kB     \nDownloading: 48 kB     \nDownloading: 65 kB     \nDownloading: 65 kB     \nDownloading: 97 kB     \nDownloading: 97 kB     \nDownloading: 240 kB     \nDownloading: 240 kB     \nDownloading: 410 kB     \nDownloading: 410 kB     \nDownloading: 460 kB     \nDownloading: 460 kB     \nDownloading: 510 kB     \nDownloading: 510 kB     \nDownloading: 880 kB     \nDownloading: 880 kB     \nDownloading: 900 kB     \nDownloading: 900 kB     \nDownloading: 1.1 MB     \nDownloading: 1.1 MB     \nDownloading: 1.3 MB     \nDownloading: 1.3 MB     \nDownloading: 1.5 MB     \nDownloading: 1.5 MB     \nDownloading: 1.7 MB     \nDownloading: 1.7 MB     \nDownloading: 1.8 MB     \nDownloading: 1.8 MB     \nDownloading: 2 MB     \nDownloading: 2 MB     \nDownloading: 2.1 MB     \nDownloading: 2.1 MB     \nDownloading: 2.2 MB     \nDownloading: 2.2 MB     \nDownloading: 2.4 MB     \nDownloading: 2.4 MB     \nDownloading: 2.6 MB     \nDownloading: 2.6 MB     \nDownloading: 2.7 MB     \nDownloading: 2.7 MB     \nDownloading: 2.9 MB     \nDownloading: 2.9 MB     \nDownloading: 2.9 MB     \nDownloading: 2.9 MB     \nDownloading: 3.1 MB     \nDownloading: 3.1 MB     \nDownloading: 3.1 MB     \nDownloading: 3.1 MB     \nDownloading: 3.5 MB     \nDownloading: 3.5 MB     \nDownloading: 3.6 MB     \nDownloading: 3.6 MB     \nDownloading: 3.7 MB     \nDownloading: 3.7 MB     \nDownloading: 3.8 MB     \nDownloading: 3.8 MB     \nDownloading: 4 MB     \nDownloading: 4 MB     \nDownloading: 4.2 MB     \nDownloading: 4.2 MB     \nDownloading: 4.4 MB     \nDownloading: 4.4 MB     \nDownloading: 4.6 MB     \nDownloading: 4.6 MB     \nDownloading: 4.7 MB     \nDownloading: 4.7 MB     \nDownloading: 4.9 MB     \nDownloading: 4.9 MB     \nDownloading: 5 MB     \nDownloading: 5 MB     \nDownloading: 5.1 MB     \nDownloading: 5.1 MB     \nDownloading: 5.2 MB     \nDownloading: 5.2 MB     \nDownloading: 5.4 MB     \nDownloading: 5.4 MB     \nDownloading: 5.6 MB     \nDownloading: 5.6 MB     \nDownloading: 5.7 MB     \nDownloading: 5.7 MB     \nDownloading: 5.8 MB     \nDownloading: 5.8 MB     \nDownloading: 5.9 MB     \nDownloading: 5.9 MB     \nDownloading: 6 MB     \nDownloading: 6 MB     \nDownloading: 6.1 MB     \nDownloading: 6.1 MB     \nDownloading: 6.3 MB     \nDownloading: 6.3 MB     \nDownloading: 6.5 MB     \nDownloading: 6.5 MB     \nDownloading: 6.7 MB     \nDownloading: 6.7 MB     \nDownloading: 6.9 MB     \nDownloading: 6.9 MB     \nDownloading: 6.9 MB     \nDownloading: 6.9 MB     \nDownloading: 7.1 MB     \nDownloading: 7.1 MB     \nDownloading: 7.2 MB     \nDownloading: 7.2 MB     \nDownloading: 7.4 MB     \nDownloading: 7.4 MB     \nDownloading: 7.5 MB     \nDownloading: 7.5 MB     \nDownloading: 7.5 MB     \nDownloading: 7.5 MB     \nDownloading: 7.7 MB     \nDownloading: 7.7 MB     \nDownloading: 7.9 MB     \nDownloading: 7.9 MB     \nDownloading: 8.1 MB     \nDownloading: 8.1 MB     \nDownloading: 8.2 MB     \nDownloading: 8.2 MB     \nDownloading: 8.4 MB     \nDownloading: 8.4 MB     \nDownloading: 8.5 MB     \nDownloading: 8.5 MB     \nDownloading: 8.5 MB     \nDownloading: 8.5 MB     \nDownloading: 8.5 MB     \nDownloading: 8.5 MB     \nDownloading: 8.7 MB     \nDownloading: 8.7 MB     \nDownloading: 8.9 MB     \nDownloading: 8.9 MB     \nDownloading: 9.1 MB     \nDownloading: 9.1 MB     \nDownloading: 9.3 MB     \nDownloading: 9.3 MB     \nDownloading: 9.5 MB     \nDownloading: 9.5 MB     \nDownloading: 9.5 MB     \nDownloading: 9.5 MB     \nDownloading: 9.6 MB     \nDownloading: 9.6 MB     \nDownloading: 9.8 MB     \nDownloading: 9.8 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 10 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 11 MB     \nDownloading: 12 MB     \nDownloading: 12 MB     \nDownloading: 12 MB     \nDownloading: 12 MB     \nDownloading: 12 MB     \nDownloading: 12 MB     \n\n\n\n# terra::(SpatRaster, sf)\n\nprism_tmax_0701_KS_sr &lt;- \n  terra::crop(\n    prism_tmax_0701_sr,\n    KS_county_sf\n  )\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats 1.0.0     ✔ readr   2.1.5\n✔ ggplot2 3.5.1     ✔ stringr 1.5.1\n✔ purrr   1.0.4     ✔ tibble  3.2.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ data.table::between() masks dplyr::between()\n✖ tidyr::extract()      masks raster::extract(), terra::extract()\n✖ tidyterra::filter()   masks dplyr::filter(), stats::filter()\n✖ data.table::first()   masks dplyr::first()\n✖ data.table::hour()    masks lubridate::hour()\n✖ data.table::isoweek() masks lubridate::isoweek()\n✖ dplyr::lag()          masks stats::lag()\n✖ data.table::last()    masks dplyr::last()\n✖ data.table::mday()    masks lubridate::mday()\n✖ data.table::minute()  masks lubridate::minute()\n✖ data.table::month()   masks lubridate::month()\n✖ data.table::quarter() masks lubridate::quarter()\n✖ data.table::second()  masks lubridate::second()\n✖ tidyterra::select()   masks dplyr::select(), raster::select()\n✖ purrr::transpose()    masks data.table::transpose()\n✖ data.table::wday()    masks lubridate::wday()\n✖ data.table::week()    masks lubridate::week()\n✖ data.table::yday()    masks lubridate::yday()\n✖ data.table::year()    masks lubridate::year()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nggplot() +\n  geom_spatraster(data = prism_tmax_0701_KS_sr) +\n  geom_sf(data = KS_county_sf, fill = NA, color = \"blue\") +\n  scale_fill_whitebox_c(\n    name = \"tmax\",\n    palette = \"muted\",\n    labels = scales::label_number(suffix = \"o\"),\n    n.breaks = 12,\n    guide = guide_legend(reverse = TRUE)\n  ) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n#--- download PRISM precipitation data ---#\nprism::get_prism_dailys(\n  type = \"tmax\",\n  date = \"2018-07-02\",\n  keepZip = FALSE\n)\n\n\n  |                                                                            \n  |                                                                      |   0%\n\n\n\nPRISM_tmax_stable_4kmD2_20180702_bil.zip already exists. Skipping downloading.\n\n\n\n  |                                                                            \n  |======================================================================| 100%\n\n#--- the file name of the PRISM data just downloaded ---#\nprism_file &lt;- \"Data/PRISM_tmax_stable_4kmD2_20180702_bil/PRISM_tmax_stable_4kmD2_20180702_bil.bil\"\n\n#--- read in the prism data and crop it to Kansas state border ---#\nprism_tmax_0702_KS_sr &lt;-\n  terra::rast(prism_file) %&gt;%\n  terra::crop(KS_county_sf)\n\n#--- read in the KS points data ---#\n(\n  KS_wells &lt;- readRDS(\"Data/Chap_5_wells_KS.rds\")\n)\n\nSimple feature collection with 37647 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -102.0495 ymin: 36.99552 xmax: -94.62089 ymax: 40.00199\nGeodetic CRS:  NAD83\nFirst 10 features:\n   well_id                   geometry\n1        1 POINT (-100.4423 37.52046)\n2        3 POINT (-100.7118 39.91526)\n3        5 POINT (-99.15168 38.48849)\n4        7 POINT (-101.8995 38.78077)\n5        8  POINT (-100.7122 38.0731)\n6        9 POINT (-97.70265 39.04055)\n7       11 POINT (-101.7114 39.55035)\n8       12 POINT (-95.97031 39.16121)\n9       15 POINT (-98.30759 38.26787)\n10      17 POINT (-100.2785 37.71539)\n\nggplot()  +\n  geom_sf(data = KS_county_sf, fill = NA) +\n  geom_sf(data = KS_wells, size = 0.05) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n# terra:extract(raster, points)\n\ntmax_from_prism &lt;- terra::extract(prism_tmax_0701_KS_sr, KS_wells)\n\nhead(tmax_from_prism)\n\n  ID PRISM_tmax_stable_4kmD2_20180701_bil\n1  1                               34.241\n2  2                               29.288\n3  3                               32.585\n4  4                               30.104\n5  5                               34.232\n6  6                               35.168\n\nKS_wells$tmax_07_01 &lt;- tmax_from_prism[,-1]\n\n\n#--- create a multi-layer SpatRaster ---#\nprism_tmax_stack &lt;- c(prism_tmax_0701_KS_sr, prism_tmax_0702_KS_sr)\n\n#--- extract tmax values ---#\ntmax_from_prism_stack &lt;- terra::extract(prism_tmax_stack, KS_wells)\n\n#--- take a look ---#\nhead(tmax_from_prism_stack)\n\n  ID PRISM_tmax_stable_4kmD2_20180701_bil PRISM_tmax_stable_4kmD2_20180702_bil\n1  1                               34.241                               30.544\n2  2                               29.288                               29.569\n3  3                               32.585                               29.866\n4  4                               30.104                               29.819\n5  5                               34.232                               30.481\n6  6                               35.168                               30.640\n\n\n\ntmax_by_county &lt;- terra::extract(prism_tmax_0701_KS_sr, KS_county_sf)\n\nclass(tmax_by_county)\n\n[1] \"data.frame\"\n\nhead(tmax_by_county)\n\n  ID PRISM_tmax_stable_4kmD2_20180701_bil\n1  1                               34.181\n2  1                               34.180\n3  1                               34.210\n4  1                               34.190\n5  1                               34.292\n6  1                               34.258\n\n\n\n#--- get mean tmax ---#\nmean_tmax &lt;-\n  tmax_by_county %&gt;%\n  group_by(ID) %&gt;%\n  summarize(tmax = mean(PRISM_tmax_stable_4kmD2_20180701_bil))\n\n(\n  KS_county_sf &lt;-\n    #--- back to sf ---#\n    KS_county_sf %&gt;%\n    #--- define ID ---#\n    mutate(ID := seq_len(nrow(.))) %&gt;%\n    #--- merge by ID ---#\n    left_join(., mean_tmax, by = \"ID\")\n)\n\nSimple feature collection with 105 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -102.0517 ymin: 36.99302 xmax: -94.58841 ymax: 40.00316\nGeodetic CRS:  NAD83\nFirst 10 features:\n   STATEFP COUNTYFP COUNTYNS       AFFGEOID GEOID         NAME\n1       20      175 00485050 0500000US20175 20175       Seward\n2       20      027 00484983 0500000US20027 20027         Clay\n3       20      171 00485048 0500000US20171 20171        Scott\n4       20      047 00484993 0500000US20047 20047      Edwards\n5       20      147 00485037 0500000US20147 20147     Phillips\n6       20      149 00485038 0500000US20149 20149 Pottawatomie\n7       20      055 00485326 0500000US20055 20055       Finney\n8       20      167 00485046 0500000US20167 20167      Russell\n9       20      135 00485031 0500000US20135 20135         Ness\n10      20      093 00485011 0500000US20093 20093       Kearny\n              NAMELSAD STUSPS STATE_NAME LSAD      ALAND   AWATER ID     tmax\n1        Seward County     KS     Kansas   06 1656693304  1961444  1 34.47948\n2          Clay County     KS     Kansas   06 1671314413 26701337  2 34.51894\n3         Scott County     KS     Kansas   06 1858536838   306079  3 33.56413\n4       Edwards County     KS     Kansas   06 1610699245   206413  4 32.26368\n5      Phillips County     KS     Kansas   06 2294395636 22493383  5 32.57836\n6  Pottawatomie County     KS     Kansas   06 2177493041 54149843  6 35.67524\n7        Finney County     KS     Kansas   06 3372157854  1716371  7 33.96920\n8       Russell County     KS     Kansas   06 2295402858 34126776  8 33.37254\n9          Ness County     KS     Kansas   06 2783562234   667491  9 32.86275\n10       Kearny County     KS     Kansas   06 2254696661  1133601 10 33.40829\n                         geometry\n1  MULTIPOLYGON (((-101.0681 3...\n2  MULTIPOLYGON (((-97.3707 39...\n3  MULTIPOLYGON (((-101.1284 3...\n4  MULTIPOLYGON (((-99.56988 3...\n5  MULTIPOLYGON (((-99.62821 3...\n6  MULTIPOLYGON (((-96.72774 3...\n7  MULTIPOLYGON (((-101.103 37...\n8  MULTIPOLYGON (((-99.04234 3...\n9  MULTIPOLYGON (((-100.2477 3...\n10 MULTIPOLYGON (((-101.5419 3...\n\n\n\ntmax_by_county &lt;- \n  terra::extract(\n    prism_tmax_0701_KS_sr,\n    KS_county_sf,\n    fun = mean\n  )\nhead(tmax_by_county)\n\n  ID PRISM_tmax_stable_4kmD2_20180701_bil\n1  1                             34.47948\n2  2                             34.51894\n3  3                             33.56413\n4  4                             32.26368\n5  5                             32.57836\n6  6                             35.67524\n\n\n\n# exactextractr::exact_extract(raster, polygons sf, include_cols = list of vars)\n\ntmax_by_county &lt;- \n  exactextractr::exact_extract(\n    prism_tmax_0701_KS_sr,\n    KS_county_sf,\n    include_cols = \"COUNTYFP\",\n    progress = FALSE\n  )\n\n  tmax_by_county[1:2] %&gt;% lapply(function(x) head(x))\n\n[[1]]\n  COUNTYFP  value coverage_fraction\n1      175 34.222         0.1074141\n2      175 34.181         0.8066747\n3      175 34.180         0.8066615\n4      175 34.210         0.8066448\n5      175 34.190         0.8061629\n6      175 34.292         0.8054324\n\n[[2]]\n  COUNTYFP  value coverage_fraction\n1      027 33.847        0.03732148\n2      027 33.897        0.10592906\n3      027 34.010        0.10249268\n4      027 34.186        0.10018899\n5      027 34.293        0.10074520\n6      027 34.220        0.09701102\n\n\n\ntmax_combined &lt;- tmax_by_county %&gt;% \n  dplyr::bind_rows() %&gt;% \n  tibble::as_tibble()",
    "crumbs": [
      "Notes",
      "terra package"
    ]
  },
  {
    "objectID": "computing/docker.html",
    "href": "computing/docker.html",
    "title": "Dockerfile",
    "section": "",
    "text": "This is a Dockerfile template for my computing environment. If you want to know more about what this does, check out my blog post.\n\n# Use Rocker image as the base for R\nFROM rocker/r-ver:4.4.0\n\nLABEL maintainer=\"Hyoungchul Kim &lt;hchul.kim96@gmail.com&gt;\"\n\n## Update and install system dependencies\nRUN apt-get update && apt-get install -y \\\n    libcurl4-openssl-dev \\\n    libssl-dev \\\n    libfontconfig1-dev \\\n    libharfbuzz-dev \\\n    libfribidi-dev \\\n    libfreetype6-dev \\\n    libpng-dev \\\n    libtiff5-dev \\\n    libjpeg-dev \\\n    libglpk-dev \\\n    libxml2-dev \\\n    libcairo2-dev \\\n    libgit2-dev \\\n    libpq-dev \\\n    libsasl2-dev \\\n    libsqlite3-dev \\\n    libssh2-1-dev \\\n    libxt-dev \\\n    libgdal-dev \\\n    wget \\\n    curl \\\n    vim \\\n    git \n\n## Install Pandoc and Quarto (Required for RMarkdown, Quarto, etc.)\n# RUN /rocker_scripts/install_pandoc.sh\n# RUN /rocker_scripts/install_quarto.sh\n\n## Install Python & Poetry\nRUN /rocker_scripts/install_python.sh && \\\n    pip3 install --upgrade pip && \\\n    pip3 install poetry\n\n# Ensure Poetry installs dependencies in the system environment\nRUN poetry config virtualenvs.create false\n\n# Copy Poetry files and install dependencies\nCOPY pyproject.toml poetry.lock .\nRUN poetry install --no-interaction --no-root\n\n## Install Julia 1.11.3 (to match Manifest.toml)\nENV JULIA_VERSION=1.11.3\nRUN /rocker_scripts/install_julia.sh\n\n## Set working directory\nWORKDIR /project\n\n## Copy renv.lock file into the folder\nCOPY renv.lock .\n\n# Set environment variables for renv\nENV RENV_VERSION=1.0.7\nENV RENV_PATHS_CACHE=/renv/cache\nENV RENV_CONFIG_REPOS_OVERRIDE=https://cloud.r-project.org\nENV RENV_CONFIG_AUTOLOADER_ENABLED=FALSE\nENV RENV_WATCHDOG_ENABLED=FALSE\nRUN echo \"options(renv.consent = TRUE)\" &gt;&gt; .Rprofile\nRUN echo \"options(RETICULATE_MINICONDA_ENABLED = FALSE)\" &gt;&gt; .Rprofile\n\n# Install renv from CRAN (avoiding bootstrapping by specifying version)\nRUN R -e \"install.packages('renv', repos = c(CRAN = 'https://cloud.r-project.org'))\"\nRUN R -e \"renv::consent(provided = TRUE)\"\n\n# Run renv restore to restore the environment\nRUN R -e \"renv::restore(confirm = FALSE)\"\n\n# Install Julia packages and manage dependencies\nCOPY Manifest.toml Project.toml .\nENV JULIA_PROJECT=/project\nRUN julia -e \"import Pkg; Pkg.activate(\\\".\\\"); Pkg.instantiate()\"\n\n# Copy over the rest of the project files\nCOPY . .\n\n# Default command\nCMD [\"bash\"]",
    "crumbs": [
      "Notes",
      "Dockerfile"
    ]
  },
  {
    "objectID": "computing/trade-notes.html",
    "href": "computing/trade-notes.html",
    "title": "(Graduate) International trade notes",
    "section": "",
    "text": "These are some notes for my self-study on International Trade (graduate level).",
    "crumbs": [
      "Notes",
      "Trade notes"
    ]
  },
  {
    "objectID": "computing/duckdb-dplyr.html",
    "href": "computing/duckdb-dplyr.html",
    "title": "DuckDB + dplyr (R)",
    "section": "",
    "text": "This is just a direct copy of the resources from Grant McDermott. Thus, I do not have any credit for it. It is solely for the archive purpose.",
    "crumbs": [
      "Notes",
      "DuckDB + dplyr (R)"
    ]
  },
  {
    "objectID": "computing/duckdb-dplyr.html#load-libraries",
    "href": "computing/duckdb-dplyr.html#load-libraries",
    "title": "DuckDB + dplyr (R)",
    "section": "Load libraries",
    "text": "Load libraries\n\nlibrary(duckdb)\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(tidyr, warn.conflicts = FALSE)",
    "crumbs": [
      "Notes",
      "DuckDB + dplyr (R)"
    ]
  },
  {
    "objectID": "computing/duckdb-dplyr.html#create-a-database-connection",
    "href": "computing/duckdb-dplyr.html#create-a-database-connection",
    "title": "DuckDB + dplyr (R)",
    "section": "Create a database connection",
    "text": "Create a database connection\nFor the d(b)plyr workflow, the connection step is very similar to the pure SQL approach. The only difference is that, after instantiating the database connection, we need to register our parquet dataset as a table in our connection via the dplyr::tbl() function. Note that we also assign it to an object (here: nyc) that can be referenced from R.\n\n## Instantiate the in-memory DuckDB connection \ncon = dbConnect(duckdb(), shutdown = TRUE)\n\n## Register our parquet dataset as table in our connection (and that assign it\n## to an object that R understands)\n# nyc = tbl(con, \"nyc-taxi/**/*.parquet\") # works, but safer to use the read_parquet func)\nnyc = tbl(con, \"read_parquet('nyc-taxi/**/*.parquet', hive_partitioning = true)\")",
    "crumbs": [
      "Notes",
      "DuckDB + dplyr (R)"
    ]
  },
  {
    "objectID": "computing/duckdb-dplyr.html#first-example",
    "href": "computing/duckdb-dplyr.html#first-example",
    "title": "DuckDB + dplyr (R)",
    "section": "First example",
    "text": "First example\nThis next command runs instantly because all computation is deferred (i.e., lazy eval). In other words, it is just a query object.\n\nq1 = nyc |&gt;\n  summarize(\n    mean_tip = mean(tip_amount),\n    .by = passenger_count\n  )\n\n\n\n\n\n\n\n.by versus group_by\n\n\n\nIn case you weren’t aware: summarize(..., .by = x) is a shorthand (and non-persistent) version of group_by(x) |&gt; summarize(...). More details here.\n\n\nWe can see what DuckDB’s query tree looks like by asking it to explain the plan\n\nexplain(q1)\n\nSimilarly, to show the SQL translation that will be implemented on the backend, using show_query.\n\nshow_query(q1)\n\nNote that printing the query object actually does enforce some computation. OTOH it’s still just a preview of the data (we haven’t pulled everything into R’s memory).\n\nq1\n\nTo actually pull all of the result data into R, we must call collect() on the query object\n\ntic = Sys.time()\ndat1 = collect(q1)\ntoc = Sys.time()\n\ndat1\ntoc - tic",
    "crumbs": [
      "Notes",
      "DuckDB + dplyr (R)"
    ]
  },
  {
    "objectID": "computing/duckdb-dplyr.html#aggregation",
    "href": "computing/duckdb-dplyr.html#aggregation",
    "title": "DuckDB + dplyr (R)",
    "section": "Aggregation",
    "text": "Aggregation\nHere’s our earlier filtering example with multiple grouping + aggregation variables…\n\nq2 = nyc |&gt;\n  filter(month &lt;= 3) |&gt;\n  summarize(\n    across(c(tip_amount, fare_amount), mean),\n    .by = c(month, passenger_count)\n  )\nq2\n\nAside: note the optimised query includes hash groupings and projection (basically: fancy column subsetting, which is a suprisingly effective strategy in query optimization)\n\nexplain(q2)\n\nAnd our high-dimensional aggregation example. We’ll create a query for this first, since I’ll reuse it shortly again\n\nq3 = nyc |&gt;\n  group_by(passenger_count, trip_distance) |&gt;\n  summarize(\n    across(c(tip_amount, fare_amount), mean),\n  ) \ncollect(q3)",
    "crumbs": [
      "Notes",
      "DuckDB + dplyr (R)"
    ]
  },
  {
    "objectID": "computing/duckdb-dplyr.html#pivot-reshape",
    "href": "computing/duckdb-dplyr.html#pivot-reshape",
    "title": "DuckDB + dplyr (R)",
    "section": "Pivot (reshape)",
    "text": "Pivot (reshape)\n\n# library(tidyr) ## already loaded\n\nq3 |&gt;\n  pivot_longer(tip_amount:fare_amount) |&gt;\n  collect()",
    "crumbs": [
      "Notes",
      "DuckDB + dplyr (R)"
    ]
  },
  {
    "objectID": "computing/duckdb-dplyr.html#joins-merges",
    "href": "computing/duckdb-dplyr.html#joins-merges",
    "title": "DuckDB + dplyr (R)",
    "section": "Joins (merges)",
    "text": "Joins (merges)\n\nmean_tips  = nyc |&gt; summarise(mean_tips = mean(tip_amount), .by = month)\nmean_fares = nyc |&gt; summarise(mean_fares = mean(fare_amount), .by = month)\n\nAgain, these commands complete instantly because all computation has been deferred until absolutely necessary (i.e.,. lazy eval).\n\nleft_join(\n  mean_fares,\n  mean_tips\n  ) |&gt;\n  collect()",
    "crumbs": [
      "Notes",
      "DuckDB + dplyr (R)"
    ]
  },
  {
    "objectID": "computing/duckdb-dplyr.html#windowing",
    "href": "computing/duckdb-dplyr.html#windowing",
    "title": "DuckDB + dplyr (R)",
    "section": "Windowing",
    "text": "Windowing\nIf you recall from the native SQL API, we sampled 1 percent of the data before creating decile bins to reduce the computation burden of sorting the entire table. Unfortunately, this approach doesn’t work as well for the dplyr frontend because the underlying SQL translation uses a generic sampling approach (rather than DuckDB’s optimised USING SAMPLE statement.)",
    "crumbs": [
      "Notes",
      "DuckDB + dplyr (R)"
    ]
  },
  {
    "objectID": "computing/duckdb-dplyr.html#close-connection",
    "href": "computing/duckdb-dplyr.html#close-connection",
    "title": "DuckDB + dplyr (R)",
    "section": "Close connection",
    "text": "Close connection\n\ndbDisconnect(con)",
    "crumbs": [
      "Notes",
      "DuckDB + dplyr (R)"
    ]
  },
  {
    "objectID": "computing/duckdb-dplyr.html#appendix-related-interfaces",
    "href": "computing/duckdb-dplyr.html#appendix-related-interfaces",
    "title": "DuckDB + dplyr (R)",
    "section": "Appendix: Related interfaces",
    "text": "Appendix: Related interfaces\n\narrow+duckdb\n\nlibrary(arrow)\nlibrary(duckdb) ## Already loaded\nlibrary(dplyr)  ## Already loaded\nlibrary(tidyr)  ## Already loaded\n\nWhen going through the arrow intermediary, we don’t need to establish a database with DBI::dbConnect like we did above. Instead, we can create a link (pointers) to the dataset on disk directly via the arrow::open_dataset() convience function. Here I’ll assign it to a new R object called nyc2.\n\nnyc2 = open_dataset(\"nyc-taxi\")\n\n\n\n\n\n\n\nopen_dataset() versus read_parquet()\n\n\n\n(For individual parquet files, we could just read then via arrow::read_parquet(), perhaps efficiently subsetting columns at the same time. But I find the open_dataset is generally what I’m looking for.)\n\n\nNote that printing our nyc2 dataset to the R console will just display the data schema. This is a cheap and convenient way to quickly interrogate the basic structure of your data, including column types, etc.\n\nnyc2\n\nThe key step for this “arrow + duckdb” dplyr workflow is to pass our arrow dataset to DuckDB via the to_duckdb() function.\n\nto_duckdb(nyc2)\n\nNote that this transfer from Arrow to DuckDB is very quick (and memory cheap) because it is a zero copy. We are just passing around pointers instead of actually moving any data. See this blog post for more details, but the high-level take away is that we are benefitting from the tightly integrated architectures of these two libraries.1\nAt this, point all of the regular dplyr workflow logic from above should carry over. Just remember to first pass the arrow dataset via the to_duckdb() funciton. For example, here’s our initial aggregation query again:\n\nnyc2 |&gt;\n  to_duckdb() |&gt; ## &lt;= key step\n  summarise(\n    mean_tip = mean(tip_amount),\n    .by = passenger_count\n  ) |&gt;\n  collect()\n\n\n\n\n\n\n\nArrow’s native acero engine\n\n\n\nSome of you may be used to performing computation with the arrow package without going through DuckDB. What’s happening here is that arrow provides its own computation engine called “acero”. This Arrow-native engine is actually pretty performant… albeit not a fast as DuckDB, nor as feature rich. So I personally recommend always passing to DuckDB if you can. Still, if you’re curious then you can test yourself by re-trying the code chunk, but commenting out the to_duckdb() line. For more details, see here.\n\n\n\n\nduckplyr\nThe new kid on the block is duckplyr (announcement / homepage). Without going into too much depth, the promise of duckplyr is that it can provide a “fully native” dplyr experience that is directly coupled to DuckDB’s query engine. So, for example, it won’t have to rely on DBI’s generic’ SQL translations. Instead, the relevant dplyr “verbs” are being directly translated to DuckDB’s relational API to construct logical query plans. If that’s too much jargon, just know that it should involve less overhead, fewer translation errors, and better optimization. Moreover, a goal of duckplyr is for it to be a drop-in replace for dplyr in general. In other words, you could just swap out library(dplyr) for library(duckplyr) and all of your data wrangling operations will come backed by the power of DuckDB. This includes for working on “regular” R data frames in memory.\nAll of this is exciting and I would urge you stay tuned. Right now, duckplyr is still marked as experimental and has a few rough edges. But the basics are there. For example:\n\nlibrary(duckplyr, warn.conflicts = FALSE)\n\nduckplyr_df_from_parquet(\"nyc-taxi/**/*.parquet\") |&gt;\n  summarise(\n    mean_tip = mean(tip_amount),\n    .by = passenger_count\n  )",
    "crumbs": [
      "Notes",
      "DuckDB + dplyr (R)"
    ]
  },
  {
    "objectID": "computing/duckdb-dplyr.html#footnotes",
    "href": "computing/duckdb-dplyr.html#footnotes",
    "title": "DuckDB + dplyr (R)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“Similar” might be a better description than “integrated”, since DuckdB does not use the Arrow memory model. But they are both columnar-orientated (among other things) and so the end result is pretty seamless integration.↩︎",
    "crumbs": [
      "Notes",
      "DuckDB + dplyr (R)"
    ]
  },
  {
    "objectID": "computing/duckdb-sql.html",
    "href": "computing/duckdb-sql.html",
    "title": "DuckDB SQL",
    "section": "",
    "text": "This is example templates that use DuckDB with SQL for R and Python. Note that these are short examples. If you want to know more about what they can do, check out this site.\n\nRPython\n\n\n\nlibrary(duckdb)\n\ncon = dbConnect(duckdb(), shutdown = TRUE)\n\n# uncomment and run the next line if you'd like to create a persistent, disk-based database instead. It is good for computation for bigger than RAM data.\n\n# con = dbConnect(duckdb(), dbdir = \"nyc.duck\")\n\n# SELECT\n#   passenger_count,\n#   AVG(tip_amount) AS mean_tip\n# FROM 'nyc-taxi/**/*.parquet'\n# GROUP BY passenger_count\n# ORDER BY passenger_count\n\ntic = Sys.time()\ndat1 = dbGetQuery(\n  con,\n  \"\n  FROM 'nyc-taxi/**/*.parquet'\n  SELECT\n    passenger_count,\n    AVG(tip_amount) AS mean_tip\n  GROUP BY ALL\n  ORDER BY ALL\n  \"\n)\ntoc = Sys.time()\n\ndat1\ntoc - tic\n\ntic = Sys.time()\ndat2 = dbGetQuery(\n  con,\n  \"\n  FROM 'nyc-taxi/**/*.parquet'\n  SELECT\n    month,\n    passenger_count,\n    AVG(tip_amount) AS mean_tip\n  WHERE month &lt;= 3\n  GROUP BY ALL\n  \"\n    )\ntoc = Sys.time()\n\nhead(dat2)\n\ndbDisconnect(con)\n\n\n\n\nimport duckdb\nimport time\n\ncon = duckdb.connect(database=':memory:', read_only=False)\n\n\n# uncomment and run the next line if you'd like to create a persistent, disk-based database instead. It is good for computation for bigger than RAM data.\n\ncon = duckdb.connect(database='nyc.duck', read_only=False)\n\ntic = time.time()\ndat1 = (\n  con.\n  query(\n    '''\n    FROM 'nyc-taxi/**/*.parquet'\n    SELECT\n      passenger_count,\n      AVG(tip_amount) AS mean_tip\n    GROUP BY ALL\n    ORDER BY ALL\n    '''\n    )\n)\ntoc = time.time()\n\ndat1\n\ntic = time.time()\ndat2 = (\n  con.\n  query(\n    '''\n    FROM 'nyc-taxi/**/*.parquet'\n    SELECT\n      month,\n      passenger_count,\n      AVG(tip_amount) AS mean_tip\n    WHERE month &lt;= 3\n    GROUP BY ALL\n    '''\n  )\n)\ntoc = time.time()\n\ndat2\n\ncon.close()",
    "crumbs": [
      "Notes",
      "DuckDB SQL"
    ]
  },
  {
    "objectID": "misc/links.html",
    "href": "misc/links.html",
    "title": "Useful links",
    "section": "",
    "text": "Category\nLinks\n\n\n\n\nPersonal website\n🔗 GitHub site\n\n\nCoffee chat (with me)\n🔗 Appointment link\n\n\nComputing and replication resources\n🔗 Web-scraping 1  🔗 Web-scraping 2  🔗 Parallel programming  🔗 Docker  🔗 Google compute engine 1  🔗 Google compute engine 2  🔗 Databases  🔗 Spark  🔗 DuckDB and polars  🔗 NYU Course Notes & Resources  🔗 Quantitative dynamic model\n\n\nSpatial packages\n🔗 R as GIS for economists  🔗 Spatial data science\n\n\nJulia\n🔗 Julia for data science  🔗 Computational economics for PhDs  🔗 Computational Methods in Macroeconomics  🔗 BLPDemand.jl  🔗 Guide to Efficient Computational Work in Economics  🔗 Econometrics with Julia  🔗 Advanced Dynamic Programming\n\n\nUseful packages\n🔗 New DiD methods\n\n\nGraduate trade and spatial\n🔗 Treb Allen\n\n\nNotes\n🔗 BLP-notes (Adam Smith)",
    "crumbs": [
      "Misc.",
      "Useful links"
    ]
  },
  {
    "objectID": "computing/computing.html",
    "href": "computing/computing.html",
    "title": "Overview",
    "section": "",
    "text": "This section is a set of templates related to computing. It also contains some notes on various subjects in Economics.",
    "crumbs": [
      "Notes",
      "Overview"
    ]
  },
  {
    "objectID": "computing/julia-self-study.html",
    "href": "computing/julia-self-study.html",
    "title": "Julia self study",
    "section": "",
    "text": "using DataFrames\nusing CSV\nusing XLSX\nMake a toy dataframe\nfunction grades_2020()\n    name = [\"Sally\", \"Bob\", \"Alice\", \"Hank\"]\n    grade_2020 = [1, 5, 8.5, 4]\n    DataFrame(; name, grade_2020)\nend\ngrades_2020()\n\nfunction grades_2021()\n    name = [\"Sally\", \"Bob\", \"Alice\", \"Hank\", \"John\"]\n    grade_2021 = [3, 5, 8.5, 7, 10]\n    DataFrame(; name, grade_2021)\nend\ngrades_2021()\n\n5×2 DataFrame\n\n\n\nRow\nname\ngrade_2021\n\n\n\nString\nFloat64\n\n\n\n\n1\nSally\n3.0\n\n\n2\nBob\n5.0\n\n\n3\nAlice\n8.5\n\n\n4\nHank\n7.0\n\n\n5\nJohn\n10.0",
    "crumbs": [
      "Notes",
      "Julia self-study"
    ]
  },
  {
    "objectID": "computing/julia-self-study.html#csv-files",
    "href": "computing/julia-self-study.html#csv-files",
    "title": "Julia self study",
    "section": "CSV files",
    "text": "CSV files\nWrite the dataframe into CSV file\n\nCSV.write(\"toy.csv\", grades_2020())\n\n\"toy.csv\"\n\n\nRead in the CSV file\n\ntoy = CSV.read(\"toy.csv\", DataFrame)\nfirst(toy, 5)\n\n4×2 DataFrame\n\n\n\nRow\nname\ngrade_2020\n\n\n\nString7\nFloat64\n\n\n\n\n1\nSally\n1.0\n\n\n2\nBob\n5.0\n\n\n3\nAlice\n8.5\n\n\n4\nHank\n4.0",
    "crumbs": [
      "Notes",
      "Julia self-study"
    ]
  },
  {
    "objectID": "computing/julia-self-study.html#excel-files",
    "href": "computing/julia-self-study.html#excel-files",
    "title": "Julia self study",
    "section": "Excel files",
    "text": "Excel files\n\nfunction write_xlsx(name, df::DataFrame)\n    path = \"$name.xlsx\"\n    data = collect(eachcol(df))\n    cols = names(df)\n    write(path, data, cols)\nend\n\nfunction write_grades_xlsx()\n    path = \"grades\"\n    write_xlsx(path, grades_2020())\n    \"$path.xlsx\"\nend\n\npath = write_grades_xlsx()\nxf = readxlsx(path)",
    "crumbs": [
      "Notes",
      "Julia self-study"
    ]
  },
  {
    "objectID": "computing/julia-self-study.html#filter-and-subset",
    "href": "computing/julia-self-study.html#filter-and-subset",
    "title": "Julia self study",
    "section": "Filter and subset",
    "text": "Filter and subset\n\nequals_alice(name::String) = name == \"Alice\"\n\nfilter(:name =&gt; equals_alice, grades_2020())\n\nfilter(:name =&gt; n -&gt; n == \"Alice\", grades_2020())\n\nfilter(:name =&gt; ==(\"Alice\"), grades_2020())\nfilter(:name =&gt; !=(\"Alice\"), grades_2020())\n\n3×2 DataFrame\n\n\n\nRow\nname\ngrade_2020\n\n\n\nString\nFloat64\n\n\n\n\n1\nSally\n1.0\n\n\n2\nBob\n5.0\n\n\n3\nHank\n4.0\n\n\n\n\n\n\n\nfunction complex_filter(name, grade)::Bool\n    interesting_name = startswith(name, 'A') || startswith(name, 'B')\n    interesting_grade = 6 &lt; grade\n    interesting_name && interesting_grade\nend\n\nfilter([:name, :grade_2020] =&gt; complex_filter, grades_2020())\n\n1×2 DataFrame\n\n\n\nRow\nname\ngrade_2020\n\n\n\nString\nFloat64\n\n\n\n\n1\nAlice\n8.5",
    "crumbs": [
      "Notes",
      "Julia self-study"
    ]
  },
  {
    "objectID": "computing/julia-self-study.html#select",
    "href": "computing/julia-self-study.html#select",
    "title": "Julia self study",
    "section": "Select",
    "text": "Select\n\nfunction responses()\n    id = [1, 2]\n    q1 = [28, 61]\n    q2 = [:us, :fr]\n    q3 = [\"F\", \"B\"]\n    q4 = [\"B\", \"C\"]\n    q5 = [\"A\", \"E\"]\n    DataFrame(; id, q1, q2, q3, q4, q5)\nend\nresponses()\n\n2×6 DataFrame\n\n\n\nRow\nid\nq1\nq2\nq3\nq4\nq5\n\n\n\nInt64\nInt64\nSymbol\nString\nString\nString\n\n\n\n\n1\n1\n28\nus\nF\nB\nA\n\n\n2\n2\n61\nfr\nB\nC\nE\n\n\n\n\n\n\n\nselect(responses(), :id, :q1)\n\n# regex\nselect(responses(), r\"^q\")\n\nselect(responses(), Not(:q5))\nselect(responses(), Not([:q4, :q5]))\nselect(responses(), :q5, Not(:q5))\n\n# renaming\nselect(responses(), 1 =&gt; \"participant\", :q1 =&gt; \"age\", :q2 =&gt; \"nationality\")\nrenames = (1 =&gt; \"participant\", :q1 =&gt; \"age\", :q2 =&gt; \"nationality\")\nselect(responses(), renames...)\n\n2×3 DataFrame\n\n\n\nRow\nparticipant\nage\nnationality\n\n\n\nInt64\nInt64\nSymbol\n\n\n\n\n1\n1\n28\nus\n\n\n2\n2\n61\nfr",
    "crumbs": [
      "Notes",
      "Julia self-study"
    ]
  },
  {
    "objectID": "computing/julia-self-study.html#left-and-right-joins",
    "href": "computing/julia-self-study.html#left-and-right-joins",
    "title": "Julia self study",
    "section": "Left and right joins",
    "text": "Left and right joins\n\nleftjoin(grades_2020(), grades_2021(); on=:name)\n\nrightjoin(grades_2020(), grades_2021(); on=:name)\n\n5×3 DataFrame\n\n\n\nRow\nname\ngrade_2020\ngrade_2021\n\n\n\nString\nFloat64?\nFloat64\n\n\n\n\n1\nSally\n1.0\n3.0\n\n\n2\nBob\n5.0\n5.0\n\n\n3\nAlice\n8.5\n8.5\n\n\n4\nHank\n4.0\n7.0\n\n\n5\nJohn\nmissing\n10.0",
    "crumbs": [
      "Notes",
      "Julia self-study"
    ]
  },
  {
    "objectID": "computing/julia-self-study.html#transform-variables",
    "href": "computing/julia-self-study.html#transform-variables",
    "title": "Julia self study",
    "section": "Transform variables",
    "text": "Transform variables\n\nplus_one(grades) = grades .+ 1\ntransform(grades_2020(), :grade_2020 =&gt; plus_one)\n\ntransform(grades_2020(), :grade_2020 =&gt; plus_one =&gt; :grade_2020)\ntransform(grades_2020(), :grade_2020 =&gt; plus_one; renamecols=false)\n\n4×2 DataFrame\n\n\n\nRow\nname\ngrade_2020\n\n\n\nString\nFloat64\n\n\n\n\n1\nSally\n2.0\n\n\n2\nBob\n6.0\n\n\n3\nAlice\n9.5\n\n\n4\nHank\n5.0\n\n\n\n\n\n\n\nleftjoined = leftjoin(grades_2020(), grades_2021(); on=:name)\n\nfunction only_pass()\n    leftjoined = leftjoin(grades_2020(), grades_2021(); on=:name)\n    pass(A, B) = [5.5 &lt; a || 5.5 &lt; b for (a, b) in zip(A, B)]\n    leftjoined = transform(leftjoined, [:grade_2020, :grade_2021] =&gt; pass =&gt; :pass)\n    passed = subset(leftjoined, :pass; skipmissing=true)\n    return passed.name\nend\nonly_pass()\n\n2-element Vector{String}:\n \"Alice\"\n \"Hank\"",
    "crumbs": [
      "Notes",
      "Julia self-study"
    ]
  },
  {
    "objectID": "computing/julia-self-study.html#groupby-and-combine",
    "href": "computing/julia-self-study.html#groupby-and-combine",
    "title": "Julia self study",
    "section": "Groupby and combine",
    "text": "Groupby and combine\n\nfunction all_grades()\n    df1 = grades_2020()\n    df1 = select(df1, :name, :grade_2020 =&gt; :grade)\n    df2 = grades_2021()\n    df2 = select(df2, :name, :grade_2021 =&gt; :grade)\n    rename_bob2(data_col) = replace.(data_col, \"Bob 2\" =&gt; \"Bob\")\n    df2 = transform(df2, :name =&gt; rename_bob2 =&gt; :name)\n    return vcat(df1, df2)\nend\nall_grades()\n\n9×2 DataFrame\n\n\n\nRow\nname\ngrade\n\n\n\nString\nFloat64\n\n\n\n\n1\nSally\n1.0\n\n\n2\nBob\n5.0\n\n\n3\nAlice\n8.5\n\n\n4\nHank\n4.0\n\n\n5\nSally\n3.0\n\n\n6\nBob\n5.0\n\n\n7\nAlice\n8.5\n\n\n8\nHank\n7.0\n\n\n9\nJohn\n10.0\n\n\n\n\n\n\n\ngroupby(all_grades(), :name)\n\nusing Statistics\n\ngdf = groupby(all_grades(), :name)\ncombine(gdf, :grade =&gt; mean)\n\n5×2 DataFrame\n\n\n\nRow\nname\ngrade_mean\n\n\n\nString\nFloat64\n\n\n\n\n1\nSally\n2.0\n\n\n2\nBob\n5.0\n\n\n3\nAlice\n8.5\n\n\n4\nHank\n5.5\n\n\n5\nJohn\n10.0\n\n\n\n\n\n\n\ngroup = [:A, :A, :B, :B]\nX = 1:4\nY = 5:8\ndf = DataFrame(; group, X, Y)\n\ngdf = groupby(df, :group)\ncombine(gdf, [:X, :Y] .=&gt; mean; renamecols=false)\n\n2×3 DataFrame\n\n\n\nRow\ngroup\nX\nY\n\n\n\nSymbol\nFloat64\nFloat64\n\n\n\n\n1\nA\n1.5\n5.5\n\n\n2\nB\n3.5\n7.5",
    "crumbs": [
      "Notes",
      "Julia self-study"
    ]
  },
  {
    "objectID": "computing/julia-self-study.html#missing-values",
    "href": "computing/julia-self-study.html#missing-values",
    "title": "Julia self study",
    "section": "Missing values",
    "text": "Missing values\n\ndf_missing = DataFrame(;\n    name=[missing, \"Sally\", \"Alice\", \"Hank\"],\n    age=[17, missing, 20, 19],\n    grade_2020=[5.0, 1.0, missing, 4.0],\n)\n\ndropmissing(df_missing)\n\ndropmissing(df_missing, :name)\n\ndropmissing(df_missing, [:name, :age])\n\n2×3 DataFrame\n\n\n\nRow\nname\nage\ngrade_2020\n\n\n\nString\nInt64\nFloat64?\n\n\n\n\n1\nAlice\n20\nmissing\n\n\n2\nHank\n19\n4.0\n\n\n\n\n\n\n\nfilter(:name =&gt; !ismissing, df_missing)\n\ncombine(df_missing, :grade_2020 =&gt; mean ∘ skipmissing )\n\n1×1 DataFrame\n\n\n\nRow\ngrade_2020_mean_skipmissing\n\n\n\nFloat64\n\n\n\n\n1\n3.33333",
    "crumbs": [
      "Notes",
      "Julia self-study"
    ]
  },
  {
    "objectID": "computing/julia-self-study.html#select-1",
    "href": "computing/julia-self-study.html#select-1",
    "title": "Julia self study",
    "section": "Select",
    "text": "Select\n\ndf = all_grades()\n\n9×2 DataFrame\n\n\n\nRow\nname\ngrade\n\n\n\nString\nFloat64\n\n\n\n\n1\nSally\n1.0\n\n\n2\nBob\n5.0\n\n\n3\nAlice\n8.5\n\n\n4\nHank\n4.0\n\n\n5\nSally\n3.0\n\n\n6\nBob\n5.0\n\n\n7\nAlice\n8.5\n\n\n8\nHank\n7.0\n\n\n9\nJohn\n10.0\n\n\n\n\n\n\n\n@select df :name\n@select df :name :grade\n@select df $(Not(:grade))\n\n9×1 DataFrame\n\n\n\nRow\nname\n\n\n\nString\n\n\n\n\n1\nSally\n\n\n2\nBob\n\n\n3\nAlice\n\n\n4\nHank\n\n\n5\nSally\n\n\n6\nBob\n\n\n7\nAlice\n\n\n8\nHank\n\n\n9\nJohn\n\n\n\n\n\n\n\n@select df :grade_100 = :grade .* 10\n@rselect df :grade_100 = :grade * 10\n\n9×1 DataFrame\n\n\n\nRow\ngrade_100\n\n\n\nFloat64\n\n\n\n\n1\n10.0\n\n\n2\n50.0\n\n\n3\n85.0\n\n\n4\n40.0\n\n\n5\n30.0\n\n\n6\n50.0\n\n\n7\n85.0\n\n\n8\n70.0\n\n\n9\n100.0",
    "crumbs": [
      "Notes",
      "Julia self-study"
    ]
  },
  {
    "objectID": "computing/julia-self-study.html#column-transformation",
    "href": "computing/julia-self-study.html#column-transformation",
    "title": "Julia self study",
    "section": "Column transformation",
    "text": "Column transformation\n\n@rtransform df :grade_100 = :grade * 10\n\n9×3 DataFrame\n\n\n\nRow\nname\ngrade\ngrade_100\n\n\n\nString\nFloat64\nFloat64\n\n\n\n\n1\nSally\n1.0\n10.0\n\n\n2\nBob\n5.0\n50.0\n\n\n3\nAlice\n8.5\n85.0\n\n\n4\nHank\n4.0\n40.0\n\n\n5\nSally\n3.0\n30.0\n\n\n6\nBob\n5.0\n50.0\n\n\n7\nAlice\n8.5\n85.0\n\n\n8\nHank\n7.0\n70.0\n\n\n9\nJohn\n10.0\n100.0\n\n\n\n\n\n\n\n@rtransform df :grade_100 = :grade * 10 :grade_5 = :grade / 2\n\n@rtransform df begin\n    :grade_100 = :grade * 10\n    :grade_5 = :grade / 2\nend\n\n9×4 DataFrame\n\n\n\nRow\nname\ngrade\ngrade_100\ngrade_5\n\n\n\nString\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\nSally\n1.0\n10.0\n0.5\n\n\n2\nBob\n5.0\n50.0\n2.5\n\n\n3\nAlice\n8.5\n85.0\n4.25\n\n\n4\nHank\n4.0\n40.0\n2.0\n\n\n5\nSally\n3.0\n30.0\n1.5\n\n\n6\nBob\n5.0\n50.0\n2.5\n\n\n7\nAlice\n8.5\n85.0\n4.25\n\n\n8\nHank\n7.0\n70.0\n3.5\n\n\n9\nJohn\n10.0\n100.0\n5.0\n\n\n\n\n\n\n\nleftjoined = leftjoin(grades_2020(), grades_2021(); on=:name)\n@rtransform! leftjoined :grade_2021 = coalesce(:grade_2021, 5)\n\n@rtransform leftjoined :mean_grades = (:grade_2020 + :grade_2021) / 2\n\n4×4 DataFrame\n\n\n\nRow\nname\ngrade_2020\ngrade_2021\nmean_grades\n\n\n\nString\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\nSally\n1.0\n3.0\n2.0\n\n\n2\nBob\n5.0\n5.0\n5.0\n\n\n3\nAlice\n8.5\n8.5\n8.5\n\n\n4\nHank\n4.0\n7.0\n5.5",
    "crumbs": [
      "Notes",
      "Julia self-study"
    ]
  },
  {
    "objectID": "computing/julia-self-study.html#row-selection",
    "href": "computing/julia-self-study.html#row-selection",
    "title": "Julia self study",
    "section": "Row selection",
    "text": "Row selection\n\n@rsubset df :grade &gt; 7\n\n3×2 DataFrame\n\n\n\nRow\nname\ngrade\n\n\n\nString\nFloat64\n\n\n\n\n1\nAlice\n8.5\n\n\n2\nAlice\n8.5\n\n\n3\nJohn\n10.0\n\n\n\n\n\n\n\n@subset df :grade .&gt; mean(:grade)\n\n4×2 DataFrame\n\n\n\nRow\nname\ngrade\n\n\n\nString\nFloat64\n\n\n\n\n1\nAlice\n8.5\n\n\n2\nAlice\n8.5\n\n\n3\nHank\n7.0\n\n\n4\nJohn\n10.0\n\n\n\n\n\n\n\n@rsubset df begin\n    :grade &gt; 7\n    startswith(:name, \"A\")\nend\n\n2×2 DataFrame\n\n\n\nRow\nname\ngrade\n\n\n\nString\nFloat64\n\n\n\n\n1\nAlice\n8.5\n\n\n2\nAlice\n8.5",
    "crumbs": [
      "Notes",
      "Julia self-study"
    ]
  },
  {
    "objectID": "computing/julia-self-study.html#row-sorting",
    "href": "computing/julia-self-study.html#row-sorting",
    "title": "Julia self study",
    "section": "Row sorting",
    "text": "Row sorting\n\n@orderby leftjoined :grade_2021\n@orderby leftjoined -:grade_2021\n\n@orderby leftjoined begin\n    :grade_2021\n    :name\nend\n\n4×3 DataFrame\n\n\n\nRow\nname\ngrade_2020\ngrade_2021\n\n\n\nString\nFloat64\nFloat64\n\n\n\n\n1\nSally\n1.0\n3.0\n\n\n2\nBob\n5.0\n5.0\n\n\n3\nHank\n4.0\n7.0\n\n\n4\nAlice\n8.5\n8.5",
    "crumbs": [
      "Notes",
      "Julia self-study"
    ]
  },
  {
    "objectID": "computing/julia-self-study.html#data-summaries",
    "href": "computing/julia-self-study.html#data-summaries",
    "title": "Julia self study",
    "section": "Data summaries",
    "text": "Data summaries\n\n@combine leftjoined :mean_grade_2020 = mean(:grade_2020)\n\n1×1 DataFrame\n\n\n\nRow\nmean_grade_2020\n\n\n\nFloat64\n\n\n\n\n1\n4.625\n\n\n\n\n\n\n\n@combine leftjoined begin\n    :mean_grade_2020 = mean(:grade_2020)\n    :mean_grade_2021 = mean(:grade_2021)\nend\n\n1×2 DataFrame\n\n\n\nRow\nmean_grade_2020\nmean_grade_2021\n\n\n\nFloat64\nFloat64\n\n\n\n\n1\n4.625\n5.875\n\n\n\n\n\n\n\ngdf = groupby(leftjoined, :name)\n@combine gdf begin\n    :mean_grade_2020 = mean(:grade_2020)\n    :mean_grade_2021 = mean(:grade_2021)\nend\n\n4×3 DataFrame\n\n\n\nRow\nname\nmean_grade_2020\nmean_grade_2021\n\n\n\nString\nFloat64\nFloat64\n\n\n\n\n1\nSally\n1.0\n3.0\n\n\n2\nBob\n5.0\n5.0\n\n\n3\nAlice\n8.5\n8.5\n\n\n4\nHank\n4.0\n7.0",
    "crumbs": [
      "Notes",
      "Julia self-study"
    ]
  },
  {
    "objectID": "computing/julia-self-study.html#piping-operations",
    "href": "computing/julia-self-study.html#piping-operations",
    "title": "Julia self study",
    "section": "Piping operations",
    "text": "Piping operations\n\n@chain leftjoined begin\n    groupby(:name)\n    @combine :mean_grade_2020 = mean(:grade_2020)\nend\n\n4×2 DataFrame\n\n\n\nRow\nname\nmean_grade_2020\n\n\n\nString\nFloat64\n\n\n\n\n1\nSally\n1.0\n\n\n2\nBob\n5.0\n\n\n3\nAlice\n8.5\n\n\n4\nHank\n4.0\n\n\n\n\n\n\n\n@chain leftjoined begin\n    groupby(:name)\n    @combine begin\n        :mean_grade_2020 = mean(:grade_2020)\n        :mean_grade_2021 = mean(:grade_2021)\n    end\nend\n\n4×3 DataFrame\n\n\n\nRow\nname\nmean_grade_2020\nmean_grade_2021\n\n\n\nString\nFloat64\nFloat64\n\n\n\n\n1\nSally\n1.0\n3.0\n\n\n2\nBob\n5.0\n5.0\n\n\n3\nAlice\n8.5\n8.5\n\n\n4\nHank\n4.0\n7.0",
    "crumbs": [
      "Notes",
      "Julia self-study"
    ]
  },
  {
    "objectID": "computing/julia-self-study.html#finale",
    "href": "computing/julia-self-study.html#finale",
    "title": "Julia self study",
    "section": "Finale!",
    "text": "Finale!\n\n@chain leftjoined begin\n    @rtransform begin\n        :grade_2020 = :grade_2020 * 10\n        :grade_2021 = :grade_2021 * 10\n    end\n    groupby(:name)\n    @combine begin\n        :mean_grade_2020 = mean(:grade_2020)\n        :mean_grade_2021 = mean(:grade_2021)\n    end\n    @rtransform :mean_grades = (:mean_grade_2020 + :mean_grade_2021) / 2\n    @rsubset :mean_grades &gt; 50\n    @orderby -:mean_grades\nend\n\n2×4 DataFrame\n\n\n\nRow\nname\nmean_grade_2020\nmean_grade_2021\nmean_grades\n\n\n\nString\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\nAlice\n85.0\n85.0\n85.0\n\n\n2\nHank\n40.0\n70.0\n55.0",
    "crumbs": [
      "Notes",
      "Julia self-study"
    ]
  },
  {
    "objectID": "computing/julia-self-study.html#geometric-series",
    "href": "computing/julia-self-study.html#geometric-series",
    "title": "Julia self study",
    "section": "Geometric series",
    "text": "Geometric series\n\n# using Pkg\n# Pkg.update()\nusing LinearAlgebra, Statistics\nusing Distributions, Plots, Random, LaTeXStrings, Symbolics\n\nExample: The Keynesian multiplier\n\\(y_t = \\frac{1-b^{t+1}}{1-b}i \\Rightarrow \\frac{1}{1-b}i\\).\n\n# function that calculates a path of y\nfunction calculate_y(i, b, g, T, y_init)\n    y = zeros(T+1)\n    y[1] = i + b * y_init + g\n    for t in 2:(T+1)\n        y[t] = b * y[t-1] + i + g\n    end\n    return y\nend\n\n# Initial values\ni_0 = 0.3\ng_0 = 0.3\nb = 2 / 3\ny_init = 0\nT = 100\n\nplt = plot(0:T, calculate_y(i_0, b, g_0, T, y_init),\n            title = \"Path of Aggregate Output Over Time\",\n            ylim = (0.5, 1.9), xlabel = L\"t\", ylabel = L\"y_t\")\n\n# Output predicted by geometric series\nhline!([i_0 / (1-b) + g_0 / (1-b)], linestyle = :dash, seriestype = \"hline\", legend = false)",
    "crumbs": [
      "Notes",
      "Julia self-study"
    ]
  },
  {
    "objectID": "computing/julia-self-study.html#linear-algebra",
    "href": "computing/julia-self-study.html#linear-algebra",
    "title": "Julia self study",
    "section": "Linear algebra",
    "text": "Linear algebra",
    "crumbs": [
      "Notes",
      "Julia self-study"
    ]
  },
  {
    "objectID": "computing/sf-vector-data.html",
    "href": "computing/sf-vector-data.html",
    "title": "Geospatial with R",
    "section": "",
    "text": "This is my practice sections following R as GIS for Economists.\n\nBasics\nLoad the packages:\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  sf, # vector data operations\n  tidyverse, # data wrangling\n  data.table, # data wrangling\n  tmap, # make maps\n  mapview, # create an interactive map\n  patchwork, # arranging maps\n  rmapshaper\n)\n\n\n#--- a dataset that comes with the sf package ---#\nnc &lt;- sf::st_read(system.file(\"shape/nc.shp\", package = \"sf\"))\n\nReading layer `nc' from data source \n  `/home/himakun/R/x86_64-pc-linux-gnu-library/4.4/sf/shape/nc.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 100 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\nGeodetic CRS:  NAD27\n\nggplot() +\n  geom_sf(data = nc) +\n  theme_void()\n\n\n\n\n\n\n\n\n\nhead(nc)\n\nSimple feature collection with 6 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -81.74107 ymin: 36.07282 xmax: -75.77316 ymax: 36.58965\nGeodetic CRS:  NAD27\n   AREA PERIMETER CNTY_ CNTY_ID        NAME  FIPS FIPSNO CRESS_ID BIR74 SID74\n1 0.114     1.442  1825    1825        Ashe 37009  37009        5  1091     1\n2 0.061     1.231  1827    1827   Alleghany 37005  37005        3   487     0\n3 0.143     1.630  1828    1828       Surry 37171  37171       86  3188     5\n4 0.070     2.968  1831    1831   Currituck 37053  37053       27   508     1\n5 0.153     2.206  1832    1832 Northampton 37131  37131       66  1421     9\n6 0.097     1.670  1833    1833    Hertford 37091  37091       46  1452     7\n  NWBIR74 BIR79 SID79 NWBIR79                       geometry\n1      10  1364     0      19 MULTIPOLYGON (((-81.47276 3...\n2      10   542     3      12 MULTIPOLYGON (((-81.23989 3...\n3     208  3616     6     260 MULTIPOLYGON (((-80.45634 3...\n4     123   830     2     145 MULTIPOLYGON (((-76.00897 3...\n5    1066  1606     3    1197 MULTIPOLYGON (((-77.21767 3...\n6     954  1838     5    1237 MULTIPOLYGON (((-76.74506 3...\n\n\nsfg: LINESTRING\n\n#--- collection of points in a matrix form ---#\ns1 &lt;- rbind(c(2, 3), c(3, 4), c(3, 5), c(1, 5))\n\n#--- see what s1 looks like ---#\ns1\n\n     [,1] [,2]\n[1,]    2    3\n[2,]    3    4\n[3,]    3    5\n[4,]    1    5\n\n#--- create a \"LINESTRING\" ---#\na_linestring &lt;- sf::st_linestring(s1)\n\n#--- check the class ---#\nclass(a_linestring)\n\n[1] \"XY\"         \"LINESTRING\" \"sfg\"       \n\n\nsfg: POLYGON\n\n#--- a hole within p1 ---#\np1 &lt;- rbind(c(0, 0), c(3, 0), c(3, 2), c(2, 5), c(1, 3), c(0, 0))\np2 &lt;- rbind(c(1, 1), c(1, 2), c(2, 2), c(1, 1))\n\n#--- create a polygon with hole ---#\na_plygon_with_a_hole &lt;- sf::st_polygon(list(p1, p2))\n\nplot(a_plygon_with_a_hole)\n\n\n\n\n\n\n\n\nCreating sfc object or sf object:\n\n#--- create an sfc ---#\nsfc_ex &lt;- sf::st_sfc(list(a_linestring, a_plygon_with_a_hole))\n\n#--- create a data.frame ---#\ndf_ex &lt;- data.frame(name = c(\"A\", \"B\"))\n\n#--- add the sfc as a column ---#\ndf_ex$geometry &lt;- sfc_ex\n\n#--- take a look ---#\ndf_ex\n\n  name                       geometry\n1    A LINESTRING (2 3, 3 4, 3 5, ...\n2    B POLYGON ((0 0, 3 0, 3 2, 2 ...\n\n\n\n#--- let R recognize the data frame as sf ---#\nsf_ex &lt;- sf::st_as_sf(df_ex)\n\n#--- see what it looks like ---#\nsf_ex\n\nSimple feature collection with 2 features and 1 field\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 0 ymin: 0 xmax: 3 ymax: 5\nCRS:           NA\n  name                       geometry\n1    A LINESTRING (2 3, 3 4, 3 5, ...\n2    B POLYGON ((0 0, 3 0, 3 2, 2 ...\n\n\nRead shapefile:\n\n#--- read a NE county boundary shapefile ---#\nnc_loaded &lt;- sf::st_read(\"Data/nc.shp\")\n\nReading layer `nc' from data source \n  `/home/himakun/Documents/tools/computing/Data/nc.shp' using driver `ESRI Shapefile'\nSimple feature collection with 100 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\nGeodetic CRS:  NAD27\n\n\nProjection with different CRS:\n\nsf::st_crs(nc)\n\nCoordinate Reference System:\n  User input: NAD27 \n  wkt:\nGEOGCRS[\"NAD27\",\n    DATUM[\"North American Datum 1927\",\n        ELLIPSOID[\"Clarke 1866\",6378206.4,294.978698213898,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4267]]\n\n\nChange the CRS projection:\n\n#--- transform ---#\nnc_wgs84 &lt;- sf::st_transform(nc, 4326)\n\n#--- check if the transformation was successful ---#\nsf::st_crs(nc_wgs84)\n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n#--- transform ---#\nnc_utm17N &lt;- sf::st_transform(nc_wgs84, 26917)\n\n#--- check if the transformation was successful ---#\nsf::st_crs(nc_utm17N)\n\nCoordinate Reference System:\n  User input: EPSG:26917 \n  wkt:\nPROJCRS[\"NAD83 / UTM zone 17N\",\n    BASEGEOGCRS[\"NAD83\",\n        DATUM[\"North American Datum 1983\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4269]],\n    CONVERSION[\"UTM zone 17N\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",-81,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"North America - between 84°W and 78°W - onshore and offshore. Canada - Nunavut; Ontario; Quebec. United States (USA) - Florida; Georgia; Kentucky; Maryland; Michigan; New York; North Carolina; Ohio; Pennsylvania; South Carolina; Tennessee; Virginia; West Virginia.\"],\n        BBOX[23.81,-84,84,-78]],\n    ID[\"EPSG\",26917]]\n\n\n\n#--- transform ---#\nnc_utm17N_2 &lt;- sf::st_transform(nc_wgs84, sf::st_crs(nc_utm17N))\n\n#--- check if the transformation was successful ---#\nsf::st_crs(nc_utm17N_2)\n\nCoordinate Reference System:\n  User input: EPSG:26917 \n  wkt:\nPROJCRS[\"NAD83 / UTM zone 17N\",\n    BASEGEOGCRS[\"NAD83\",\n        DATUM[\"North American Datum 1983\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4269]],\n    CONVERSION[\"UTM zone 17N\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",-81,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"North America - between 84°W and 78°W - onshore and offshore. Canada - Nunavut; Ontario; Quebec. United States (USA) - Florida; Georgia; Kentucky; Maryland; Michigan; New York; North Carolina; Ohio; Pennsylvania; South Carolina; Tennessee; Virginia; West Virginia.\"],\n        BBOX[23.81,-84,84,-78]],\n    ID[\"EPSG\",26917]]\n\n\nTurning dataframe into sf object\n\n#--- read irrigation well registration data ---#\n(\n  wells &lt;- readRDS(\"Data/well_registration.rds\")\n)\n\nKey: &lt;wellid&gt;\n        wellid ownerid        nrdname acres  regdate section     longdd\n         &lt;int&gt;   &lt;int&gt;         &lt;char&gt; &lt;num&gt;   &lt;char&gt;   &lt;int&gt;      &lt;num&gt;\n     1:      2  106106 Central Platte   160 12/30/55       3  -99.58401\n     2:      3   14133   South Platte    46  4/29/31       8 -102.62495\n     3:      4   14133   South Platte    46  4/29/31       8 -102.62495\n     4:      5   14133   South Platte    46  4/29/31       8 -102.62495\n     5:      6   15837 Central Platte   160  8/29/32      20  -99.62580\n    ---                                                                \n105818: 244568  135045 Upper Big Blue    NA  8/26/16      30  -97.58872\n105819: 244569  105428    Little Blue    NA  8/26/16      24  -97.60752\n105820: 244570  135045 Upper Big Blue    NA  8/26/16      30  -97.58294\n105821: 244571  135045 Upper Big Blue    NA  8/26/16      25  -97.59775\n105822: 244572  105428    Little Blue    NA  8/26/16      15  -97.64086\n           latdd\n           &lt;num&gt;\n     1: 40.69825\n     2: 41.11699\n     3: 41.11699\n     4: 41.11699\n     5: 40.73268\n    ---         \n105818: 40.89017\n105819: 40.13257\n105820: 40.88722\n105821: 40.89639\n105822: 40.13380\n\n\n\n#--- recognize it as an sf ---#\nwells_sf &lt;- sf::st_as_sf(wells, coords = c(\"longdd\", \"latdd\")) %&gt;% \n  st_set_crs(4269)\n\n#--- take a look at the data ---#\nhead(wells_sf[, 1:5])\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -102.6249 ymin: 40.69824 xmax: -99.58401 ymax: 41.11699\nGeodetic CRS:  NAD83\n  wellid ownerid        nrdname acres  regdate                   geometry\n1      2  106106 Central Platte   160 12/30/55 POINT (-99.58401 40.69825)\n2      3   14133   South Platte    46  4/29/31 POINT (-102.6249 41.11699)\n3      4   14133   South Platte    46  4/29/31 POINT (-102.6249 41.11699)\n4      5   14133   South Platte    46  4/29/31 POINT (-102.6249 41.11699)\n5      6   15837 Central Platte   160  8/29/32  POINT (-99.6258 40.73268)\n6      7   90248 Central Platte   120  2/15/35 POINT (-99.64524 40.73164)\n\n\nConversion to and from sp objects\n\n#--- conversion ---#\nwells_sp &lt;- as(wells_sf, \"Spatial\")\n\n#--- check the class ---#\nclass(wells_sp)\n\n[1] \"SpatialPointsDataFrame\"\nattr(,\"package\")\n[1] \"sp\"\n\n#--- revert back to sf ---#\nwells_sf &lt;- sf::st_as_sf(wells_sp)\n\n#--- check the class ---#\nclass(wells_sf)\n\n[1] \"sf\"         \"data.frame\"\n\n\nReverting sf object back into dataframe\n\n#--- remove geometry ---#\nwells_no_longer_sf &lt;- sf::st_drop_geometry(wells_sf)\n\n#--- take a look ---#\nhead(wells_no_longer_sf)\n\n  wellid ownerid        nrdname acres  regdate section\n1      2  106106 Central Platte   160 12/30/55       3\n2      3   14133   South Platte    46  4/29/31       8\n3      4   14133   South Platte    46  4/29/31       8\n4      5   14133   South Platte    46  4/29/31       8\n5      6   15837 Central Platte   160  8/29/32      20\n6      7   90248 Central Platte   120  2/15/35      19\n\n\n\n#--- read wells location data ---#\nurnrd_wells_sf &lt;-\n  readRDS(\"Data/urnrd_wells.rds\") %&gt;%\n  #--- project to UTM 14N WGS 84 ---#\n  sf::st_transform(32614)\n\n#--- create a one-mile buffer around the wells ---#\nwells_buffer &lt;- sf::st_buffer(urnrd_wells_sf, dist = 1600)\n\nhead(wells_buffer)\n\nSimple feature collection with 6 features and 6 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 251786.6 ymin: 4483187 xmax: 301596 ymax: 4499464\nProjected CRS: WGS 84 / UTM zone 14N\n  wellid ownerid          nrdname acres regdate section\n1     79   98705 Upper Republican   400 12/9/40       5\n2    109  115260 Upper Republican    80 3/11/41      20\n3    161  115260 Upper Republican    50  3/8/43      17\n4    162   41350 Upper Republican   140 3/23/43      28\n5    164  105393 Upper Republican    90  7/6/43      12\n6    166   51341 Upper Republican   110 8/31/43      18\n                        geometry\n1 POLYGON ((277371.9 4497864,...\n2 POLYGON ((256979.6 4484787,...\n3 POLYGON ((256993.1 4485173,...\n4 POLYGON ((258354.5 4492808,...\n5 POLYGON ((301596 4485998, 3...\n6 POLYGON ((254986.6 4485670,...\n\nggplot() +\n  geom_sf(data = urnrd_wells_sf, size = 0.6, color = \"red\") +\n  geom_sf(data = wells_buffer, fill = NA) +\n  theme_void()\n\n\n\n\n\n\n\n\n\nNE_counties &lt;-\n  readRDS(\"Data/NE_county_borders.rds\") %&gt;%\n  dplyr::filter(NAME %in% c(\"Perkins\", \"Dundy\", \"Chase\")) %&gt;%\n  sf::st_transform(32614)\n\n#--- generate area by polygon ---#\n(\n  NE_counties &lt;- dplyr::mutate(NE_counties, area = st_area(NE_counties))\n)\n\nSimple feature collection with 3 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 239494.1 ymin: 4430632 xmax: 310778.1 ymax: 4543676\nProjected CRS: WGS 84 / UTM zone 14N\n  STATEFP COUNTYFP COUNTYNS       AFFGEOID GEOID    NAME LSAD      ALAND\n1      31      135 00835889 0500000US31135 31135 Perkins   06 2287828025\n2      31      029 00835836 0500000US31029 31029   Chase   06 2316533447\n3      31      057 00835850 0500000US31057 31057   Dundy   06 2381956151\n   AWATER                       geometry             area\n1 2840176 MULTIPOLYGON (((243340.2 45... 2302174854 [m^2]\n2 7978172 MULTIPOLYGON (((241201.4 44... 2316908196 [m^2]\n3 3046331 MULTIPOLYGON (((240811.3 44... 2389890530 [m^2]\n\n\n\n#--- create centroids ---#\n(\n  NE_centroids &lt;- sf::st_centroid(NE_counties)\n)\n\nWarning: st_centroid assumes attributes are constant over geometries\n\n\nSimple feature collection with 3 features and 10 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 271156.7 ymin: 4450826 xmax: 276594.1 ymax: 4525635\nProjected CRS: WGS 84 / UTM zone 14N\n  STATEFP COUNTYFP COUNTYNS       AFFGEOID GEOID    NAME LSAD      ALAND\n1      31      135 00835889 0500000US31135 31135 Perkins   06 2287828025\n2      31      029 00835836 0500000US31029 31029   Chase   06 2316533447\n3      31      057 00835850 0500000US31057 31057   Dundy   06 2381956151\n   AWATER                 geometry             area\n1 2840176 POINT (276594.1 4525635) 2302174854 [m^2]\n2 7978172 POINT (271469.9 4489429) 2316908196 [m^2]\n3 3046331 POINT (271156.7 4450826) 2389890530 [m^2]\n\nggplot() +\n  geom_sf(data = NE_counties) +\n  geom_sf_text(data = NE_centroids, aes(label = NAME)) +\n  theme_void()\n\n\n\n\n\n\n\n\n\nIL_counties &lt;- sf::st_read(\"Data/IL_county_detailed.geojson\")\n\nReading layer `IL_county_detailed' from data source \n  `/home/himakun/Documents/tools/computing/Data/IL_county_detailed.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 102 features and 61 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -91.51309 ymin: 36.9703 xmax: -87.4952 ymax: 42.50849\nGeodetic CRS:  WGS 84\n\nIL_counties_mssimplified &lt;- rmapshaper::ms_simplify(IL_counties, keep = 0.01)\n\nCook &lt;- filter(IL_counties, NAME == \"Cook County\")\n\nCook_simplify &lt;- sf::st_simplify(Cook, dTolerance = 1000)\n\nggplot() +\n  geom_sf(data = Cook_simplify) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\nSpatial Interactions of Vector Data: Subsetting and Joining\n\n#--- create points ---#\npoint_1 &lt;- sf::st_point(c(2, 2))\npoint_2 &lt;- sf::st_point(c(1, 1))\npoint_3 &lt;- sf::st_point(c(1, 3))\n\n#--- combine the points to make a single  sf of points ---#\n(\npoints &lt;- \n  list(point_1, point_2, point_3) %&gt;% \n  sf::st_sfc() %&gt;% \n  sf::st_as_sf() %&gt;% \n  dplyr::mutate(point_name = c(\"point 1\", \"point 2\", \"point 3\"))\n)\n\nSimple feature collection with 3 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 1 ymin: 1 xmax: 2 ymax: 3\nCRS:           NA\n            x point_name\n1 POINT (2 2)    point 1\n2 POINT (1 1)    point 2\n3 POINT (1 3)    point 3\n\n\n\n#--- create points ---#\nline_1 &lt;- sf::st_linestring(rbind(c(0, 0), c(2.5, 0.5)))\nline_2 &lt;- sf::st_linestring(rbind(c(1.5, 0.5), c(2.5, 2)))\n\n#--- combine the points to make a single  sf of points ---#\n(\nlines &lt;- \n  list(line_1, line_2) %&gt;% \n  sf::st_sfc() %&gt;% \n  sf::st_as_sf() %&gt;% \n  dplyr::mutate(line_name = c(\"line 1\", \"line 2\"))\n)\n\nSimple feature collection with 2 features and 1 field\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 0 ymin: 0 xmax: 2.5 ymax: 2\nCRS:           NA\n                            x line_name\n1   LINESTRING (0 0, 2.5 0.5)    line 1\n2 LINESTRING (1.5 0.5, 2.5 2)    line 2\n\n\n\n#--- create polygons ---#\npolygon_1 &lt;- sf::st_polygon(list(\n  rbind(c(0, 0), c(2, 0), c(2, 2), c(0, 2), c(0, 0)) \n))\n\npolygon_2 &lt;- sf::st_polygon(list(\n  rbind(c(0.5, 1.5), c(0.5, 3.5), c(2.5, 3.5), c(2.5, 1.5), c(0.5, 1.5)) \n))\n\npolygon_3 &lt;- sf::st_polygon(list(\n  rbind(c(0.5, 2.5), c(0.5, 3.2), c(2.3, 3.2), c(2, 2), c(0.5, 2.5)) \n))\n\n#--- combine the polygons to make an sf of polygons ---#\n(\npolygons &lt;- \n  list(polygon_1, polygon_2, polygon_3) %&gt;% \n  sf::st_sfc() %&gt;% \n  sf::st_as_sf() %&gt;% \n  dplyr::mutate(polygon_name = c(\"polygon 1\", \"polygon 2\", \"polygon 3\"))\n)\n\nSimple feature collection with 3 features and 1 field\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 0 ymin: 0 xmax: 2.5 ymax: 3.5\nCRS:           NA\n                               x polygon_name\n1 POLYGON ((0 0, 2 0, 2 2, 0 ...    polygon 1\n2 POLYGON ((0.5 1.5, 0.5 3.5,...    polygon 2\n3 POLYGON ((0.5 2.5, 0.5 3.2,...    polygon 3\n\n\n\nggplot() +\n  geom_sf(data = polygons, aes(fill = polygon_name), alpha = 0.3) +\n  scale_fill_discrete(name = \"Polygons\") +\n  geom_sf(data = lines, aes(color = line_name)) +\n  scale_color_discrete(name = \"Lines\") + \n  geom_sf(data = points, aes(shape = point_name), size = 4) +\n  scale_shape_discrete(name = \"Points\")  \n\n\n\n\n\n\n\n\n\nsf::st_intersects(points, polygons)\n\nSparse geometry binary predicate list of length 3, where the predicate\nwas `intersects'\n 1: 1, 2, 3\n 2: 1\n 3: 2, 3\n\n\n\nset.seed(38424738)\n\npoints_set_1 &lt;-\n  lapply(1:5, function(x) sf::st_point(runif(2))) %&gt;% \n  sf::st_sfc() %&gt;% sf::st_as_sf() %&gt;% \n  dplyr::mutate(id = 1:nrow(.))\n\npoints_set_2 &lt;-\n  lapply(1:5, function(x) sf::st_point(runif(2))) %&gt;% \n  sf::st_sfc() %&gt;% sf::st_as_sf() %&gt;% \n  dplyr::mutate(id = 1:nrow(.))\n\n\nsf::st_is_within_distance(points_set_1, points_set_2, dist = 0.2)\n\nSparse geometry binary predicate list of length 5, where the predicate\nwas `is_within_distance'\n 1: 1\n 2: (empty)\n 3: (empty)\n 4: (empty)\n 5: 3\n\n\n\n#--- Kansas county borders ---#\nKS_counties &lt;-\n  readRDS(\"Data/KS_county_borders.rds\") %&gt;%\n  sf::st_transform(32614)\n\nold-style crs object detected; please recreate object with a recent sf::st_crs()\n\n#--- High-Plains Aquifer boundary ---#\nhpa &lt;- \n  sf::st_read(\"Data/hp_bound2010.shp\") %&gt;%\n  .[1, ] %&gt;%\n  sf::st_transform(st_crs(KS_counties))\n\nReading layer `hp_bound2010' from data source \n  `/home/himakun/Documents/tools/computing/Data/hp_bound2010.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 199 features and 1 field\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -805708.3 ymin: 983257.8 xmax: -21500.35 ymax: 2312747\nProjected CRS: NAD_1983_Albers\n\n#--- all the irrigation wells in KS ---#\nKS_wells &lt;- \n  readRDS(\"Data/Kansas_wells.rds\") %&gt;%\n  sf::st_transform(st_crs(KS_counties))\n\n#--- US railroads in the Mid West region ---#\nrail_roads_mw &lt;- sf::st_read(\"Data/mw_railroads.geojson\")\n\nReading layer `mw_railroads' from data source \n  `/home/himakun/Documents/tools/computing/Data/mw_railroads.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 100661 features and 2 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: -414024 ymin: 3692172 xmax: 2094886 ymax: 5446589\nProjected CRS: WGS 84 / UTM zone 14N\n\n\n\nhpa_cropped_to_KS &lt;- sf::st_crop(hpa, KS_wells)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n\n\ncounties_intersecting_hpa &lt;- KS_counties[hpa_cropped_to_KS, ]\n\nggplot() +\n  geom_sf(data = counties_intersecting_hpa) +\n  geom_sf(data = hpa_cropped_to_KS, fill = \"blue\", alpha = 0.3) +\n  theme_void()\n\n\n\n\n\n\n\n\n\ncounties_intersecting_hpa\n\nSimple feature collection with 56 features and 9 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 229253.5 ymin: 4094801 xmax: 690085.5 ymax: 4432561\nProjected CRS: WGS 84 / UTM zone 14N\nFirst 10 features:\n   STATEFP COUNTYFP COUNTYNS       AFFGEOID GEOID     NAME LSAD      ALAND\n2       20      075 00485327 0500000US20075 20075 Hamilton   06 2580958328\n4       20      189 00485056 0500000US20189 20189  Stevens   06 1883595654\n5       20      155 00485041 0500000US20155 20155     Reno   06 3251786677\n6       20      129 00485135 0500000US20129 20129   Morton   06 1889994372\n8       20      023 00484981 0500000US20023 20023 Cheyenne   06 2641501921\n9       20      089 00485009 0500000US20089 20089   Jewell   06 2356813568\n11      20      077 00485004 0500000US20077 20077   Harper   06 2075290355\n14      20      165 00485358 0500000US20165 20165     Rush   06 1858997546\n15      20      097 00485013 0500000US20097 20097    Kiowa   06 1871627782\n16      20      109 00485019 0500000US20109 20109    Logan   06 2779039721\n     AWATER                       geometry\n2   2893322 MULTIPOLYGON (((233615.1 42...\n4    464936 MULTIPOLYGON (((273665.9 41...\n5  42963060 MULTIPOLYGON (((546178.6 42...\n6    507796 MULTIPOLYGON (((229431.1 41...\n8   2738021 MULTIPOLYGON (((239494.1 44...\n9  11367163 MULTIPOLYGON (((542298.2 44...\n11  3899269 MULTIPOLYGON (((557559.9 41...\n14   544720 MULTIPOLYGON (((449120.4 42...\n15   596764 MULTIPOLYGON (((450693.8 41...\n16   274632 MULTIPOLYGON (((285802.7 43...\n\n# sf_1[sf_2, op = topological_relation_type]\n\n\nKS_corn_price &lt;-\n  KS_counties %&gt;%\n  dplyr::mutate(corn_price = seq(3.2, 3.9, length = nrow(.))) %&gt;%\n  dplyr::select(COUNTYFP, corn_price)\n\nKS_corn_price\n\nSimple feature collection with 105 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 229253.5 ymin: 4094801 xmax: 890007.5 ymax: 4434288\nProjected CRS: WGS 84 / UTM zone 14N\nFirst 10 features:\n   COUNTYFP corn_price                       geometry\n1       133   3.200000 MULTIPOLYGON (((806203.1 41...\n2       075   3.206731 MULTIPOLYGON (((233615.1 42...\n3       123   3.213462 MULTIPOLYGON (((544031.7 43...\n4       189   3.220192 MULTIPOLYGON (((273665.9 41...\n5       155   3.226923 MULTIPOLYGON (((546178.6 42...\n6       129   3.233654 MULTIPOLYGON (((229431.1 41...\n7       073   3.240385 MULTIPOLYGON (((717254.1 42...\n8       023   3.247115 MULTIPOLYGON (((239494.1 44...\n9       089   3.253846 MULTIPOLYGON (((542298.2 44...\n10      059   3.260577 MULTIPOLYGON (((804792.9 42...\n\n\n\n(\n  KS_wells_County &lt;- sf::st_join(KS_wells, KS_corn_price)\n)\n\nSimple feature collection with 37647 features and 4 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 230332 ymin: 4095052 xmax: 887329.7 ymax: 4433597\nProjected CRS: WGS 84 / UTM zone 14N\nFirst 10 features:\n   site    af_used COUNTYFP corn_price                 geometry\n1     1 232.099948      069   3.556731   POINT (372548 4153588)\n2     3  13.183940      039   3.449038 POINT (353698.6 4419755)\n3     5  99.187052      165   3.287500 POINT (486771.7 4260027)\n4     7   0.000000      199   3.644231 POINT (248127.1 4296442)\n5     8 145.520499      055   3.832692 POINT (349813.2 4215310)\n6     9   3.614535      143   3.799038 POINT (612277.6 4322077)\n7    11 188.423543      181   3.590385 POINT (267030.7 4381364)\n8    12  77.335960      177   3.550000 POINT (761774.6 4339039)\n9    15   0.000000      159   3.610577 POINT (560570.3 4235763)\n10   17 167.819034      069   3.556731 POINT (387315.1 4175007)\n\n\n\nKS_wells %&gt;% aggregate(KS_counties, FUN = sum)\n\nSimple feature collection with 105 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 229253.5 ymin: 4094801 xmax: 890007.5 ymax: 4434288\nProjected CRS: WGS 84 / UTM zone 14N\nFirst 10 features:\n       site      af_used                       geometry\n1    124453 1.701790e+01 MULTIPOLYGON (((806203.1 41...\n2  12514550 6.261704e+04 MULTIPOLYGON (((233615.1 42...\n3   1964254 1.737791e+03 MULTIPOLYGON (((544031.7 43...\n4  42442400 3.023589e+05 MULTIPOLYGON (((273665.9 41...\n5  68068173 6.110576e+04 MULTIPOLYGON (((546178.6 42...\n6  15756801 9.664610e+04 MULTIPOLYGON (((229431.1 41...\n7    149202 0.000000e+00 MULTIPOLYGON (((717254.1 42...\n8  17167377 5.750807e+04 MULTIPOLYGON (((239494.1 44...\n9   1809003 2.201696e+03 MULTIPOLYGON (((542298.2 44...\n10   160064 4.571102e+00 MULTIPOLYGON (((804792.9 42...\n\n\n\n# sf::st_join(sf_1, sf_2, join = \\(x, y) st_is_within_distance(x, y, dist = 5))\n\n# st_intersection() =&gt; does cropped join",
    "crumbs": [
      "Notes",
      "sf package"
    ]
  },
  {
    "objectID": "computing/gis.html",
    "href": "computing/gis.html",
    "title": "Geospatial with R",
    "section": "",
    "text": "This is my practice sections following blog posts by Andrew Heiss.",
    "crumbs": [
      "Notes",
      "Geospatial (R)"
    ]
  },
  {
    "objectID": "computing/gis.html#middle-earth-maps-with-r",
    "href": "computing/gis.html#middle-earth-maps-with-r",
    "title": "Geospatial with R",
    "section": "Middle earth maps with R",
    "text": "Middle earth maps with R\nMiddle earth maps with R\n\nQuick reminder: latitude vs. longitude\n\nlibrary(data.table)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(rnaturalearth)\nlibrary(scales)\nlibrary(patchwork)\nlibrary(leaflet)\nlibrary(glue)\n\npoint_example &lt;- tibble(x = 2, y = 1) %&gt;% \n  mutate(label = glue::glue(\"{x} x, {y} y\\n{y} lat, {x} lon\"))\nlat_labs &lt;- tibble(x = -3, y = seq(-2, 3, 1), label = \"Latitude\")\nlon_labs &lt;- tibble(x = seq(-2, 3, 1), y = -2, label = \"Longitude\")\n\nggplot() +\n  geom_point(data = point_example, aes(x = x, y = y), size = 5) +\n  geom_label(data = point_example, aes(x = x, y = y, label = label),\n            nudge_y = 0.6, family = \"Overpass ExtraBold\") +\n  geom_text(data = lat_labs, aes(x = x, y = y, label = label),\n            hjust = 0.5, vjust = -0.3, family = \"Overpass Light\") +\n  geom_text(data = lon_labs, aes(x = x, y = y, label = label),\n            hjust = 1.1, vjust = -0.5, angle = 90, family = \"Overpass Light\") +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  scale_x_continuous(breaks = seq(-2, 3, 1)) +\n  scale_y_continuous(breaks = seq(-2, 3, 1)) +\n  coord_equal(xlim = c(-3.5, 3), ylim = c(-3, 3)) +\n  labs(x = NULL, y = NULL) +\n  theme_minimal() +\n  theme(panel.grid.minor = element_blank(),\n        axis.text = element_blank()) \n\n\n\n\n\n\n\n\n\n\nStart the middle earth mapping\n\ncoastline &lt;- read_sf(\"data/ME-GIS/Coastline2.shp\") %&gt;% \n  mutate(across(where(is.character), ~iconv(., from = \"ISO-8859-1\", to = \"UTF-8\")))\n\ncontours &lt;- read_sf(\"data/ME-GIS/Contours_18.shp\") %&gt;% \n  mutate(across(where(is.character), ~iconv(., from = \"ISO-8859-1\", to = \"UTF-8\")))\n\nrivers &lt;- read_sf(\"data/ME-GIS/Rivers.shp\") %&gt;% \n  mutate(across(where(is.character), ~iconv(., from = \"ISO-8859-1\", to = \"UTF-8\")))\n\nroads &lt;- read_sf(\"data/ME-GIS/Roads.shp\") %&gt;% \n  mutate(across(where(is.character), ~iconv(., from = \"ISO-8859-1\", to = \"UTF-8\")))\n\nlakes &lt;- read_sf(\"data/ME-GIS/Lakes.shp\") %&gt;% \n  mutate(across(where(is.character), ~iconv(., from = \"ISO-8859-1\", to = \"UTF-8\")))\n\nregions &lt;- read_sf(\"data/ME-GIS/Regions_Anno.shp\") %&gt;% \n  mutate(across(where(is.character), ~iconv(., from = \"ISO-8859-1\", to = \"UTF-8\")))\n\nforests &lt;- read_sf(\"data/ME-GIS/Forests.shp\") %&gt;% \n  mutate(across(where(is.character), ~iconv(., from = \"ISO-8859-1\", to = \"UTF-8\")))\n\nmountains &lt;- read_sf(\"data/ME-GIS/Mountains_Anno.shp\") %&gt;% \n  mutate(across(where(is.character), ~iconv(., from = \"ISO-8859-1\", to = \"UTF-8\")))\n\nplacenames &lt;- read_sf(\"data/ME-GIS/Combined_Placenames.shp\") %&gt;% \n  mutate(across(where(is.character), ~iconv(., from = \"ISO-8859-1\", to = \"UTF-8\")))\n\n\nmiles_to_meters &lt;- function(x) {\n  x * 1609.344\n}\n\nmeters_to_miles &lt;- function(x) {\n  x / 1609.344\n}\n\nclr_green &lt;- \"#035711\"\nclr_blue &lt;- \"#0776e0\"\nclr_yellow &lt;- \"#fffce3\"\n\n# Format numeric coordinates with degree symbols and cardinal directions\nformat_coords &lt;- function(coords) {\n  ns &lt;- ifelse(coords[[1]][2] &gt; 0, \"N\", \"S\")\n  ew &lt;- ifelse(coords[[1]][1] &gt; 0, \"E\", \"W\")\n  \n  glue(\"{latitude}°{ns} {longitude}°{ew}\",\n       latitude = sprintf(\"%.6f\", coords[[1]][2]),\n       longitude = sprintf(\"%.6f\", coords[[1]][1]))\n}\n\n\nExploring the different layers\n\nggplot() +\n  geom_sf(data = coastline, linewidth = 0.25, color = \"grey50\")\n\n\n\n\n\n\n\n\n\n\nAdd rivers and lakes\n\nggplot() +\n  geom_sf(data = coastline, linewidth = 0.25, color = \"grey50\") +\n  geom_sf(data = rivers, linewidth = 0.2, color = clr_blue, alpha = 0.5) +\n  geom_sf(data = lakes, linewidth = 0.2, color = clr_blue, fill = clr_blue)\n\n\n\n\n\n\n\n\n\n\nPutting some labels\n\nplaces &lt;- placenames %&gt;% \n  filter(NAME %in% c(\"Hobbiton\", \"Rivendell\", \"Edoras\", \"Minas Tirith\"))\n\nggplot()+\n  geom_sf(data = coastline, linewidth = 0.25, color = \"grey50\") +\n  geom_sf(data = rivers, linewidth = 0.2, color = clr_blue, alpha = 0.5) +\n  geom_sf(data = lakes, linewidth = 0.2, color = clr_blue, fill = clr_blue) +\n  geom_sf(data = places, size = 1) +\n  geom_sf_label(data = places, aes(label = NAME), nudge_y = miles_to_meters(50))\n\n\n\n\n\n\n\n\n\n\nFancy map with lot of layers\n\nplaces &lt;- placenames %&gt;% \n  filter(NAME %in% c(\"Hobbiton\", \"Rivendell\", \"Edoras\", \"Minas Tirith\"))\n\nggplot() +\n  geom_sf(data = contours, linewidth = 0.15, color = \"grey90\") +\n  geom_sf(data = coastline, linewidth = 0.25, color = \"grey50\") +\n  geom_sf(data = rivers, linewidth = 0.2, color = clr_blue, alpha = 0.5) +\n  geom_sf(data = lakes, linewidth = 0.2, color = clr_blue, fill = clr_blue) +\n  geom_sf(data = forests, linewidth = 0, fill = clr_green, alpha = 0.5) +\n  geom_sf(data = mountains, linewidth = 0.25, linetype = \"dashed\") +\n  geom_sf(data = places) +\n  geom_sf_label(data = places, aes(label = NAME), nudge_y = miles_to_meters(40),\n                family = \"Overpass ExtraBold\", fontface = \"plain\") +\n  theme_void() +\n  theme(plot.background = element_rect(fill = clr_yellow))\n\n\n\n\n\n\n\n\n\n\nFocusing on Shire\n\nhobbiton &lt;- places %&gt;% \n  filter(NAME == \"Hobbiton\") %&gt;% \n  mutate(geometry_x = map_dbl(geometry, ~as.numeric(.)[1]),\n         geometry_y = map_dbl(geometry, ~as.numeric(.)[2]))\n\nhobbiton %&gt;% \n  select(LAYER, NAME, geometry_x, geometry_y)\n\nSimple feature collection with 1 feature and 4 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 515948 ymin: 1043820 xmax: 515948 ymax: 1043820\nProjected CRS: UTM_Zone_31_Northern_Hemisphere\n# A tibble: 1 × 5\n  LAYER     NAME     geometry_x geometry_y         geometry\n  &lt;chr&gt;     &lt;chr&gt;         &lt;dbl&gt;      &lt;dbl&gt;      &lt;POINT [m]&gt;\n1 TownNames Hobbiton    515948.   1043820. (515948 1043820)\n\nshire_towns &lt;- placenames %&gt;% filter(LAYER == \"TownNames\")\n\nggplot() +\n  geom_sf(data = rivers, linewidth = 0.45, color = clr_blue, alpha = 0.5) +\n  geom_sf(data = roads) +\n  geom_sf(data = shire_towns, size = 2) +\n  geom_sf_label(data = shire_towns, aes(label = NAME), nudge_y = miles_to_meters(3),\n                family = \"Overpass ExtraBold\", fontface = \"plain\") +\n  coord_sf(xlim = c(hobbiton$geometry_x - miles_to_meters(30), \n                    hobbiton$geometry_x + miles_to_meters(60)),\n           ylim = c(hobbiton$geometry_y - miles_to_meters(35), \n                    hobbiton$geometry_y + miles_to_meters(20)))\n\n\n\n\n\n\n\n\n\n\nFancy map of Shire\n\nlibrary(ggspatial) \n\nshire_towns &lt;- placenames %&gt;% filter(LAYER == \"TownNames\")\n\nggplot() +\n  geom_sf(data = roads, aes(linewidth = TYPE), color = \"grey80\") +\n  geom_sf(data = coastline, linewidth = 0.25, color = \"grey50\") +\n  geom_sf(data = rivers, linewidth = 0.45, color = clr_blue, alpha = 0.5) +\n  geom_sf_text(data = rivers, aes(label = NAME), color = clr_blue,\n               family = \"Overpass SemiBold\", fontface = \"italic\", size = 3.5) +\n  geom_sf_text(data = regions, aes(label = name),\n               family = \"Overpass Heavy\", size = 5, color = \"grey30\") +\n  geom_sf(data = forests, linewidth = 0, fill = clr_green, alpha = 0.4) +\n  geom_sf_text(data = forests, aes(label = NAME), nudge_y = miles_to_meters(1),\n               color = clr_green, family = \"Overpass ExtraBold\", fontface = \"italic\", size = 4) +\n  geom_sf(data = shire_towns, size = 2) +\n  geom_sf_label(data = shire_towns, aes(label = NAME), nudge_y = miles_to_meters(3),\n                family = \"Overpass ExtraBold\", fontface = \"plain\") +\n  scale_linewidth_discrete(range = c(1, 0.3), guide = \"none\") +\n  annotation_scale(location = \"tl\", bar_cols = c(\"grey30\", \"white\"),\n                   text_family = \"Overpass\",\n                   unit_category = \"imperial\") +\n  annotation_north_arrow(\n    location = \"tl\", pad_y = unit(1.5, \"lines\"),\n    style = north_arrow_fancy_orienteering(fill = c(\"grey30\", \"white\"), \n                                           line_col = \"grey30\",\n                                           text_family = \"Overpass\")) +\n  coord_sf(xlim = c(hobbiton$geometry_x - miles_to_meters(30), \n                    hobbiton$geometry_x + miles_to_meters(60)),\n           ylim = c(hobbiton$geometry_y - miles_to_meters(35), \n                    hobbiton$geometry_y + miles_to_meters(20))) +\n  labs(title = \"The Shire\") +\n  theme_void() +\n  theme(plot.background = element_rect(fill = clr_yellow),\n        plot.title = element_text(family = \"Aniron\", size = rel(2), \n                                  hjust = 0.02))\n\n\n\n\n\n\n\n\n\n\nDistances between places\n\nrivendell &lt;- places %&gt;% filter(NAME == \"Rivendell\")\n\nst_distance(hobbiton, rivendell) %&gt;% meters_to_miles()\n\nUnits: [m]\n         [,1]\n[1,] 229.0673",
    "crumbs": [
      "Notes",
      "Geospatial (R)"
    ]
  },
  {
    "objectID": "computing/gis.html#how-to-use-a-histogram-as-a-legend-in-ggplot2",
    "href": "computing/gis.html#how-to-use-a-histogram-as-a-legend-in-ggplot2",
    "title": "Geospatial with R",
    "section": "How to use a histogram as a legend in {ggplot2}",
    "text": "How to use a histogram as a legend in {ggplot2}\nCite: link",
    "crumbs": [
      "Notes",
      "Geospatial (R)"
    ]
  },
  {
    "objectID": "computing/blp.html",
    "href": "computing/blp.html",
    "title": "BLP Demystified: From Basics to Brain-Busting Models!",
    "section": "",
    "text": "Install necessary julia packages for later computation:\n# You might need these commented codes to install packages\n# using Pkg\n# Pkg.add([\"DataFrames\", \"CSV\", \"GLM\", \"Statistics\", \"LinearAlgebra\", \"Distributions\", \"NLopt\", \"FixedEffectModels\", \"RegressionTables\", \"DataFramesMeta\", \"Random\", \"StatsModels\", \"Optim\", \"StatsBase\"])",
    "crumbs": [
      "Notes",
      "BLP code (Julia)"
    ]
  },
  {
    "objectID": "computing/blp.html#motivation",
    "href": "computing/blp.html#motivation",
    "title": "BLP Demystified: From Basics to Brain-Busting Models!",
    "section": "1.1 Motivation",
    "text": "1.1 Motivation\nWhy do this? Demand estimation is very important in IO literature because measuring market power is important in IO. How do we quantify market power? Usually we use markup as the measure. But it is hard to directly calculate markup because it depends on the cost function of the firm which is not observed. But IO theory shows that we can actually get the markup using demand elasticity. Thus estimating demand is important.",
    "crumbs": [
      "Notes",
      "BLP code (Julia)"
    ]
  },
  {
    "objectID": "computing/blp.html#basic-mcfadden74-style-logit-model",
    "href": "computing/blp.html#basic-mcfadden74-style-logit-model",
    "title": "BLP Demystified: From Basics to Brain-Busting Models!",
    "section": "1.2 Basic: McFadden (1974) style logit model",
    "text": "1.2 Basic: McFadden (1974) style logit model\n\n1.2.1 Model setup\nWe will first estimate a basic logit model with no unobserved demand shifters and no random coefficents. But let’s just talk bit about the background of this discrete choice model. Note that most of it is from Train (2009).\nEven before McFadden (1974), there has been a long history of the development of the logit model. But McFadden (1974) provides a complete, well-defined econometric model that is consistent with the utility maximization behavior of individuals.\nIndividual’s (\\(i\\)) utility maximizing behavior (indirect utility) can be specified as follows:\n\\[\nu_{ij} = \\underbrace{x_j \\beta + \\alpha p_j}_{\\delta_j} + \\varepsilon_{ij}\n\\]\nwhere mean utility of outside option is normalized to zero. Also, idiosyncratic shock (i.i.d) follows Type 1 Extreme Value distribution (T1EV). We also assume there are \\(0, \\ldots, J\\) products (denote \\(0\\) as the outside option) where one option is outside option. We can think of \\(\\delta_j\\) as the mean utility from the product \\(j\\). This is because in this parameterization, \\(\\delta_j\\) does not depend on \\(i\\).\nNow let’s do some math to derive the logit choice probabilities. One benefit about logit model is that we can get a close-form solution. We are going to compute the probability of individuals choosing product \\(j\\) given \\(p_j\\), and \\(x_j\\).\n\\[\\begin{align}\n  P (u_{ij} \\geq \\forall_{j' \\neq j} u_{ij'} \\mid x_j, p_j) &= P (x_j \\beta + \\alpha p_j + \\varepsilon_{ij} \\geq \\forall_{j' \\neq j} x_{j'}\\beta + \\alpha p_{j'} + \\varepsilon_{ij'} \\mid x_j, p_j) \\\\\n  &= P ( \\varepsilon_{ij'} \\leq \\varepsilon_{ij} + \\delta_j - \\delta_{j'} \\, \\forall j' \\neq j).\n\\end{align}\\]\nIf we assume that \\(\\varepsilon_{ij}\\) is given, we can think of the last term as the cumulative distribution of the T1EV where \\(F(\\varepsilon_{ij}) = e^{-e^{- \\varepsilon_{ij}}}\\). Since we assumed i.i.d., we can express the last term as the product of the individual cumulative distributions (For brevity, we will now denote the conditional logit choice probability as \\(P_{ij}\\)):\n\\[\n  P_{ij} \\mid \\varepsilon_{ij} = \\prod_{j' \\neq j} e^{ - e^{-(\\varepsilon_{ij} + \\delta_j - \\delta_{j'})}}.\n\\]\nSince \\(\\varepsilon_{ij}\\) is not given, we need o integrate it over density of \\(\\varepsilon_{ij}\\):\n\\[\n  P_{ij} = \\int \\left(\\prod_{j' \\neq j} e^{ - e^{-(\\varepsilon_{ij} + \\delta_j - \\delta_{j'})}} \\right) e^{- \\varepsilon_{ij}} e^{-e^{\\varepsilon_{ij}}} d \\varepsilon_{ij}.\n\\]\nNow let’s get this into a closed-form expression:\nAs a result, we can get the closed-form expression:\n\\[\n  P_{ij} = \\frac{e^{\\delta_{ij}}}{\\sum_{j'} e^{\\delta_{ij'}}}\n\\]\nThis could be understood as the predicted share function given the fixed values of the parameters.\nNote that this is a very simple model because we are not assuming any unobserved product demand shifters that could be affected the utility gained from the product. In fact, we are assuming that econometricians can fully observe all the necessary variables that constructs the mean utility. Thus there is not much econometrics involved. You can just get the parameters as follows:\n\nAssuming you have the data on market share, you can use it to match it to \\(P_{ij} \\cdot M\\) where \\(M\\)is the total market size.\nThen since we will get \\(J\\) equations using \\(J\\) market share, we can do simple algebra to get the mean utility \\(\\delta_j\\).\nThen you can do some nonlinear least squares that minimize the sum of the differences between oberved and predicted shares of all products. This will get you the parameters that best fit the data.\n\n\n\n1.2.2 Adding unobserved demand shifters\nWe can add the additional unobserved variables \\(\\xi_j\\) which can be thought of as some demand shifter for product \\(j\\). This allows the model to be more flexible to incorporate the realistic situation where econometrician might not be able to observe some components that might be affecting the utility of getting some product. Thus most of what we did above does not change much. The only problem would be understanding the nature of this unobserved terms with other main parameters of interest. If there is endogeneity, we would need some IV to estimate the parameter. In this section, we will do both cases (OLS, IV).\n\n\n1.2.3 Computation (Following Berry (1994))\nSo how can we retrieve the parameters of interest? Naive way to think about it would be doing some nonlinear least squares where you minimize the sum of differences between predicted share and observed shares of all products. The problem is that this directy way is implausible: You would need to know the \\(\\xi_j\\). Since this is unobservable, it is problematic.\nThis is where Berry (1994) comes in. He introduces this clever two steps estimation process.\nStep 1: Inversion\nNotation: Let \\(\\hat{s}_j (\\delta)\\) be predicted shares and let \\(s_j\\) be observed shares.1\nThen you can use the system of equations from matching actual to predicted shares and invert them to get the mean utility. For this simple case, we can get the following equations:\n\\[\n  \\delta_j = \\log s_j - \\log \\hat{s}_0, \\quad j = 1, \\ldots, J.\n\\]\nSo this inversion gets us the value of the mean utility. Then we have the second step.\nStep 2: IV estimation\nBy definition, we have \\(\\delta_j = x_j \\beta + \\alpha p_j + \\xi_j\\). So we can do the regression to retrieve the parameters. I put IV, but this could be just OLS if you can assume the unobserved term is uncorrelated with the price.\n\n\n1.2.4 Coding (with Julia)\nFinally we will do some coding to get the result we just talked about.\n\nusing FixedEffectModels, DataFrames, CSV, RegressionTables \n\n# read in the data\notc = CSV.read(\"data/otc.csv\", DataFrame)\n\n# Run regressions\nols1 = reg(otc, @formula(ln_mkt_share_diff ~ price + promotion)) \nols2 = reg(otc, @formula(ln_mkt_share_diff ~ price + promotion + fe(product)))\nols3 = reg(otc, @formula(ln_mkt_share_diff ~ price + promotion + fe(product) + fe(store)))\niv1 = reg(otc, @formula(ln_mkt_share_diff ~ (price ~ cost) + promotion))\niv2 = reg(otc, @formula(ln_mkt_share_diff ~ (price ~ cost) + promotion + fe(product)))\n\nregtable(ols1, ols2, ols3, iv1, iv2, order = [\"price\"], drop = [\"(Intercept)\"], regression_statistics = [FStatIV, Nobs, R2],\n  labels = Dict(\n    \"price\" =&gt; \"Price\",\n    \"promotion\" =&gt; \"Promotion\",\n    \"ln_mkt_share_diff\" =&gt; \"Log Mkt share difference\"\n  ))\n## Some R codes that I followed\n\n# m1 &lt;- lm(ln_mkt_share_diff ~ price + promotion , data = otc)\n# m2 &lt;- lm(ln_mkt_share_diff ~ price + promotion + factor(product), data = otc)\n# m3 &lt;- lm(ln_mkt_share_diff ~ price + promotion + factor(product) + factor(store), data = otc)\n# m4 &lt;- ivreg(ln_mkt_share_diff ~ price + promotion | . - price + cost, data = otc)\n# m5 &lt;- ivreg(ln_mkt_share_diff ~ price + promotion + factor(product) | . - price + cost, data = otc)\n# stargazer(m1, m2, m3, m4, m5, \n#           omit = c(\"product\", \"store\"),\n#           type = \"text\")\n\n\n-----------------------------------------------------------------------------\n                                        Log Mkt share difference             \n                          ---------------------------------------------------\n                              (1)        (2)       (3)         (4)        (5)\n-----------------------------------------------------------------------------\nPrice                       0.020   -0.189**   -0.145*    0.069***      0.169\n                          (0.014)    (0.059)   (0.059)     (0.015)    (0.115)\nPromotion                   0.121     0.187*   0.201**       0.149   0.308***\n                          (0.093)    (0.074)   (0.073)     (0.093)    (0.082)\n-----------------------------------------------------------------------------\nproduct Fixed Effects                    Yes       Yes                    Yes\nstore Fixed Effects                                Yes                       \n-----------------------------------------------------------------------------\nEstimator                     OLS        OLS       OLS          IV         IV\n-----------------------------------------------------------------------------\nControls                      Yes                              Yes           \n-----------------------------------------------------------------------------\nFirst-stage F statistic                                  8,147.921    394.113\nN                           1,056      1,056     1,056       1,056      1,056\nR2                          0.003      0.440     0.456      -0.008      0.420\n-----------------------------------------------------------------------------\n\n\n\n\n\n1.2.5 Caveats\nBut we don’t usually use this basic setup in IO. This is because the model is bit too simple to fully capture the reality. One of the well known problem is the Independence of irrelevant alternatives (IIA). Basically what this means is that we don’t get a realistic demand elasticities. If you want to know more about it, google the famouse Red bus, blue bus story.\n\n\n1.2.6 Solutions?\nThere are some ways to alleviate this problem. One of them (which we will not discuss), is using nested logit. Basically we are defining certain group of products where IIA holds within the group but may not hold across the group. So for the case of red bus, blue bus, they would be in a same group.\nAnother way is to do enhance the random utility model into logit model with random coefficients. In essence, this is sort of introducing preference heterogeneity of consumers into the model. This is done by interacting consumer preferences with product characteristics. The nuisance with this case is that now closed-form expression for choice probability is not obtainable. We need to do some numerical computation.",
    "crumbs": [
      "Notes",
      "BLP code (Julia)"
    ]
  },
  {
    "objectID": "computing/blp.html#advanced-blp-random-coefficients-logit-model",
    "href": "computing/blp.html#advanced-blp-random-coefficients-logit-model",
    "title": "BLP Demystified: From Basics to Brain-Busting Models!",
    "section": "1.3 Advanced: Berry, Levinsohn, and Pakes (1995) (Random coefficients logit model)",
    "text": "1.3 Advanced: Berry, Levinsohn, and Pakes (1995) (Random coefficients logit model)\nWe again start with the individual utility function. But now something is added (we will now also explicitly denote markets as \\(t\\)):\n\\[\nu_{ijt} = x_{jt} \\beta_{it} + \\alpha p_{jt} + \\xi_{jt} + \\varepsilon_{ijt}\n\\]\nThe difference is that slope coefficients can now vary across individuals \\(i\\). For now, we will assume \\(\\beta_{it}^k = \\beta_0^k + \\sigma_{kt} v_{it}^k\\). We now have \\(k\\) which is the dimension of \\(\\beta\\). \\(\\beta_0^k\\) are fixed taste for characteristics \\(k\\) and \\(v_{it}^k\\) are random tastes that follow standard normal distribution.\nNow we can expand the model:\n\\[\\begin{align}\n  u_{ijt} &= (x_{j1t}, \\ldots, x_{jKt}) \\cdot (\\beta_{0}^1 + \\sigma_1 v_{it}^1, \\ldots, \\beta_{0}^K + \\sigma_K v_{it}^K)^T + \\alpha p_{jt} + \\xi_{jt} + \\varepsilon_{ijt}\\\\\n  &= x_{jt}\\beta_{it} + \\sum_k x_{jkt} \\sigma_{k}v_{ikt} + \\alpha p_{jt} + \\xi_{jt} + \\varepsilon_{ijt}\\\\\n  &= \\underbrace{x_{jt}\\beta_{it} + \\alpha p_{jt} + \\xi_{jt}}_{\\delta_{jt}} + \\underbrace{\\sum_k x_{jkt} \\sigma_{k}v_{ikt}}_{\\mu_{ijt}} +  \\varepsilon_{ijt}.\n\n\\end{align}\\]\nWe can easily see that this is just an extension of what we did for the basic random utility model. Indirect utility is made up of mean utility \\(\\delta_{jt}\\) and random coefficient term \\(\\mu_{ijt} + \\varepsilon_{ijt}\\).\nNow we will make some simplication. We will assume that characteristics dimension of individual is one: \\(K = 1\\). Using this simplication, we can again use the assumption that idiosyncratic shock follows T1EV to get aggregate share:\n\\[\ns_{jt} = \\int \\frac{\\exp(\\delta_{jt} + x_{jt} \\sigma_t v_{it})}{1 + \\sum_j \\exp(\\delta_{jt} + x_{jt} \\sigma_t v_{it})} f(v_i)dv_i\n\\]\nThe integral has no analytical solution in the random coefficient model, so we need to compute the integral by simulation. One way to do it is as follows:\n\\[\n\\hat{s}_{jt} = \\frac{1}{ns} \\sum_{i=1}^{ns} \\frac{\\exp(\\delta_{jt} + x_{jt} \\sigma_t v_{it})}{1 + \\sum_j \\exp(\\delta_{jt} + x_{jt} \\sigma_t v_{it})}\n\\]\nwhere \\(ns\\) is number of random draws from \\(v_i\\).\nNow we can see the inversion method we did before is not easy to implement. This is because we now have additional parameters that we do not know the values.\nSo in BLP, we need to do nested estimation algorithm.\n\nIn the outer loop, we iterate over different values of the parameters.\nIn the inner loop, for a given parameter value, we do the inversion to get the mean utility and estimate the GMM objective function.\nWe keep on doing this iteration until we get the parameters that minimize the GMM function.\n\nNow let’s do some coding!\n\n1.3.1 Another coding (with Julia)\nThis portion of the code is from here\n\n###############################################################################\n####  BLP fixed-point algorithm, inverting mkt shares to get mean utility  ####\n###############################################################################\nusing CSV\nusing DataFrames\nusing GLM\nusing Statistics\nusing LinearAlgebra\nusing Distributions\nusing NLopt\notc = CSV.read(\"data/otc.csv\", DataFrame)\n\nns = 500;\nnmkt = maximum(otc.mkt);\nmkt = unique(otc.mkt);\nnprod = maximum(otc.product);\n\nvi = quantile.(Normal(), collect(range(0.5/ns, step = 1/ns, length = ns)));\nsigma = 1;\n\nfunction calc_mkt_share_t(delta_t, sigma_t, x_t, vi_t)\n    # Dimension: delta_t 11*1, simga_t 1*1, x_t 11*1\n    delta_t = delta_t .* ones(nprod, ns)\n    mu_t = x_t*sigma_t*vi_t'\n    numerator = exp.(delta_t .+ mu_t)\n    denominator = ones(nprod, ns) .+ sum(numerator, dims = 1)\n    mkt_share_t = mean(numerator./denominator, dims = 2)\nend\n\nfunction contraction_t(d0, sigma_t, x_t, vi_t, mkt_t, tol = 1e-5, maxiter = 1e5)\n    obs_mkt_share_t = mkt_t.mkt_share\n    d_old = d0\n    normdiff = Inf\n    iter = 0\n    while normdiff &gt; tol && iter &lt;= maxiter\n        model_mkt_share_t = calc_mkt_share_t(d_old, sigma_t, x_t, vi_t)\n        d_new = d_old .+ log.(obs_mkt_share_t) .- log.(model_mkt_share_t)\n        normdiff = maximum(norm.(d_new .- d_old))\n        d_old = d_new\n        iter += 1\n    end\n    return d_old\nend\n\nfunction calc_delta(sigma)\n    delta_fp = zeros(nprod, nmkt);\n    for t in mkt\n        mkt_t = otc[otc.mkt .== t, :];\n        x_t = ones(nprod, 1);\n        delta_t = zeros(nprod, 1);\n        sigma_t = sigma;\n        vi_t = vi;\n        delta_fp[:, t] = contraction_t(delta_t, sigma_t, x_t, vi_t, mkt_t);\n    end\n    return vec(delta_fp);\nend\n\n@time delta_fp = calc_delta(sigma);\nmean(delta_fp)\nstd(delta_fp)\n\n  1.348311 seconds (3.75 M allocations: 365.638 MiB, 3.21% gc time, 96.17% compilation time)\n\n\n0.8537059827877238\n\n\n\n################################################################\n#### Estimate beta and sigma using GMM (cost as instrument) ####\n################################################################\nX = hcat(ones(nprod*nmkt, 1),\n         otc.price, otc.promotion,\n         otc.product_2, otc.product_3, otc.product_4, otc.product_5,\n         otc.product_6, otc.product_7, otc.product_8, otc.product_9,\n         otc.product_10, otc.product_11);\nz = hcat(X, otc.cost);\nPhi = z'*z/1056;\ninv_Phi = inv(Phi);\n\nfunction GMMObjFunc(theta2::Vector, grad::Vector)\n    sigma = theta2[1]\n    delta = calc_delta(sigma)\n    theta1 = inv(X'*z*inv_Phi*z'*X)*X'*z*inv_Phi*z'*delta\n    error = delta - X*theta1\n    obj = error'*z*inv_Phi*z'*error\n    return obj\nend\n\nopt = Opt(:LN_COBYLA, 1)\nopt.xtol_rel = 1e-4\nopt.lower_bounds = [0.00001]\nopt.min_objective = GMMObjFunc\n@time (minf,minx,ret) = optimize(opt, [1])\n\n@show sigma = minx[1]\ndelta = calc_delta(sigma[1]);\ntheta1 = inv(X'*z*inv_Phi*z'*X)*X'*z*inv_Phi*z'*delta\n\n 18.817741 seconds (20.18 M allocations: 109.887 GiB, 14.52% gc time, 0.73% compilation time)\nsigma = minx[1] = 28.720209709193362\n\n\n13-element Vector{Float64}:\n -71.52085329511755\n  -1.0852292863458644\n   0.22206483339059624\n   1.903952993172239\n   3.990433860485858\n  -0.6194995526125144\n   1.2498418962356002\n   3.9120756908464607\n  -2.12500554825884\n  -1.1737916667530266\n   0.2462099284926299\n  -2.7352849052497947\n   0.011924728064080292\n\n\n\n\n1.3.2 My personal coding (again Julia)\nI will try to write my personal code using Julia following the exercises in Mixtape session for Demand Estimation. Note that I will be just copying the exercise questions from that site below. Also, I am going to use Julia.",
    "crumbs": [
      "Notes",
      "BLP code (Julia)"
    ]
  },
  {
    "objectID": "computing/blp.html#introduction",
    "href": "computing/blp.html#introduction",
    "title": "BLP Demystified: From Basics to Brain-Busting Models!",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\nThis is the first of three exercises that will give you a solid foundation for doing BLP-style estimation. The running example is the same as in lecture: what if we halved an important product’s price?",
    "crumbs": [
      "Notes",
      "BLP code (Julia)"
    ]
  },
  {
    "objectID": "computing/blp.html#setup",
    "href": "computing/blp.html#setup",
    "title": "BLP Demystified: From Basics to Brain-Busting Models!",
    "section": "2.2 Setup",
    "text": "2.2 Setup\nMost of the computational heavy-lifting in these exercises will be done by the open-source Python package PyBLP. It is easiest to use PyBLP in Python, and the hints/solutions for the exercises will be given in Python. But for those who are more familiar with R, it is straightforward to call PyBLP from R with the reticulate package. It is technically possible to call PyBLP from other languages like Julia and MATLAB, but most users either use Python or R.\nYou should install PyBLP on top of the Anaconda Distribution. Anaconda comes pre-packaged with all of PyBLP’s dependencies and many more Python packages that are useful for statistical computing. Steps:\n\nInstall Anaconda if you haven’t already. You may wish to create a new environment for just these exercises, but this isn’t strictly necessary.\nInstall PyBLP. On the Anaconda command line, you can run the command pip install pyblp.\n\nIf you’re using Python, you have two broad options for how to do the coding exercises.\n\nUse a Jupyter Notebook. The solutions to each exercise will be in a notebook. In general, notebooks are a good way to weave text and code for short exercises, and to distribute quick snippets of code with others.\nUse an integrated development environment (IDE). Once you get beyond a few hundred lines of code, I strongly recommend using an IDE and not notebooks. For Python, I recommend VS Code or PyCharm. The former is free and the latter has a free community edition with all the features you’ll need for standard Python development. Both integrate well with Anaconda.\n\nIf using a notebook, you can right click and save the following notebook template: notebook.ipynb. If using an IDE, you can right click and save the following script template: script.py. Both import various packages used throughout the exercise.\n#| eval: false\n \nimport pyblp\nimport numpy as np\nimport pandas as pd\nimport statsmodels.formula.api as smf\nThe notebook additionally configures these packages to reduce the amount of information printed to the screen.\n#| eval: false\n \npyblp.options.digits = 3\npyblp.options.verbose = False\npd.options.display.precision = 3\npd.options.display.max_columns = 50\n\nimport IPython.display\nIPython.display.display(IPython.display.HTML('&lt;style&gt;pre { white-space: pre !important; }&lt;/style&gt;'))\nFinally, both show how to load the data that we’ll be using today.",
    "crumbs": [
      "Notes",
      "BLP code (Julia)"
    ]
  },
  {
    "objectID": "computing/blp.html#data",
    "href": "computing/blp.html#data",
    "title": "BLP Demystified: From Basics to Brain-Busting Models!",
    "section": "2.3 Data",
    "text": "2.3 Data\nToday, you’ll use the products.csv dataset, which is a simplified version of Nevo’s (2000) fake cereal data with less information and fewer derived columns. The data were motivated by real grocery store scanner data, but due to the proprietary nature of this type of data, the provided data are not entirely real. This dataset has been used as a standard example in much of the literature on BLP estimation.\nCompared to typical datasets you might use in your own work, the number of observations in this example dataset is quite small. This helps with making these exercises run very fast, but in practice one would want more data than just a couple thousand data points to estimate a flexible model of demand. Typical datasets will also include many more product characteristics. This one only includes a couple to keep the length of the exercises manageable.\nThe data contains information about 24 breakfast cereals across 94 markets. Each row is a product-market pair. Each market has the same set of breakfast cereals, although with different prices and quantities. The columns in the data are as follows.\n\n\n\n\n\n\n\n\nColumn\nData Type\nDescription\n\n\n\n\nmarket\nString\nThe city-quarter pair that defines markets \\(t\\) used in these exercises. The data were motivated by real cereal purchase data across 47 US cities in the first 2 quarters of 1988.\n\n\nproduct\nString\nThe firm-brand pair that defines products \\(j\\) used in these exercises. Each of 5 firms produces between 1 and 9 brands of cereal.\n\n\nmushy\nBinary\nA dummy product characteristic equal to one if the product gets soggy in milk.\n\n\nservings_sold\nFloat\nTotal quantity \\(q_{jt}\\) of servings of the product sold in a market, which will be used to compute market shares.\n\n\ncity_population\nFloat\nTotal population of the city, which will be used to define a market size.\n\n\nprice_per_serving\nFloat\nThe product’s price \\(p_{jt}\\) used in these exercises.\n\n\nprice_instrument\nFloat\nAn instrument to handle price endogeneity in these exercises. Think of it as a cost-shifter, a Hausman instrument, or any other valid IV that we discussed in class.\n\n\n\nThroughout the exercises, we use these data to estimate an increasingly flexible BLP-style model of demand for cereal. We will use predictions from this model to see how our running example, cutting the price of one cereal, affects demand for that cereal and for its substitutes.",
    "crumbs": [
      "Notes",
      "BLP code (Julia)"
    ]
  },
  {
    "objectID": "computing/blp.html#describe-the-data",
    "href": "computing/blp.html#describe-the-data",
    "title": "BLP Demystified: From Basics to Brain-Busting Models!",
    "section": "3.1 1. Describe the data",
    "text": "3.1 1. Describe the data\nYou can download products.csv from this link. To load it, you can use pd.read_csv. To look at a random sample of its rows, you can use .sample. To compute summary statistics for different columns, you can use .describe. Throughout these exercises, you’ll be given links to functions and methods that can be used to answer the questions. If you’re unsure about how to use them, you should click on the link, where there is typically example code lower down on the page.\n\n\nusing CSV, DataFrames, DataFramesMeta, StatsModels, LinearAlgebra, Random, Statistics, Distributions\n\ndf = CSV.read(\"data/raw/products.csv\", DataFrame) \n\ndescribe(df)\n\n7×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nDataType\n\n\n\n\n1\nmarket\n\nC01Q1\n\nC65Q2\n0\nString7\n\n\n2\nproduct\n\nF1B04\n\nF6B18\n0\nString7\n\n\n3\nmushy\n0.333333\n0\n0.0\n1\n0\nInt64\n\n\n4\nservings_sold\n1.20296e6\n4085.41\n3.62285e5\n9.85623e7\n0\nFloat64\n\n\n5\ncity_population\n641995.0\n173072\n332943.0\n7322564\n0\nInt64\n\n\n6\nprice_per_serving\n0.12574\n0.0454871\n0.123829\n0.225728\n0\nFloat64\n\n\n7\nprice_instrument\n0.0908116\n-0.00265638\n0.0890101\n0.188043\n0\nFloat64",
    "crumbs": [
      "Notes",
      "BLP code (Julia)"
    ]
  },
  {
    "objectID": "computing/blp.html#compute-market-shares",
    "href": "computing/blp.html#compute-market-shares",
    "title": "BLP Demystified: From Basics to Brain-Busting Models!",
    "section": "3.2 2. Compute market shares",
    "text": "3.2 2. Compute market shares\nTo transform observed quantities \\(q_{jt}\\) into market shares \\(s_{jt} = q_{jt} / M_t\\), we first need to define a market size \\(M_t\\). We’ll assume that the potential number of servings sold in a market is the city’s total population multiplied by 90 days in the quarter. Create a new column called market_size equal to city_population times 90. Note that this assumption is somewhat reasonable but also somewhat arbitrary. Perhaps a sizable portion of the population in a city would never even consider purchasing cereal. Or perhaps those who do tend to want more than one serving per day. In the third exercise, we’ll think more about how to discipline our market size assumption with data.\nNext, compute a new column market_share equal to servings_sold divided by market_size. This gives our market shares \\(s_{jt}\\). We’ll also need the outside share \\(s_{0t} = 1 - \\sum_{j \\in J_t} s_{jt}\\). Create a new column outside_share equal to this expression. You can use .groupby to group by market and .transform('sum') to compute the within-market sum of inside shares. Compute summary statistics for your inside and outside shares. If you computed market shares correctly, the smallest outside share should be \\(s_{0t} \\approx 0.305\\) and the largest should be \\(s_{0t} \\approx 0.815\\).\n\n\n@transform! df :market_size = :city_population .* 90\n\n@transform! df :market_share = :servings_sold ./ :market_size\n\n@chain df begin\n  groupby(:market)\n  @transform! :outside_share = 1 .- sum(:market_share)\nend\n\ndescribe(df.outside_share)\n\nSummary Stats:\nLength:         2256\nMissing Count:  0\nMean:           0.524197\nStd. Deviation: 0.109622\nMinimum:        0.304575\n1st Quartile:   0.439481\nMedian:         0.535808\n3rd Quartile:   0.604251\nMaximum:        0.815168\nType:           Float64",
    "crumbs": [
      "Notes",
      "BLP code (Julia)"
    ]
  },
  {
    "objectID": "computing/blp.html#estimate-the-pure-logit-model-with-ols",
    "href": "computing/blp.html#estimate-the-pure-logit-model-with-ols",
    "title": "BLP Demystified: From Basics to Brain-Busting Models!",
    "section": "3.3 3. Estimate the pure logit model with OLS",
    "text": "3.3 3. Estimate the pure logit model with OLS\nRecall the pure logit estimating equation: \\(\\log(s_{jt} / s_{0t}) = \\delta_{jt} = \\alpha p_{jt} + x_{jt}' \\beta + \\xi_{jt}\\). First, create a new column logit_delta equal to the left-hand side of this expression. You can use np.log to compute the log.\n\n\n@transform! df :logit_delta = log.(:market_share) .- log.(:outside_share)\n\nfirst(df, 5)\n\n5×11 DataFrame\n\n\n\nRow\nmarket\nproduct\nmushy\nservings_sold\ncity_population\nprice_per_serving\nprice_instrument\nmarket_size\nmarket_share\noutside_share\nlogit_delta\n\n\n\nString7\nString7\nInt64\nFloat64\nInt64\nFloat64\nFloat64\nInt64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\nC01Q1\nF1B04\n1\n5.76945e5\n516259\n0.0720879\n0.0354837\n46463310\n0.0124172\n0.555225\n-3.80029\n\n\n2\nC01Q1\nF1B06\n1\n362850.0\n516259\n0.114178\n0.0725791\n46463310\n0.00780939\n0.555225\n-4.26405\n\n\n3\nC01Q1\nF1B07\n1\n603768.0\n516259\n0.132391\n0.101842\n46463310\n0.0129945\n0.555225\n-3.75485\n\n\n4\nC01Q1\nF1B09\n0\n2.68092e5\n516259\n0.130344\n0.104332\n46463310\n0.00576996\n0.555225\n-4.56671\n\n\n5\nC01Q1\nF1B11\n0\n8.3328e5\n516259\n0.154823\n0.121111\n46463310\n0.0179341\n0.555225\n-3.43267\n\n\n\n\n\n\nThen, use the package of your choice to run an OLS regression of logit_delta on a constant, mushy, and price_per_serving. There are many packages for running OLS regressions in Python. One option is to use the formula interface for statsmodels. To use robust standard errors, you can specify cov_type='HC0' in OLS.fit.\nInterpret your estimates. Your coefficient on price_per_serving should be around -7.48. In particular, can you re-express your estimate on mushy in terms of how much consumers are willing to pay for mushy, using your estimated price coefficient?\n\n\nusing FixedEffectModels, RegressionTables\n\nols = reg(df, @formula(logit_delta ~ mushy + price_per_serving), Vcov.robust())\n\nregtable(ols, order = [\"price_per_serving\"], drop = [\"(Intercept)\"], regression_statistics = [Nobs, R2])\n\n\n-------------------------------\n                    logit_delta\n-------------------------------\nprice_per_serving     -7.480***\n                        (0.840)\nmushy                     0.075\n                        (0.054)\n-------------------------------\nControls                    Yes\n-------------------------------\nN                         2,256\nR2                        0.034\n-------------------------------",
    "crumbs": [
      "Notes",
      "BLP code (Julia)"
    ]
  },
  {
    "objectID": "computing/blp.html#run-the-same-regression-with-pyblp",
    "href": "computing/blp.html#run-the-same-regression-with-pyblp",
    "title": "BLP Demystified: From Basics to Brain-Busting Models!",
    "section": "3.4 4. Run the same regression with PyBLP",
    "text": "3.4 4. Run the same regression with PyBLP\nFor the rest of the exercises, we’ll use PyBLP do to our demand estimation. This isn’t necessary for estimating the pure logit model, which can be done with linear regressions, but using PyBLP allows us to easily run our price cut counterfactual and make the model more flexible in subsequent days’ exercises.\nPyBLP requires that some key columns have specific names. You can use .rename to rename the following columns so that they can be understood by PyBLP.\n\nmarket –&gt; market_ids\nproduct –&gt; product_ids\nmarket_share –&gt; shares\nprice_per_serving –&gt; prices\n\nBy default, PyBLP treats prices as endogenous, so it won’t include them in its matrix of instruments. But the “instruments” for running an OLS regression are the same as the full set of regressors. So when running an OLS regression and not account for price endogeneity, we’ll “instrument” for prices with prices themselves. We can do this by creating a new column demand_instruments0 equal to prices. PyBLP will recognize all columns that start with demand_instruments and end with 0, 1, 2, etc., as “excluded” instruments to be stacked with the exogenous characteristics to create the full set of instruments.\nWith the correct columns in hand, we can initialize our pyblp.Problem. To specify the same R-style formula for our regressors, use pyblp.Formulation. The full code should look like the following.\n#| eval: false\n \nols_problem = pyblp.Problem(pyblp.Formulation('1 + mushy + prices'), product_data)\nIf you print(ols_problem), you’ll get information about the configured problem. There should be 94 markets (T), 2256 observations (N), 3 product characteristics (K1), and 3 total instruments (MD). You can verify that these instruments are simply the regressors by looking at ols_problem.products.X1 and ols_problem.products.ZD, comparing these with mushy and prices in your dataframe. For the full set of notation used by PyBLP, which is very close to the notation used in the lectures, see this page.\nTo estimate the configured problem, use .solve. Use method='1s' to just do 1-step GMM instead of the default 2-step GMM. In this case, this will just run a simple linear OLS regression. The full code should look like the following.\n#| eval: false\n \nols_results = ols_problem.solve(method='1s')\nAgain, if you print(ols_results), you’ll get estimates from the logit model. Make sure that your estimates are the same as those you got from your OLS regression. If you used 'HC0' standard errors like suggested above, you standard errors should also be the same.",
    "crumbs": [
      "Notes",
      "BLP code (Julia)"
    ]
  },
  {
    "objectID": "computing/blp.html#add-market-and-product-fixed-effects",
    "href": "computing/blp.html#add-market-and-product-fixed-effects",
    "title": "BLP Demystified: From Basics to Brain-Busting Models!",
    "section": "3.5 5. Add market and product fixed effects",
    "text": "3.5 5. Add market and product fixed effects\nSince we expect price \\(p_{jt}\\) to be correlated with unobserved product quality \\(\\xi_{jt}\\), we should be worried that our estimated \\(\\hat{\\alpha}\\) on price is biased. Since we have multiple observations per market and product, and prices vary both across and within markets, it is feasible for us to add both market and product fixed effects. If \\(\\xi_{jt} = \\xi_j + \\xi_t + \\Delta\\xi_{jt}\\) and most of the correlation between \\(p_{jt}\\) and \\(\\xi_{jt}\\) is due to correlation between \\(p_{jt}\\) and either \\(\\xi_j\\) (product fixed effects) or \\(\\xi_t\\) (market fixed effects), then explicitly accounting for these fixed effects during estimation should help reduce the bias of our \\(\\hat{\\alpha}\\).\nThe simplest way to add fixed effects is as dummy variables. We won’t do this today, but for your own reference, you could do this either by making a separate column for each possible market and product fixed effects and adding these to your formulation, or you could use the shorthand mushy + prices + C(market_ids) + C(product_ids). See pyblp.Formulation for different shorthands you can use. Since there are only 24 products and 94 markets for a total of 118 fixed effects, this approach is actually feasible in this case. But in a more realistic dataset with hundreds or thousands of products and markets, running an OLS regression with this many dummy variables starts to become computationally infeasible.\nThe alternative, which we’ll do today, is to “absorb” the fixed effects. For a single fixed effect, we could just de-mean our outcome variable and each of our regressors within the fixed effects levels, and then run our regression. For multiple fixed effects, we need to iteratively de-mean. PyBLP does this automatically if you specify absorb='C(market_ids) + C(product_ids)' in your formulation instead of adding these as dummy variables.\nSince mushy is always either 1 or 0 for the same product across different markets, it’s collinear with product fixed effects, and you can drop it from your formula. Similarly, you can drop the constant. After dropping these, re-create your problem with absorbed fixed effects and re-solve it. Compare the new \\(\\hat{\\alpha}\\) estimate with the last one. You should now be getting a coefficient on price of around -28.6. Does its change suggest that price was positively or negatively correlated with unobserved product-level/market-level quality?\n\n\nols2 = reg(df, @formula(logit_delta ~ price_per_serving + fe(market) + fe(product)), Vcov.robust())\n\nregtable(ols2, order = [\"price_per_serving\"], drop = [\"(Intercept)\"], regression_statistics = [Nobs, R2])\n\n\n-----------------------------------\n                        logit_delta\n-----------------------------------\nprice_per_serving        -28.618***\n                            (0.916)\n-----------------------------------\nmarket Fixed Effects            Yes\nproduct Fixed Effects           Yes\n-----------------------------------\nN                             2,256\nR2                            0.560\n-----------------------------------",
    "crumbs": [
      "Notes",
      "BLP code (Julia)"
    ]
  },
  {
    "objectID": "computing/blp.html#add-an-instrument-for-price",
    "href": "computing/blp.html#add-an-instrument-for-price",
    "title": "BLP Demystified: From Basics to Brain-Busting Models!",
    "section": "3.6 6. Add an instrument for price",
    "text": "3.6 6. Add an instrument for price\nAdding market and product fixed effects can be helpful, but since unobserved quality typically varies by both product and market, we really want to instrument for prices. The data comes with a column price_instrument that we should interpret as a valid instrument for price that satisfies the needed exclusion restriction. It could be a cost-shifter, a valid Hausman instrument, or similar.\nBefore using it, we should first run a first-stage regression to make sure that it’s a relevant instrument for price. To do so, use the same package you used above to run an OLS regression to run a second OLS regression of prices on price_instrument and your market and product fixed effects. If using the formula interface for statsmodels, you can use the same fixed effect shorthand as in PyBLP, with your full formula looking like prices ~ price_instrument + C(market_ids) + C(product_ids). Does price_instrument seem like a relevant instrument for prices?\n\n\n# first stage\niv0 = reg(df, @formula(price_per_serving ~ price_instrument + fe(market) + fe(product)), Vcov.robust())\n\nregtable(iv0, order = [\"price_instrument\"], drop = [\"(Intercept)\"], regression_statistics = [FStatIV, Nobs, R2])\n\n\n-------------------------------------------\n                          price_per_serving\n-------------------------------------------\nprice_instrument                   0.877***\n                                    (0.007)\n-------------------------------------------\nmarket Fixed Effects                    Yes\nproduct Fixed Effects                   Yes\n-------------------------------------------\nFirst-stage F statistic                    \nN                                     2,256\nR2                                    0.964\n-------------------------------------------\n\n\n\nNow that we’ve checked relevance, we can set our demand_instruments0 column equal to price_instrument, re-create the problem, and re-solve it. You should get a new coefficient on price of around -30.6. Does the change in \\(\\hat{\\alpha}\\) suggest that price was positively or negatively correlated with \\(\\Delta\\xi_{jt}\\) in \\(\\xi_{jt} = \\xi_j + \\xi_t + \\Delta\\xi_{jt}\\)?\n\n\n# first stage\niv1 = reg(df, @formula(logit_delta ~ (price_per_serving ~ price_instrument) + fe(market) + fe(product)), Vcov.robust())\n\nregtable(iv1, order = [\"price_per_serving\"], drop = [\"(Intercept)\"], regression_statistics = [FStatIV, Nobs, R2])\n\n\n-------------------------------------\n                          logit_delta\n-------------------------------------\nprice_per_serving          -30.600***\n                              (0.994)\n-------------------------------------\nmarket Fixed Effects              Yes\nproduct Fixed Effects             Yes\n-------------------------------------\nFirst-stage F statistic    16,814.451\nN                               2,256\nR2                              0.559\n-------------------------------------",
    "crumbs": [
      "Notes",
      "BLP code (Julia)"
    ]
  },
  {
    "objectID": "computing/blp.html#cut-a-price-in-half-and-see-what-happens",
    "href": "computing/blp.html#cut-a-price-in-half-and-see-what-happens",
    "title": "BLP Demystified: From Basics to Brain-Busting Models!",
    "section": "3.7 7. Cut a price in half and see what happens",
    "text": "3.7 7. Cut a price in half and see what happens\nNow that we have our pure logit model estimated, we can run our counterfactual of interest: what if we halved an important product’s price? We’ll select a single market, the most recent quarter in the first city: C01Q2. Create a new dataframe called counterfactual_data by selecting data for just that market and inspect the data. We’ll pretend that we’re firm one, and deciding whether we want to cut the price of our brand four’s product F1B04. In particular, we might be worried about cannibalization, i.e. how much this price cut will result in consumers of our other 8 brands of cereal in this market just substituting from their old choice to the new, cheaper cereal. Alternatively, we could be a regulator or academic interested in how taxing that product would affect demand in the market.\nIn your new dataframe with just data from C01Q2, create a new_prices column that is the same as prices but with the price of F1B04 cut in half. To do this, you could use DataFrame.loc. Then, use .compute_shares on your results from the last question, passing market_id='C01Q2' to only compute new market shares for our market of interest, and passing prices=counterfactual_data['new_prices'] to specify that prices should be set to the new prices. This function will re-compute market shares at the changed prices implied by the model’s estimates. Store them in a new_shares column.\nCompute the percent change in shares for each product in the market. From firm one’s perspective, do the estimates of cannibalization make sense. That is, do the signs on the percent changes for product F1B04 and for other products make sense? Would you normally expect percent changes for other products to be different depending on how other products compare to the one whose price is being changed?\n\n\ncounterfactual_data = @rsubset df :market == \"C01Q2\" \n\n@rtransform! df :new_prices = ifelse(:product == \"F1B04\", :price_per_serving / 2, :price_per_serving)\n\nfirst(counterfactual_data, 5)\n\n5×11 DataFrame\n\n\n\nRow\nmarket\nproduct\nmushy\nservings_sold\ncity_population\nprice_per_serving\nprice_instrument\nmarket_size\nmarket_share\noutside_share\nlogit_delta\n\n\n\nString7\nString7\nInt64\nFloat64\nInt64\nFloat64\nFloat64\nInt64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\nC01Q2\nF1B04\n1\n2.99352e5\n516259\n0.0777177\n0.0441138\n46463310\n0.00644276\n0.502744\n-4.35712\n\n\n2\nC01Q2\nF1B06\n1\n6.56443e6\n516259\n0.141041\n0.101043\n46463310\n0.141282\n0.502744\n-1.26932\n\n\n3\nC01Q2\nF1B07\n1\n4.08387e6\n516259\n0.0726827\n0.0164402\n46463310\n0.0878946\n0.502744\n-1.74394\n\n\n4\nC01Q2\nF1B09\n0\n3.07654e5\n516259\n0.0769744\n0.0498315\n46463310\n0.00662145\n0.502744\n-4.32977\n\n\n5\nC01Q2\nF1B11\n0\n2.52179e6\n516259\n0.167009\n0.132775\n46463310\n0.0542749\n0.502744\n-2.22602",
    "crumbs": [
      "Notes",
      "BLP code (Julia)"
    ]
  },
  {
    "objectID": "computing/blp.html#compute-demand-elasticities",
    "href": "computing/blp.html#compute-demand-elasticities",
    "title": "BLP Demystified: From Basics to Brain-Busting Models!",
    "section": "3.8 8. Compute demand elasticities",
    "text": "3.8 8. Compute demand elasticities\nTo better understand what’s going on, use .compute_elasticities, again specifying market_id='C01Q2', to compute price elasticities for our market of interest. These measure what the model predicts will happen to demand in percentage terms when there’s a 1% change in price of a product. The diagonal elements are own-price elasticities and the off-diagonal elements are cross-price elasticities. Does demand seem very elastic? Do the cross-price elasticities seem particularly reasonable?",
    "crumbs": [
      "Notes",
      "BLP code (Julia)"
    ]
  },
  {
    "objectID": "computing/blp.html#supplemental-questions",
    "href": "computing/blp.html#supplemental-questions",
    "title": "BLP Demystified: From Basics to Brain-Busting Models!",
    "section": "3.9 Supplemental Questions",
    "text": "3.9 Supplemental Questions\nThese questions will not be directly covered in lecture, but will be useful to think about when doing BLP-style estimation in your own work.",
    "crumbs": [
      "Notes",
      "BLP code (Julia)"
    ]
  },
  {
    "objectID": "computing/blp.html#try-different-standard-errors",
    "href": "computing/blp.html#try-different-standard-errors",
    "title": "BLP Demystified: From Basics to Brain-Busting Models!",
    "section": "3.10 1. Try different standard errors",
    "text": "3.10 1. Try different standard errors\nBy default, PyBLP computed standard errors that are robust to heteroskedasticity. But we may be concerned that unobserved quality \\(\\xi_{jt}\\) is systematically correlated across markets for a given product \\(j\\), or across products for a given market \\(t\\). Choose which one you think is more likely and try clustering your standard errors by that dimension. You can do this with se_type='clustered' in .solve, for which you’ll need a clustering_ids column in your product data. See how your standard error for \\(\\alpha\\) changes.",
    "crumbs": [
      "Notes",
      "BLP code (Julia)"
    ]
  },
  {
    "objectID": "computing/blp.html#compute-confidence-intervals-for-your-counterfactual",
    "href": "computing/blp.html#compute-confidence-intervals-for-your-counterfactual",
    "title": "BLP Demystified: From Basics to Brain-Busting Models!",
    "section": "3.11 2. Compute confidence intervals for your counterfactual",
    "text": "3.11 2. Compute confidence intervals for your counterfactual\nYour estimate of \\(\\hat{\\alpha}\\) comes with a standard error, but your counterfactual demand predictions don’t. Ideally we’d like to not only have a point estimate for a counterfactual prediction, but also a measure (up to model misspecification) of how confident we are in these predictions. The easiest way to do this is with a “parametric bootstrap.” The intuition is we can draw from the estimated asymptotic distribution of our \\(\\hat{\\alpha}\\), and for each draw, re-compute demand, and see how demand responds to the same price cut.\nYou can do a parametric bootstrap with the .bootstrap method. Start with just a few draws (e.g., draws=100) and remember to set your seed so that you get the same draws every time you run the code. When new parameters are drawn, you get new .boostrapped_shares, which take the place of your old shares. You can use the same .compute_shares method on the BootstrapedResults class, although you’ll have to pass a prices argument with prices replicated along a new axis by as many draws as you have.\nOnce you have some bootstrapped shares, compute the same percent changes, and compute the 2.5th and 97.5th percentiles of these changes for each product. Are these 95% confidence intervals for your predictions particularly wide?",
    "crumbs": [
      "Notes",
      "BLP code (Julia)"
    ]
  },
  {
    "objectID": "computing/blp.html#impute-marginal-costs-from-pricing-optimality",
    "href": "computing/blp.html#impute-marginal-costs-from-pricing-optimality",
    "title": "BLP Demystified: From Basics to Brain-Busting Models!",
    "section": "3.12 3. Impute marginal costs from pricing optimality",
    "text": "3.12 3. Impute marginal costs from pricing optimality\nThe canonical demand side of the BLP model assumes firms set prices in static Bertrand-Nash equilibrium. See this section for a quick summary using PyBLP notation. Given an estimated demand model and such assumptions about pricing, we can impute marginal costs c_{jt}.\nTo do so, you first need to tell PyBLP what firms own what products. Create a new firm_ids column in your data, re-initialize your problem, and re-solve it. Then, you should be able to run the .compute_costs method to impute firms’ marginal cost of producing each cereal. Do these marginal costs look particularly reasonable? How might limitations of your demand model and supply model bias them? What would they and observed prices imply about firms’ markups and economic profits?",
    "crumbs": [
      "Notes",
      "BLP code (Julia)"
    ]
  },
  {
    "objectID": "computing/blp.html#check-your-code-by-simulating-data",
    "href": "computing/blp.html#check-your-code-by-simulating-data",
    "title": "BLP Demystified: From Basics to Brain-Busting Models!",
    "section": "3.13 4. Check your code by simulating data",
    "text": "3.13 4. Check your code by simulating data\nEven experienced software developers make a lot of mistakes when writing code. Writing “unit tests” or “integration tests” that check whether the code you’ve written seems to be working properly is incredibly important when writing complicated code to estimate demand. Perhaps the most useful test you can write when doing demand estimation (or most other types of structural estimation) is the following.\n\nSimulate fake data under some true parameters.\nEstimate your model on the simulated data and make sure that you can recover the true parameters, up to sampling error.\n\nIf you do these steps many times, the resulting Monte Carlo experiment will also give you a good sense for the finite sample statistical properties of your estimator.\nPyBLP’s Simulation class makes simulating data fairly straightforward. Its interface is similar to Problem, but you also specify your parameter estimates and structural errors. In addition to checking your code, you can also use this class for more complicated counterfactuals. After initializing your simulation, you can use .replace_endogenous to have PyBLP replace the prices \\(p_{jt}\\) and market shares \\(s_{jt}\\) with those that rationalize the chosen true parameters and other parts of the simulation. It does so by solving the pricing equilibrium. You’ll have to pass your imputed marginal costs via the costs argument.\nInitialize a simulation of the pure logit model with the same product_data and the same estimated xi but with an \\(\\alpha\\) somewhat different than the one you estimated. Make sure your chosen \\(\\alpha\\) is negative, otherwise demand will be upward sloping and PyBLP will have trouble solving for equilibrium prices. To the estimated .xi you can add the estimated fixed effects .xi_fe, since the simulation class does not support fixed effects absorption.\nHave PyBLP solve for prices and market shares, and use the resulting data to re-estimate your pure logit regression. See if you can get an estimated \\(\\hat{\\alpha}\\) close to the true one.",
    "crumbs": [
      "Notes",
      "BLP code (Julia)"
    ]
  },
  {
    "objectID": "computing/blp.html#introduction-1",
    "href": "computing/blp.html#introduction-1",
    "title": "BLP Demystified: From Basics to Brain-Busting Models!",
    "section": "4.1 Introduction",
    "text": "4.1 Introduction\nThis is the second of three exercises that will give you a solid foundation for doing BLP-style estimation. We’ll continue with the same running example: what if we halved an important product’s price? Our goal today is to relax some of the unrealistic substitution patterns implied by the pure logit model by incorporating preference heterogeneity. To do so, we will use cross-market variation in our product and some new demographic data to estimate parameters that govern preference heterogeneity.",
    "crumbs": [
      "Notes",
      "BLP code (Julia)"
    ]
  },
  {
    "objectID": "computing/blp.html#setup-1",
    "href": "computing/blp.html#setup-1",
    "title": "BLP Demystified: From Basics to Brain-Busting Models!",
    "section": "4.2 Setup",
    "text": "4.2 Setup\nWe’ll be continuing where we left off after the first exercise. You should just keep adding to your code, using its solutions if you had trouble with some parts.",
    "crumbs": [
      "Notes",
      "BLP code (Julia)"
    ]
  },
  {
    "objectID": "computing/blp.html#data-1",
    "href": "computing/blp.html#data-1",
    "title": "BLP Demystified: From Basics to Brain-Busting Models!",
    "section": "4.3 Data",
    "text": "4.3 Data\nToday, you’ll incorporate demographics.csv into estimation, which again is a simplified version of Nevo’s (2000) demographic data with less information and fewer derived columns. The data were originally draws from the Current Population Survey.\nIn your own work, when incorporating demographic data into estimation, you will want to sample from the whole Current Population Survey (or whatever survey/census data you are using), not just from a subset of it. The small size of today’s demographic data helps with distributing the data, but in practice you should ideally be sampling from a much larger dataset of demographic information. In your own work you will also want to incorporate more demographic variables than the one included in this dataset. Like the product data, in these exercises we only consider a few columns to keep the exercises a manageable length.\nThe demographic dataset contains information about 20 individuals drawn from the Current Population Survey for each of the 94 markets in the product data. Each row is a different individual. The columns in the data are as follows.\n\n\n\n\n\n\n\n\nColumn\nData Type\nDescription\n\n\n\n\nmarket\nString\nThe city-quarter pair that defines markets \\(t\\) used in these exercises. The data were motivated by real cereal purchase data across 47 US cities in the first 2 quarters of 1988.\n\n\nquarterly_income\nFloat\nThe quarterly income of the individual in dollars.\n\n\n\nIn today and tomorrow’s exercises, we will use these demographic data to introduce income-specific preference heterogeneity into our BLP model of demand for cereal and see how our counterfactual changes. By incorporating income, we will also be able to speak to distributional concerns: how will counterfactual changes in the market differentially affect high- and low-income consumers?",
    "crumbs": [
      "Notes",
      "BLP code (Julia)"
    ]
  },
  {
    "objectID": "computing/blp.html#my-blp.jl-this-section-failed-miserably",
    "href": "computing/blp.html#my-blp.jl-this-section-failed-miserably",
    "title": "BLP Demystified: From Basics to Brain-Busting Models!",
    "section": "4.4 My BLP.jl (This section failed miserably…)",
    "text": "4.4 My BLP.jl (This section failed miserably…)\nIn this section, I will try my best to make a very flexible BLP estimation code.\nFirst load two data we will use and see how they look like:\n\nusing CSV, DataFrames, DataFramesMeta, Random, LinearAlgebra, Statistics, Distributions, StatsBase\nusing FixedEffectModels, RegressionTables, Optim\n\n# Load the necessary data\ndata_prod = df;\n\n# rename each certain columns to match it to the key argument in later functions\n\n@rename! data_prod begin\n  :market_id = :market\n  :product_id = :product\n  :price_x0 = :price_per_serving\n  :mushy_x1 = :mushy\n  :price_instrument_z0 = :price_instrument\n  :s_obs = :market_share\n  :s_out = :outside_share\nend\n\nfirst(data_prod, 5)\n\nWARNING: using Optim.optimize in module Notebook conflicts with an existing identifier.\n\n\n5×12 DataFrame\n\n\n\nRow\nmarket_id\nproduct_id\nmushy_x1\nservings_sold\ncity_population\nprice_x0\nprice_instrument_z0\nmarket_size\ns_obs\ns_out\nlogit_delta\nnew_prices\n\n\n\nString7\nString7\nInt64\nFloat64\nInt64\nFloat64\nFloat64\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\nC01Q1\nF1B04\n1\n5.76945e5\n516259\n0.0720879\n0.0354837\n46463310\n0.0124172\n0.555225\n-3.80029\n0.036044\n\n\n2\nC01Q1\nF1B06\n1\n362850.0\n516259\n0.114178\n0.0725791\n46463310\n0.00780939\n0.555225\n-4.26405\n0.114178\n\n\n3\nC01Q1\nF1B07\n1\n603768.0\n516259\n0.132391\n0.101842\n46463310\n0.0129945\n0.555225\n-3.75485\n0.132391\n\n\n4\nC01Q1\nF1B09\n0\n2.68092e5\n516259\n0.130344\n0.104332\n46463310\n0.00576996\n0.555225\n-4.56671\n0.130344\n\n\n5\nC01Q1\nF1B11\n0\n8.3328e5\n516259\n0.154823\n0.121111\n46463310\n0.0179341\n0.555225\n-3.43267\n0.154823\n\n\n\n\n\n\n\n# Load the necessary data\ndata_demo = CSV.read(\"data/raw/demographics.csv\", DataFrame)\n\n@rename! data_demo begin\n  :market_id = :market\n  :log_income_y0 = :quarterly_income\nend\n\n@rtransform! data_demo :log_income_y0 = log(:log_income_y0)\n\nfirst(data_demo, 5)\n\n5×2 DataFrame\n\n\n\nRow\nmarket_id\nlog_income_y0\n\n\n\nString7\nFloat64\n\n\n\n\n1\nC01Q1\n8.58651\n\n\n2\nC01Q1\n9.48285\n\n\n3\nC01Q1\n8.40662\n\n\n4\nC01Q1\n7.69358\n\n\n5\nC01Q1\n6.42269\n\n\n\n\n\n\nNext we sample the demo data to have 1000 random draw for each market:\n\nfunction sample_ind(df, n = 1000, seedn = 123)\n    Random.seed!(seedn)\n\n    grouped = groupby(df, :market_id)\n\n    sampled_df = combine(grouped) do subdf\n        sampled_rows = sample(eachrow(subdf), n; replace = true)\n        DataFrame(sampled_rows)\n    end\n\n    sampled_df = transform(groupby(sampled_df, :market_id), eachindex =&gt; :ind_id)\n\n    return sampled_df\nend\n\n# 94 markets x 1000 individual\ndata_demo = sample_ind(data_demo);\n\n\ndata_demo\n# first(data_demo, 10)\n\n94000×3 DataFrame93975 rows omitted\n\n\n\nRow\nmarket_id\nlog_income_y0\nind_id\n\n\n\nString7\nFloat64\nInt64\n\n\n\n\n1\nC01Q1\n8.39605\n1\n\n\n2\nC01Q1\n7.76281\n2\n\n\n3\nC01Q1\n7.75411\n3\n\n\n4\nC01Q1\n9.09651\n4\n\n\n5\nC01Q1\n9.09651\n5\n\n\n6\nC01Q1\n7.75411\n6\n\n\n7\nC01Q1\n8.29923\n7\n\n\n8\nC01Q1\n8.40662\n8\n\n\n9\nC01Q1\n8.47015\n9\n\n\n10\nC01Q1\n8.1857\n10\n\n\n11\nC01Q1\n7.77479\n11\n\n\n12\nC01Q1\n7.76281\n12\n\n\n13\nC01Q1\n9.09651\n13\n\n\n⋮\n⋮\n⋮\n⋮\n\n\n93989\nC65Q2\n8.46581\n989\n\n\n93990\nC65Q2\n7.4559\n990\n\n\n93991\nC65Q2\n8.68641\n991\n\n\n93992\nC65Q2\n7.5741\n992\n\n\n93993\nC65Q2\n7.91547\n993\n\n\n93994\nC65Q2\n8.98306\n994\n\n\n93995\nC65Q2\n8.45945\n995\n\n\n93996\nC65Q2\n8.95375\n996\n\n\n93997\nC65Q2\n7.80121\n997\n\n\n93998\nC65Q2\n7.17447\n998\n\n\n93999\nC65Q2\n7.58398\n999\n\n\n94000\nC65Q2\n5.13197\n1000\n\n\n\n\n\n\nCreate unobserved individual \\(v_{it}\\):\n\nfunction create_v!(df::DataFrame, node::Int)\n    for i in 1:node\n        colname = Symbol(\"node_y\", i)\n        df[!, colname] = randn(nrow(df))\n    end\n    return df\nend\n\ncreate_v!(data_demo, 1)\n\n94000×4 DataFrame93975 rows omitted\n\n\n\nRow\nmarket_id\nlog_income_y0\nind_id\nnode_y1\n\n\n\nString7\nFloat64\nInt64\nFloat64\n\n\n\n\n1\nC01Q1\n8.39605\n1\n0.808288\n\n\n2\nC01Q1\n7.76281\n2\n-1.12207\n\n\n3\nC01Q1\n7.75411\n3\n-1.10464\n\n\n4\nC01Q1\n9.09651\n4\n-0.416993\n\n\n5\nC01Q1\n9.09651\n5\n0.287588\n\n\n6\nC01Q1\n7.75411\n6\n0.229819\n\n\n7\nC01Q1\n8.29923\n7\n-0.421769\n\n\n8\nC01Q1\n8.40662\n8\n-1.35559\n\n\n9\nC01Q1\n8.47015\n9\n0.0694591\n\n\n10\nC01Q1\n8.1857\n10\n-0.117323\n\n\n11\nC01Q1\n7.77479\n11\n1.21928\n\n\n12\nC01Q1\n7.76281\n12\n0.292914\n\n\n13\nC01Q1\n9.09651\n13\n-0.0311481\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n93989\nC65Q2\n8.46581\n989\n1.45184\n\n\n93990\nC65Q2\n7.4559\n990\n-0.873824\n\n\n93991\nC65Q2\n8.68641\n991\n0.534542\n\n\n93992\nC65Q2\n7.5741\n992\n0.487625\n\n\n93993\nC65Q2\n7.91547\n993\n-1.12754\n\n\n93994\nC65Q2\n8.98306\n994\n0.991109\n\n\n93995\nC65Q2\n8.45945\n995\n-0.112785\n\n\n93996\nC65Q2\n8.95375\n996\n-1.30939\n\n\n93997\nC65Q2\n7.80121\n997\n-1.64881\n\n\n93998\nC65Q2\n7.17447\n998\n0.645287\n\n\n93999\nC65Q2\n7.58398\n999\n-0.0317194\n\n\n94000\nC65Q2\n5.13197\n1000\n1.23878\n\n\n\n\n\n\nJoin all dataframes into one:\n\n\ndf1 = unique(@select data_prod :market_id :product_id)\ndf2 = unique(@select data_demo :ind_id)\n\n# Cross join to mimic expand_grid\nmain_data = crossjoin(df1, df2)\n\nleftjoin!(main_data, data_prod; on=[:market_id, :product_id])\nleftjoin!(main_data, data_demo; on=[:market_id, :ind_id])\n\nmain_data\n\n2256000×15 DataFrame2255975 rows omitted\n\n\n\nRow\nmarket_id\nproduct_id\nind_id\nmushy_x1\nservings_sold\ncity_population\nprice_x0\nprice_instrument_z0\nmarket_size\ns_obs\ns_out\nlogit_delta\nnew_prices\nlog_income_y0\nnode_y1\n\n\n\nString7\nString7\nInt64\nInt64?\nFloat64?\nInt64?\nFloat64?\nFloat64?\nInt64?\nFloat64?\nFloat64?\nFloat64?\nFloat64?\nFloat64?\nFloat64?\n\n\n\n\n1\nC01Q1\nF1B04\n1\n1\n5.76945e5\n516259\n0.0720879\n0.0354837\n46463310\n0.0124172\n0.555225\n-3.80029\n0.036044\n8.39605\n0.808288\n\n\n2\nC01Q1\nF1B04\n2\n1\n5.76945e5\n516259\n0.0720879\n0.0354837\n46463310\n0.0124172\n0.555225\n-3.80029\n0.036044\n7.76281\n-1.12207\n\n\n3\nC01Q1\nF1B04\n3\n1\n5.76945e5\n516259\n0.0720879\n0.0354837\n46463310\n0.0124172\n0.555225\n-3.80029\n0.036044\n7.75411\n-1.10464\n\n\n4\nC01Q1\nF1B04\n4\n1\n5.76945e5\n516259\n0.0720879\n0.0354837\n46463310\n0.0124172\n0.555225\n-3.80029\n0.036044\n9.09651\n-0.416993\n\n\n5\nC01Q1\nF1B04\n5\n1\n5.76945e5\n516259\n0.0720879\n0.0354837\n46463310\n0.0124172\n0.555225\n-3.80029\n0.036044\n9.09651\n0.287588\n\n\n6\nC01Q1\nF1B04\n6\n1\n5.76945e5\n516259\n0.0720879\n0.0354837\n46463310\n0.0124172\n0.555225\n-3.80029\n0.036044\n7.75411\n0.229819\n\n\n7\nC01Q1\nF1B04\n7\n1\n5.76945e5\n516259\n0.0720879\n0.0354837\n46463310\n0.0124172\n0.555225\n-3.80029\n0.036044\n8.29923\n-0.421769\n\n\n8\nC01Q1\nF1B04\n8\n1\n5.76945e5\n516259\n0.0720879\n0.0354837\n46463310\n0.0124172\n0.555225\n-3.80029\n0.036044\n8.40662\n-1.35559\n\n\n9\nC01Q1\nF1B04\n9\n1\n5.76945e5\n516259\n0.0720879\n0.0354837\n46463310\n0.0124172\n0.555225\n-3.80029\n0.036044\n8.47015\n0.0694591\n\n\n10\nC01Q1\nF1B04\n10\n1\n5.76945e5\n516259\n0.0720879\n0.0354837\n46463310\n0.0124172\n0.555225\n-3.80029\n0.036044\n8.1857\n-0.117323\n\n\n11\nC01Q1\nF1B04\n11\n1\n5.76945e5\n516259\n0.0720879\n0.0354837\n46463310\n0.0124172\n0.555225\n-3.80029\n0.036044\n7.77479\n1.21928\n\n\n12\nC01Q1\nF1B04\n12\n1\n5.76945e5\n516259\n0.0720879\n0.0354837\n46463310\n0.0124172\n0.555225\n-3.80029\n0.036044\n7.76281\n0.292914\n\n\n13\nC01Q1\nF1B04\n13\n1\n5.76945e5\n516259\n0.0720879\n0.0354837\n46463310\n0.0124172\n0.555225\n-3.80029\n0.036044\n9.09651\n-0.0311481\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n2255989\nC65Q2\nF6B18\n989\n0\n4.43638e5\n188082\n0.127557\n0.0815838\n16927380\n0.0262083\n0.642477\n-3.19925\n0.127557\n8.46581\n1.45184\n\n\n2255990\nC65Q2\nF6B18\n990\n0\n4.43638e5\n188082\n0.127557\n0.0815838\n16927380\n0.0262083\n0.642477\n-3.19925\n0.127557\n7.4559\n-0.873824\n\n\n2255991\nC65Q2\nF6B18\n991\n0\n4.43638e5\n188082\n0.127557\n0.0815838\n16927380\n0.0262083\n0.642477\n-3.19925\n0.127557\n8.68641\n0.534542\n\n\n2255992\nC65Q2\nF6B18\n992\n0\n4.43638e5\n188082\n0.127557\n0.0815838\n16927380\n0.0262083\n0.642477\n-3.19925\n0.127557\n7.5741\n0.487625\n\n\n2255993\nC65Q2\nF6B18\n993\n0\n4.43638e5\n188082\n0.127557\n0.0815838\n16927380\n0.0262083\n0.642477\n-3.19925\n0.127557\n7.91547\n-1.12754\n\n\n2255994\nC65Q2\nF6B18\n994\n0\n4.43638e5\n188082\n0.127557\n0.0815838\n16927380\n0.0262083\n0.642477\n-3.19925\n0.127557\n8.98306\n0.991109\n\n\n2255995\nC65Q2\nF6B18\n995\n0\n4.43638e5\n188082\n0.127557\n0.0815838\n16927380\n0.0262083\n0.642477\n-3.19925\n0.127557\n8.45945\n-0.112785\n\n\n2255996\nC65Q2\nF6B18\n996\n0\n4.43638e5\n188082\n0.127557\n0.0815838\n16927380\n0.0262083\n0.642477\n-3.19925\n0.127557\n8.95375\n-1.30939\n\n\n2255997\nC65Q2\nF6B18\n997\n0\n4.43638e5\n188082\n0.127557\n0.0815838\n16927380\n0.0262083\n0.642477\n-3.19925\n0.127557\n7.80121\n-1.64881\n\n\n2255998\nC65Q2\nF6B18\n998\n0\n4.43638e5\n188082\n0.127557\n0.0815838\n16927380\n0.0262083\n0.642477\n-3.19925\n0.127557\n7.17447\n0.645287\n\n\n2255999\nC65Q2\nF6B18\n999\n0\n4.43638e5\n188082\n0.127557\n0.0815838\n16927380\n0.0262083\n0.642477\n-3.19925\n0.127557\n7.58398\n-0.0317194\n\n\n2256000\nC65Q2\nF6B18\n1000\n0\n4.43638e5\n188082\n0.127557\n0.0815838\n16927380\n0.0262083\n0.642477\n-3.19925\n0.127557\n5.13197\n1.23878\n\n\n\n\n\n\nNow let’s start construcing BLP estimation algorithm! First we will construct functions that calculate predicted market share and perform contraction mapping to get \\(\\delta\\) when given \\(\\Pi\\), \\(\\Sigma\\):\n\n# pi: 2x1\n# sigma: 2x1\n# delta should be jt variable\n \nfunction calc_mkt_share(df, delta, pi, sigma)\n  df = leftjoin(df, delta; on=[:market_id, :product_id])\n\n  data = @rtransform df :util = exp(:d + :price_x0 * sigma[1] * :node_y1 + :mushy_x1 * sigma[2] * :node_y1 + :price_x0 * pi[1] * :log_income_y0 + :mushy_x1 * pi[2] * :log_income_y0)\n\n  gdp = groupby(data, [:market_id, :ind_id])\n  data = @transform gdp :util_it = exp.(:util) ./ (sum(exp.(:util)) + 1)\n\n  gdp = groupby(data, [:market_id, :product_id])\n  data = @combine gdp :pred_sh_jt = sum(:util_it .* (1/1000))\nend\n\n  delta_fp = unique(df, [:market_id, :product_id])\n  @select! delta_fp :market_id :product_id\n  @rtransform! delta_fp :d = 0\ncalc_mkt_share(main_data, delta_fp, [1,1], [1,1])\n\n# #d0 should be jt variable\n# function contraction_map(df, d0, pi, sigma, tol = 1e-5, maxiter = 1e5)\n#   obs_mkt_share = unique(df, [:market_id, :product_id])\n#   @select! obs_mkt_share :market_id :product_id :s_obs\n#   d_old = copy(d0)\n#   normdiff = Inf\n#   iter = 0\n#   while normdiff &gt; tol && iter &lt;= maxiter\n#     model_mkt_share = calc_mkt_share(df, d_old, pi, sigma)\n#     df_sh = @select df :market_id :product_id :s_obs\n#     model_mkt_share = leftjoin(model_mkt_share, df_sh, on=[:market_id, :product_id])\n\n#     d_new = leftjoin(d_old, model_mkt_share; on= [:market_id, :product_id]) \n#     d_new = @rtransform d_new :d_iter = :d + log(:s_obs) - log(:pred_sh_jt)\n\n#     normdiff = maximum(abs.(d_new.d_iter .- d_new.d))\n\n#     # normdiff = maximum(norm.(d_new.d_iter .- d_new.d))\n#     d_old = @select d_new begin\n#       :market_id\n#       :product_id\n#       :d = :d_iter \n#     end \n#     iter += 1\n#   end\n#   return d_old\n# end\n\n# function calc_delta(df, pi, sigma)\n#   delta_fp = unique(df, [:market_id, :product_id])\n#   @select! delta_fp :market_id :product_id\n#   @rtransform! delta_fp :d = 0\n\n#   delta_jt = contraction_map(df, delta_fp, pi, sigma)\n\n#   return delta_jt\n# end\n\n# @time delta_res = calc_delta(main_data, [1, 1], [1, 1])\n# mean(delta_res)\n\n2256×3 DataFrame2231 rows omitted\n\n\n\nRow\nmarket_id\nproduct_id\npred_sh_jt\n\n\n\nString7\nString7\nFloat64\n\n\n\n\n1\nC01Q1\nF1B04\nNaN\n\n\n2\nC01Q1\nF1B06\nNaN\n\n\n3\nC01Q1\nF1B07\nNaN\n\n\n4\nC01Q1\nF1B09\n2.00559e-55\n\n\n5\nC01Q1\nF1B11\n2.39821e-55\n\n\n6\nC01Q1\nF1B13\n2.10254e-55\n\n\n7\nC01Q1\nF1B17\nNaN\n\n\n8\nC01Q1\nF1B30\n1.97595e-55\n\n\n9\nC01Q1\nF1B45\n2.30514e-55\n\n\n10\nC01Q1\nF2B05\n1.73479e-55\n\n\n11\nC01Q1\nF2B08\n2.03294e-55\n\n\n12\nC01Q1\nF2B15\nNaN\n\n\n13\nC01Q1\nF2B16\nNaN\n\n\n⋮\n⋮\n⋮\n⋮\n\n\n2245\nC65Q2\nF2B16\nNaN\n\n\n2246\nC65Q2\nF2B19\n2.79288e-6\n\n\n2247\nC65Q2\nF2B26\n2.61091e-6\n\n\n2248\nC65Q2\nF2B28\nNaN\n\n\n2249\nC65Q2\nF2B40\n2.79267e-6\n\n\n2250\nC65Q2\nF2B48\n2.64727e-6\n\n\n2251\nC65Q2\nF3B06\nNaN\n\n\n2252\nC65Q2\nF3B14\n2.64427e-6\n\n\n2253\nC65Q2\nF4B02\n3.01899e-6\n\n\n2254\nC65Q2\nF4B10\n2.69486e-6\n\n\n2255\nC65Q2\nF4B12\n2.53109e-6\n\n\n2256\nC65Q2\nF6B18\n2.65098e-6",
    "crumbs": [
      "Notes",
      "BLP code (Julia)"
    ]
  },
  {
    "objectID": "computing/blp.html#my-blp.jl-standing-on-the-shoulders-of-giants",
    "href": "computing/blp.html#my-blp.jl-standing-on-the-shoulders-of-giants",
    "title": "BLP Demystified: From Basics to Brain-Busting Models!",
    "section": "4.5 My BLP.jl (standing on the shoulders of giants)",
    "text": "4.5 My BLP.jl (standing on the shoulders of giants)\nOkay, my previous works failed miserably. This is probably because it is hard to do the matrix complication in terms of the dataframe-dplyr style in R. But then again, maybe it is just because I did not do a good job. Thus I will try to write up a code following others while trying to use DataFramesMeta syntax whenever it is doable for transparency.\nBut this will take a lot of work. I will periodically come back to update my BLP. In the meantime, enjoy your life!",
    "crumbs": [
      "Notes",
      "BLP code (Julia)"
    ]
  },
  {
    "objectID": "computing/blp.html#references",
    "href": "computing/blp.html#references",
    "title": "BLP Demystified: From Basics to Brain-Busting Models!",
    "section": "References",
    "text": "References\n\n\nBerry, Steven. 1994. “Estimating Discrete-Choice Models of Product Differentiation.” The RAND Journal of Economics 25 (2): 242–62. http://www.jstor.org/stable/2555829.\n\n\nBerry, Steven, James Levinsohn, and Ariel Pakes. 1995. “Automobile Prices in Market Equilibrium.” Econometrica 63 (4): 841–90. http://www.jstor.org/stable/2171802.\n\n\nMcFadden, Daniel. 1974. “Conditional Logit Analysis of Qualitative Choice Behavior.” In Fontiers in Econometrics, edited by Paul Zarembka, 105–42. New York: Academic press.\n\n\nTrain, Kenneth E. 2009. Discrete Choice Methods with Simulation. 2nd ed. Cambridge University Press.",
    "crumbs": [
      "Notes",
      "BLP code (Julia)"
    ]
  },
  {
    "objectID": "computing/blp.html#footnotes",
    "href": "computing/blp.html#footnotes",
    "title": "BLP Demystified: From Basics to Brain-Busting Models!",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou might have already noticed, but I kind of use variables without subscript as the vector of the variables. For example, \\(\\delta\\) is just \\((\\delta_1, \\ldots, \\delta_J).\\)↩︎",
    "crumbs": [
      "Notes",
      "BLP code (Julia)"
    ]
  },
  {
    "objectID": "computing/polars.html",
    "href": "computing/polars.html",
    "title": "Polars from Python and R",
    "section": "",
    "text": "This is example templates that use Polars for R and Python. Note that these are short examples. If you want to know more about what they can do, check out this site.\n\nRPython\n\n\n\nlibrary(polars)\n\nnyc = pl$scan_parquet(\"nyc-taxi/**/*.parquet\", hive_partitioning = TRUE)\nnyc\n\nq1 = (\n    nyc\n    $group_by(\"passenger_count\")\n    $agg(\n        pl$mean(\"tip_amount\")#$alias(\"mean_tip\") ## alias is optional\n    )\n    $sort(\"passenger_count\")\n)\nq1 \n\ntic = Sys.time()\ndat1 = q1$collect()\ntoc = Sys.time()\n\ndat1\n\nq2 = (\n    nyc\n    $filter(pl$col(\"month\") &lt;= 3)\n    $group_by(\"month\", \"passenger_count\")\n    $agg(pl$mean(\"tip_amount\")$alias(\"mean_tip\"))\n    $sort(\"passenger_count\")\n) \n\n# q2              # naive\ncat(q2$explain()) # optimized\n\ntic = Sys.time()\ndat2 = q2$collect()\ntoc = Sys.time()\n\ndat2\n\nq3 = (\n    nyc\n    $group_by(\"passenger_count\", \"trip_distance\")\n    $agg(\n        pl$mean(\"tip_amount\")$alias(\"mean_tip\"),\n        pl$mean(\"fare_amount\")$alias(\"mean_fare\")\n        )\n    $sort(\"passenger_count\", \"trip_distance\")\n)\n\ntic = Sys.time()\ndat3 = q3$collect()\ntoc = Sys.time()\n \ndat3\n\ndat3$unpivot(index = c(\"passenger_count\", \"trip_distance\"))\n\nmean_tips  = nyc$group_by(\"month\")$agg(pl$col(\"tip_amount\")$mean())\nmean_fares = nyc$group_by(\"month\")$agg(pl$col(\"fare_amount\")$mean())\n\n(\n    mean_tips\n    $join(\n        mean_fares,\n        on = \"month\",\n        how = \"left\"  # default is inner join\n    )\n    $collect()\n)\n\n# You can also try tidypolars\n\nlibrary(polars) ## Already loaded\nlibrary(tidypolars)\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(tidyr, warn.conflicts = FALSE)\n\nnyc = scan_parquet_polars(\"nyc-taxi/**/*.parquet\")\n\nnyc |&gt; \n    summarise(mean_tip = mean(tip_amount), .by = passenger_count) |&gt;\n    compute()\n\n# Aside: Use collect() instead of compute() at the end if you would prefer to return a standard R data.frame instead of a Polars DataFrame.\n\n\n\n\nimport polars as pl\nimport time\nimport matplotlib\n\nnyc = pl.scan_parquet(\"nyc-taxi/**/*.parquet\", hive_partitioning=True)\nnyc\n\nq1 = (\n    nyc\n    .group_by([\"passenger_count\"])\n    .agg([\n            pl.mean(\"tip_amount\")#.alias(\"mean_tip\") ## alias is optional\n        ])\n    .sort(\"passenger_count\")\n)\nq1\n\ntic = time.time()\ndat1 = q1.collect()\ntoc = time.time()\n\ndat1\n\nq2 = (\n    nyc\n    .filter(pl.col(\"month\") &lt;= 3)\n    .group_by([\"month\", \"passenger_count\"])\n    .agg([pl.mean(\"tip_amount\").alias(\"mean_tip\")])\n    .sort(\"passenger_count\")\n)\n\n# q2             # naive\nq2.show_graph()  # optimized\n\ntic = time.time()\ndat2 = q2.collect()\ntoc = time.time()\n\ndat2\n\nq3 = (\n    nyc\n    .group_by([\"passenger_count\", \"trip_distance\"])\n    .agg([\n        pl.mean(\"tip_amount\").alias(\"mean_tip\"),\n        pl.mean(\"fare_amount\").alias(\"mean_fare\"),\n        ])\n    .sort([\"passenger_count\", \"trip_distance\"])\n)\n\ntic = time.time()\ndat3 = q3.collect()\ntoc = time.time()\n\ndat3\n\ndat3.unpivot(index = [\"passenger_count\", \"trip_distance\"])\n\nmean_tips  = nyc.group_by(\"month\").agg(pl.col(\"tip_amount\").mean())\nmean_fares = nyc.group_by(\"month\").agg(pl.col(\"fare_amount\").mean())\n\n(\n    mean_tips\n    .join(\n        mean_fares,\n        on = \"month\",\n        how = \"left\" # default is inner join\n    )\n    .collect()\n)",
    "crumbs": [
      "Notes",
      "Polars (R + Python)"
    ]
  },
  {
    "objectID": "study/io-theory.html",
    "href": "study/io-theory.html",
    "title": "Empirical IO study",
    "section": "",
    "text": "This page contains an outline of the topics, contents, and works I did for Empirical IO self-study.\nNotes\nMaterials I am using: IO syllabus by Phil Haile; MIT IO opencourse; Train’s textbook; Mixetape course on demand estimation; Some bits of Tirole’s textbook.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nweek\ndow\ndate\nwhat\ntopic\nlink\nmaterial\nnotes\n\n\n\n\n1\nFri\nMay 9\ndue-date\nfirm (tirole; mit)\n\n\n\n\n\n\n\n\n\n\ntirole; ackerberg, benkard, berry and pakes2007; gandhi and nevo2021; aguirregabiria, collard-wexler and ryan2021\n\n\n\nFri\n\n\n\n\nTrain (~logit)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFri\n\n\n\n\nackerberg, benkard, berry and pakes2007\n\n\n\n\n\n\n\n\n\n\n\n\n2\nFri\nMay 16\ndue-date\nmonopoly\n\n\n\n\n\n\n\n\n\n\ntirole; ackerberg, benkard, berry and pakes2007; gandhi and nevo2021; aguirregabiria, collard-wexler and ryan2022\n\n\n\nFri\n\n\ndue-date\nmonopoly\n\n\n\n\n\n\n\n\n\n\ntirole; ackerberg, benkard, berry and pakes2007; gandhi and nevo2021; aguirregabiria, collard-wexler and ryan2023\n\n\n\nFri\n\n\n\n\nTrain (~mixed logit)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFri\n\n\n\n\nackerberg, benkard, berry and pakes2007\n\n\n\n\n\n\n\n\n\n\n\n\n3\nFri\nMay 23\ndue-date\nproduct selection\n\n\n\n\n\n\n\n\ntirole; ackerberg, benkard, berry and pakes2007; gandhi and nevo2021; aguirregabiria, collard-wexler and ryan2024\n\n\n\nFri\n\n\n\n\nTrain (~Numerical Maximization)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFri\n\n\n\n\nackerberg, benkard, berry and pakes2007\n\n\n\n\n\n\n\n\n\n\n\n\n4\nFri\nMay 30\ndue-date\nprice discrimination\n\n\n\n\n\n\n\n\ntirole; ackerberg, benkard, berry and pakes2007; gandhi and nevo2021; aguirregabiria, collard-wexler and ryan2025\n\n\n\nFri\n\n\n\n\nprice discrimination\n\n\n\n\n\n\n\n\ntirole; ackerberg, benkard, berry and pakes2007; gandhi and nevo2021; aguirregabiria, collard-wexler and ryan2026\n\n\n\nFri\n\n\n\n\nprice discrimination\n\n\n\n\n\n\n\n\ntirole; ackerberg, benkard, berry and pakes2007; gandhi and nevo2021; aguirregabiria, collard-wexler and ryan2027\n\n\n\nFri\n\n\n\n\nTrain (~densities)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFri\n\n\n\n\nackerberg, benkard, berry and pakes2007\n\n\n\n\n\n\n\n\n\n\n\n\n5\nFri\nJun 6\ndue-date\nvertical control\n\n\n\n\n\n\n\n\ntirole; ackerberg, benkard, berry and pakes2007; gandhi and nevo2021; aguirregabiria, collard-wexler and ryan2028\n\n\n\nFri\n\n\n\n\nTrain (~simulation)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFri\n\n\n\n\nackerberg, benkard, berry and pakes2007\n\n\n\n\n\n\n\n\n\n\n\n\n6\nFri\nJun 13\ndue-date\nprice competition\n\n\n\n\n\n\n\n\n\n\ntirole; ackerberg, benkard, berry and pakes2007; gandhi and nevo2021; aguirregabiria, collard-wexler and ryan2029\n\n\n\nFri\n\n\n\n\nprice competition\n\n\n\n\n\n\n\n\ntirole; ackerberg, benkard, berry and pakes2007; gandhi and nevo2021; aguirregabiria, collard-wexler and ryan2030\n\n\n\nFri\n\n\n\n\nTrain (~individual)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFri\n\n\n\n\nackerberg, benkard, berry and pakes2007\n\n\n\n\n\n\n\n\n\n\n\n\n7\nFri\nJun 20\ndue-date\nproduct differentiation\n\n\n\n\n\n\n\n\n\n\ntirole; ackerberg, benkard, berry and pakes2007; gandhi and nevo2021; aguirregabiria, collard-wexler and ryan2031\n\n\n\nFri\n\n\n\n\nproduct differentiation\n\n\n\n\n\n\n\n\ntirole; ackerberg, benkard, berry and pakes2007; gandhi and nevo2021; aguirregabiria, collard-wexler and ryan2032\n\n\n\nFri\n\n\n\n\nmixtape (individual)\n\n\n\n\n\n\n\n\n\n\n\n\n8\nFri\nJun 27\ndue-date\nsearch\n\n\n\n\n\n\n\n\n\n\ntirole; ackerberg, benkard, berry and pakes2007; gandhi and nevo2021; aguirregabiria, collard-wexler and ryan2033\n\n\n\nFri\n\n\n\n\nsearch\n\n\n\n\n\n\n\n\ntirole; ackerberg, benkard, berry and pakes2007; gandhi and nevo2021; aguirregabiria, collard-wexler and ryan2034\n\n\n\nFri\n\n\n\n\nmixtape (individual)\n\n\n\n\n\n\n\n\n\n\n\n\n9\nFri\nJul 4\ndue-date\ndynamic competition\n\n\n\n\n\n\n\n\n\n\ntirole; ackerberg, benkard, berry and pakes2007; gandhi and nevo2021; aguirregabiria, collard-wexler and ryan2035\n\n\n\nFri\n\n\n\n\ndynamic competition\n\n\n\n\n\n\n\n\ntirole; ackerberg, benkard, berry and pakes2007; gandhi and nevo2021; aguirregabiria, collard-wexler and ryan2036\n\n\n10\nFri\nJul 11\ndue-date\nentry, exit\n\n\n\n\n\n\n\n\n\n\ntirole; ackerberg, benkard, berry and pakes2007; gandhi and nevo2021; aguirregabiria, collard-wexler and ryan2037\n\n\n\nFri\n\n\n\n\nentry, exit\n\n\n\n\n\n\n\n\ntirole; ackerberg, benkard, berry and pakes2007; gandhi and nevo2021; aguirregabiria, collard-wexler and ryan2038\n\n\n11\nFri\nJul 18\ndue-date\n-\n\n\n\n\n\n\n\n\ntirole; ackerberg, benkard, berry and pakes2007; gandhi and nevo2021; aguirregabiria, collard-wexler and ryan2039\n\n\n12\nFri\nJul 25\ndue-date\n-\n\n\n\n\n\n\n\n\ntirole; ackerberg, benkard, berry and pakes2007; gandhi and nevo2021; aguirregabiria, collard-wexler and ryan2040\n\n\n13\nFri\nAug 1\ndue-date\n-\n\n\n\n\n\n\n\n\ntirole; ackerberg, benkard, berry and pakes2007; gandhi and nevo2021; aguirregabiria, collard-wexler and ryan2041\n\n\n14\nFri\nAug 8\ndue-date\n-\n\n\n\n\n\n\n\n\ntirole; ackerberg, benkard, berry and pakes2007; gandhi and nevo2021; aguirregabiria, collard-wexler and ryan2042\n\n\n15\nFri\nAug 15\ndue-date\n-\n\n\n\n\n\n\n\n\ntirole; ackerberg, benkard, berry and pakes2007; gandhi and nevo2021; aguirregabiria, collard-wexler and ryan2043\n\n\n16\nFri\nAug 22\ndue-date\n-\n\n\n\n\n\n\n\n\ntirole; ackerberg, benkard, berry and pakes2007; gandhi and nevo2021; aguirregabiria, collard-wexler and ryan2044\n\n\n0\nNA\n\n\ndue-date\nreplication\n\n\n\n\n\n\nnevo2001; caliendo, dvorkin and parro2019\n\n\n0\nNA\n\n\ndue-date\nps\n\n\n\n\n\n\n\n\n\n\n0\nNA\n\n\nNOTES\nsummary",
    "crumbs": [
      "Study",
      "IO theory"
    ]
  },
  {
    "objectID": "study/overview.html",
    "href": "study/overview.html",
    "title": "Overview",
    "section": "",
    "text": "This section is a set of materials I used for studying.",
    "crumbs": [
      "Study",
      "Overview"
    ]
  },
  {
    "objectID": "prepare/notes-spatial.html",
    "href": "prepare/notes-spatial.html",
    "title": "Notes",
    "section": "",
    "text": "It is interesting that MTO shows that MTO is only beneficial to children who are young. At first, MTO project was understood as a failed experiment because it had no significant effect on the marginalized households that moved to better neighborhoods. However, Chetty, Hendren, and Katz (2016) showed that young children (under 13) that moved due to the MTO project had higher college attendance and earnings and lower single parenthood rates. The paper hints that it might have something to do with the disturbance effect. There are tradeoffs from moving: while moving to better neighborhoods give children better environment, it also leads to disruption effect where the children has to move away from their familiar environment. Considerng this tradeoff, it is likely that younger children are less affected by the disruption effect as their duration of living in poor neighborhoods is shorter."
  },
  {
    "objectID": "prepare/notes-spatial.html#mto-project",
    "href": "prepare/notes-spatial.html#mto-project",
    "title": "Notes",
    "section": "",
    "text": "It is interesting that MTO shows that MTO is only beneficial to children who are young. At first, MTO project was understood as a failed experiment because it had no significant effect on the marginalized households that moved to better neighborhoods. However, Chetty, Hendren, and Katz (2016) showed that young children (under 13) that moved due to the MTO project had higher college attendance and earnings and lower single parenthood rates. The paper hints that it might have something to do with the disturbance effect. There are tradeoffs from moving: while moving to better neighborhoods give children better environment, it also leads to disruption effect where the children has to move away from their familiar environment. Considerng this tradeoff, it is likely that younger children are less affected by the disruption effect as their duration of living in poor neighborhoods is shorter."
  },
  {
    "objectID": "prepare/notes-spatial.html#agglomeration-externality",
    "href": "prepare/notes-spatial.html#agglomeration-externality",
    "title": "Notes",
    "section": "Agglomeration externality",
    "text": "Agglomeration externality"
  },
  {
    "objectID": "teaching/teaching.html",
    "href": "teaching/teaching.html",
    "title": "Class name",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the semester. Note that this schedule will be updated as the semester progresses and the timeline of topics and assignments might be updated throughout the semester. Syllabus\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nweek\ndow\ndate\nwhat\ntopic\nprepare\nslides\nhw\nhw_sol\nexam\nnotes\n\n\n\n\n1\nM\nAug 29\nLab\nNo lab\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nT\nAug 30\nLec 1\nWelcome to STA 199\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nSep 1\nLec 2\nMeet the toolkit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2\nM\nSep 5\nLab 0\nHello R!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelease: Lab 0\n\n\n\nT\nSep 6\nLec 3\nGrammar of graphics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nSep 8\nLec 4\nVisualizing various types of data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelease: HW 1\n\n\n\nF\nSep 9\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: Lab 0 + AE 1\n\n\n\nSu\nSep 11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: AE 2\n\n\n3\nM\nSep 12\nLab 1\nData visualization\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelease: Lab 1\n\n\n\nT\nSep 13\nLec 5\nGrammar of data wrangling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nSep 15\nLec 6\nWorking with multiple data frames\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: HW 1 / Release: HW 2\n\n\n\nF\nSep 16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: Lab 1\n\n\n4\nM\nSep 19\nLab 2\nData wrangling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelease: Lab 2\n\n\n\nT\nSep 20\nLec 7\nTidying data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nSep 22\nLec 8\nData types and classes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue HW 2\n\n\n\nF\nSep 23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: Lab 2\n\n\n5\nM\nSep 26\nLab 3\nData tidying\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelease: Lab 3\n\n\n\nT\nSep 27\nLec 9\nImporting and recoding data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nSep 29\nLec 10\nExam 1 Review\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelease: Exam 1 at 12pm\n\n\n\nF\nSep 30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: Lab 3\n\n\n6\nM\nOct 3\nLab\nNo lab - Work on Exam 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: Exam 1 at 2pm\n\n\n\nT\nOct 4\nLec 11\nData science ethics - Misrepresentation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nOct 6\nLec 12\nData science ethics - Algorithmic bias + data privacy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelease: HW 3\n\n\n7\nM\nOct 10\nLab\nNo lab - Fall break\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nT\nOct 11\nLecture\nNo Lec - Fall break\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nOct 13\nLec 13\nWeb scraping\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: HW 3\n\n\n8\nM\nOct 17\nLab\nWork on project proposal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nT\nOct 18\nLec 14\nFunctions + iteration\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nOct 20\nLec 15\nThe language of models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nF\nOct 21\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: Project proposal\n\n\n9\nM\nOct 24\nLab 4\nProbability + Simpson's Paradox\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelease: Lab 4\n\n\n\nT\nOct 25\nLec 16\nModels with a single predictor\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nOct 27\nLec 17\nModels with multiple predictors\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelease: HW 4\n\n\n\nF\nOct 28\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: Lab 4\n\n\n10\nM\nOct 31\nLab 5\nPredicting a numerical outcome\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelease: Lab 5\n\n\n\nT\nNov 1\nLec 18\nModels with multiple predictors + Overfitting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nNov 3\nLec 19\nLogistic regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: HW 4 / Release: HW 5\n\n\n\nF\nNov 4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: Lab 5 / Release: HW 6\n\n\n\nS\nNov 5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: Team peer evaluations 1\n\n\n11\nM\nNov 7\nLab\nWork on project draft\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nT\nNov 8\nLec 20\nQuantifying uncertainty with bootstrap intervals\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nNov 10\nLec 21\nHypothesis testing via simulation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: HW 5\n\n\n\nF\nNov 11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: Project draft 1\n\n\n12\nM\nNov 14\nLab 6\nPrediction + Bootstrapping\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelease: Lab 6\n\n\n\nT\nNov 15\nLec 22\nInference overview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nNov 17\nLec 23\nExam 2 Review\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelease: Exam 2 at 12pm\n\n\n\nF\nNov 18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: Lab 6\n\n\n13\nM\nNov 21\nLab\nNo lab - Work on Exam 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: Exam 2 at 2pm\n\n\n\nT\nNov 22\nLec 24\nNo lecture - Work on projects\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nNov 24\nLecture\nNo lecture - Thanksgiving\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nF\nNov 25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelease: Exam retake (optional)\n\n\n\nSu\nNov 27\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: Project draft 2 (optional)\n\n\n14\nM\nNov 28\nLab\nWork on project peer review\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: Project peer review\n\n\n\nT\nNov 29\nLec 25\nCommunicating data science results effectively\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: Team peer evaluations 2\n\n\n\nTh\nDec 1\nLec 26\nCustomizing Quarto reports and presentations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n15\nM\nDec 5\nLab\nProject presentations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nT\nDec 6\nLec 25\nLooking further: Text analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTh\nDec 8\nLec 26\nLooking further: Interactive web applications with Shiny\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: Project everything\n\n\n\nF\nDec 9\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: HW 6 / Statistics experience\n\n\n16\nTh\nDec 15\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue: Exam retake (optional)",
    "crumbs": [
      "Teaching",
      "Class-name",
      "Overview"
    ]
  },
  {
    "objectID": "teaching/metrics.html",
    "href": "teaching/metrics.html",
    "title": "Metric theory",
    "section": "",
    "text": "This pages contains log for my self-study on metric theory.",
    "crumbs": [
      "Teaching",
      "Mathematics",
      "Metric theory resources"
    ]
  },
  {
    "objectID": "teaching/set.html",
    "href": "teaching/set.html",
    "title": "Set theory",
    "section": "",
    "text": "This pages contains log for my self-study on set theory.",
    "crumbs": [
      "Teaching",
      "Mathematics",
      "Set theory resources"
    ]
  },
  {
    "objectID": "teaching/set.html#chapter-1-sets",
    "href": "teaching/set.html#chapter-1-sets",
    "title": "Set theory",
    "section": "Chapter 1: Sets",
    "text": "Chapter 1: Sets\n\n3. The axioms\n\n3.1\nLet \\(P(x, A, B)\\) be the property: \\(x \\in A, \\, x \\notin B\\). Then this property implies that \\(x \\in A\\). Therefore, \\(\\{ x \\mid x \\in A, x \\in B \\} = \\{ x \\in A \\mid x \\in A, x \\in B \\} = \\{ x \\in A \\mid x \\in B \\}\\). This set exists by the Axiom schema of comprehension.\n\n\n3.2\nLet \\(A\\) be a set known to exist. Then using property \\(P(x) = x \\neq x\\), we can use Axiom schema of comprehension to say that there exists set \\(B = \\{ x \\in A \\mid x \\neq x \\}\\). Then we can easily see that such set is empty.\n\n\n3.3 (a)\nSuppose that there is a set of all sets \\(V\\). Then using Axiom schema of comprehension, there exists a set \\(T = \\{ x \\in V \\mid x \\notin x \\}\\). Now we will show contradiction by saying \\(T \\notin V\\) since this would mean \\(T\\) is not a set. Suppose \\(T \\in V\\). Suppose \\(T \\notin T\\). Then \\(T \\in T\\), contradiction. Suppose \\(T \\in T\\). Then \\(T \\notin T\\) must hold, again contradiction. Thus \\(T \\notin V\\).\n\n\n3.3 (b)\nThis is easily proven because if there is some set \\(A\\) where \\(x \\in A\\) for all \\(x\\), this would mean \\(A\\) is set of all sets.\n\n\n3.4\nThis can be proven by doing axiom of schema comprehension and then doing axiom of union.\n\n\n3.5\nThis can be proven by doing Axiom of pair and union.\n\n\n3.6\nWe will use the hint. we can clearly see that \\(Y \\in p(x)\\) since \\(Y\\) by definition is a subset of \\(X\\). Then we can derive contradiction by checking if \\(Y \\in Y\\). This logic is similar to the previous proofs that showed that “set of all sets” does not exist.\n\n\n3.7\nConsult link.\n\n\n\n4. Elementary operations on sets\n\n4.4\nSuppose that \\(A^c\\) exists. Then by axiom of union, the union of \\(A\\) and \\(A^c\\) exists. But this is set of all sets. Contradiction.\n\n\n4.6\nConsult link.",
    "crumbs": [
      "Teaching",
      "Mathematics",
      "Set theory resources"
    ]
  },
  {
    "objectID": "teaching/set.html#chapter-2",
    "href": "teaching/set.html#chapter-2",
    "title": "Set theory",
    "section": "Chapter 2",
    "text": "Chapter 2\n\n1. Ordered pairs\n\n1.1\nWe will just prove the general one. Let \\(a \\in A\\) and \\(b \\in A\\) (Note that in set theory, everything is a set). Then by axiom of pair, \\(\\{ a\\}\\) and \\(\\{a,b\\}\\) exists and they are elements of \\(p(A)\\). We can again do axiom of pair to finish the proof.\n\n\n1.2\nThis is just proven by multiple application of axiom of pairing.\n\n\n1.3\nSuppose \\(a \\neq b\\). Then as \\(\\{a \\} = \\{b\\}\\), this implies that \\(a=b\\), contradiction.\n\n\n1.4, 1.5, 1.6\nConsult link.\n\n\n\n2. Relations\n\n2.1\n\\((x,y) \\in R\\) implies that \\(\\{x \\}, \\{ x,y\\} \\in \\cup R\\). Thus we have \\(x,y \\in A\\). Then we can use axiom schema comprehension to show that \\(\\text{dom} R\\) exists. Same logic goes for \\(\\text{ran} R\\).\n\n\n2.2 (a)\nJust use the hint and use axiom of schema comprehension.\n\n\n2.2 (b)\nNote that \\(A \\times B\\) exists. thus \\(A \\times B \\times C = (A \\times B) \\times C\\) will also exist. We can explicitly define it as\n\\[\n  A \\times B \\times C = \\left\\{ (a,b,c) \\in p \\left[ p \\left[ (p(p(A \\cup B))) \\cup C \\right] \\right]: a \\in A, b \\in B, c \\in C  \\right\\}.\n\\]\n\n\n2.3 (a)\n\\(y \\in R [A \\cup B]\\) iff there is \\(x \\in A \\cup B\\) s.t. \\((x,y) \\in R\\). This holds if and only if \\(x \\in A\\) or \\(x \\in B\\) s.t. \\((x,y) \\in R\\). Then this shows that \\(x \\in R[A] \\cup R[B]\\).\n\n\n2.3 (b)\nAgain suppose \\(y \\in R[A \\cap B]\\). Then there is \\(x \\in A \\cap B\\) s.t. \\((x,y) \\in R\\). This means there is \\(x\\in A\\) and \\(x \\in B\\) such that \\((x,y) \\in R\\).\n\n\n2.3 (c)\nLet \\(y \\in R[A] - R[B]\\). Then there is \\(x \\in A, x \\notin B\\) s.t. \\((x,y) \\in R\\). This is literally \\(R[A-B]\\).\n\n\n2.3 (d)\nFor (b), think of a case where there is a point \\((x1,y)\\) and \\((x2,y)\\) where \\(x1 \\in A - B\\) and \\(x2 \\in B-A\\). Then this would be in \\(R[A] \\cap R[B]\\) but \\(R[A \\cap B]\\).\nFor (c), think of a case where there is some point \\(x \\in A-B\\) and \\(y \\in A \\cap B\\) where it corresponds to same value. Then for \\(R[A] - R[B]\\) will not have this value although it will be in \\(R[A-B]\\).\n\n\n2.3 (f)\nLet \\(x \\in A \\cap \\text{dom} R\\). Then \\(x \\in A\\) and there is \\(y\\) s.t. \\(xRy\\). This implies that \\(y \\in R[A]\\) by definition. Then as we know there is \\(x\\) s.t. \\(yR^{1}x\\), we get \\(x \\in R^{-1} \\left[ R[A] \\right]\\). Similar logic holds for range. We can easily see that equality will not hold for cases such as \\(x\\) where \\(x \\notin A\\) but there is \\(y\\) s.t. \\(xRy\\). That is, the range value is inside \\(R[A]\\).\n\n\n2.4 (a)\nLet \\(y \\in R[X]\\). Then \\((x,y) \\in R\\). Thus \\(y \\in \\text{ran} R\\). Note that this if iff relation to results go through. Similar to domain case.\n\n\n2.4 (b)\nSuppose \\(R^{-1}[\\{ b \\}]\\) is nonempty. Then there is \\(a\\) s.t. \\(\\{b, a \\} \\in R\\). Then \\(a \\in \\text{ran } R\\). Contradiction.\n\n\n2.4 (c)\n\\(x \\in \\text{dom } R\\). This means \\((x,y) \\in R\\). This implies \\((y,x) \\in R^{-1}\\). So \\(x\\) is also element of \\(\\text{ran } R^{-1}\\).\n\n\n2.4 (d)\nWe can easily prove it by noting that \\((x,y) \\in R \\iff (y,x) \\in R^{-1}\\).\n\n\n\n3. Functions\n\n3.1\nLet \\(x \\in \\text{dom } g \\circ f\\). Then there exists \\(z\\) s.t. \\(x(g \\circ f)z\\). Then there exists \\(y\\) s.t. \\(xfy, ygz\\). So this implies that \\(x \\in \\text{dom } f\\).\nNow suppose \\(x \\in \\text{dom } f\\). Then there is \\(y\\) s.t. \\(xfy\\). As \\(ran f \\subset dom g\\), there is \\(z\\) s.t. \\(ygz\\). So \\(x(g \\circ f)z\\).\n\n\n3.4 (a)\nLet \\((x,y) \\in f^{-1} \\circ f\\). Then there is \\(z\\) s.t. \\(xfz\\), \\(zf^{-1}y\\) which means \\(yfz\\). This means \\(x=x\\). So \\(x \\in Id\\).\nOTH, if \\((x,x) \\in Id\\), there is \\(y\\) s.t. \\(xfy\\) and \\(yf^{-1}x\\). So \\(x \\in f^{-1} \\circ f\\).\nI also proved 3.4 (b), 3.10 but lost the paper that I wrote it down and am too lazy to do it again. Contact me if you need them.\n\n\n\n4. Equivalence relations\n\n4.1 (c)\nReflexive: It does not hold. Since \\(x=x\\), \\(xRx\\) cannot hold.\nSymmetric: It holds since \\(x \\neq y\\) and \\(y \\neq x\\).\nTransitive: Does not hold. Think of \\(x=3=z\\) and \\(y=4\\).\n\n\n4.1 (e,f)\nThese are equivalence relations because the conditions of the empty sets make the claims vacuously true.\n\n\n4.2 (a)\nReflexive: We can easily see that \\(f(x)=f(x)\\).\nSymmetry: Let \\(xEy\\). Then as \\(f(x)=f(y)\\), this implies \\(f(y)=f(x)\\). Thus \\(yEx\\).\nTransitive: Let \\(xEy\\) and \\(yEz\\). Then we have \\(f(x)=f(y)=f(z)\\). Thus \\(xEz\\).\nHence \\(E\\) is equivalence relation on \\(A\\).\n\n\n4.2 (b)\nLet \\([a]_E = [a']_E\\). Then \\(\\phi([a]_E) = f(a)\\) and \\(\\phi([a']_E) = f(a')\\). Then \\(f(a) = f(a')\\) has to hold by definition of the equivalence.\n\n\n4.2 (c)\nLet \\((x,y) \\in f\\). Since \\(j\\) is a function on \\(A\\), there will be some \\(z \\in A/E\\) s.t. \\(xjz\\). In fact, \\(z = [x]_E\\) by definition. Then as \\(f(x) = \\phi([x]_E)\\), it is done.\nIf \\((x,z) \\in \\phi \\circ j\\), there is some \\(y\\) s.t. \\(j(x) = [x]_E = y\\). Also, \\(\\phi([x]_E) = f(x) = z\\). Thus done.",
    "crumbs": [
      "Teaching",
      "Mathematics",
      "Set theory resources"
    ]
  },
  {
    "objectID": "teaching/topology.html",
    "href": "teaching/topology.html",
    "title": "Topology",
    "section": "",
    "text": "This pages contains log for my self-study on topology.",
    "crumbs": [
      "Teaching",
      "Mathematics",
      "Topology resources"
    ]
  }
]