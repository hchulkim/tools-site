---
title: "BLP Demystified: From Basics to Brain-Busting Models!"
author: "Hyoungchul Kim"
engine: julia
execute: 
  freeze: auto
format: 
  html:
    toc: false
    number-sections: true
    code-overflow: wrap
bibliography: references.bib
---

Install necessary julia packages for later computation:
```{julia}
#| echo: true
#| warning: false

# You might need these commented codes to install packages
# using Pkg
# Pkg.add(["DataFrames", "CSV", "GLM", "Statistics", "LinearAlgebra", "Distributions", "NLopt", "FixedEffectModels", "RegressionTables"])
```

# BLP

This exercise estimates the demand-side BLP model.

## Motivation

Why do this? Demand estimation is very important in IO literature because measuring market power is important in IO. How do we quantify market power? Usually we use markup as the measure. But it is hard to directly calculate markup because it depends on the cost function of the firm which is not observed. But IO theory shows that we can actually get the markup using demand elasticity. Thus estimating demand is important. 

## Basic: @mcfadden74 style logit model

### Model setup

We will first estimate a basic logit model with no unobserved demand shifters and no random coefficents. But let's just talk bit about the background of this discrete choice model. Note that most of it is from @train. 

Even before @mcfadden74, there has been a long history of the development of the logit model. But @mcfadden74 provides a complete, well-defined econometric model that is consistent with the utility maximization behavior of individuals.


Individual's ($i$) utility maximizing behavior (indirect utility) can be specified as follows:

$$
u_{ij} = \underbrace{x_j \beta + \alpha p_j}_{\delta_j} + \varepsilon_{ij}
$$

where mean utility of outside option is normalized to zero. Also, idiosyncratic shock (i.i.d) follows Type 1 Extreme Value distribution (T1EV). We also assume there are $0, \ldots, J$ products (denote $0$ as the outside option) where one option is outside option. We can think of $\delta_j$ as the mean utility from the product $j$. This is because in this parameterization, $\delta_j$ does not depend on $i$.

Now let's do some math to derive the logit choice probabilities. One benefit about logit model is that we can get a close-form solution. We are going to compute the probability of individuals choosing product $j$ given $p_j$, and $x_j$.

\begin{align}
  P (u_{ij} \geq \forall_{j' \neq j} u_{ij'} \mid x_j, p_j) &= P (x_j \beta + \alpha p_j + \varepsilon_{ij} \geq \forall_{j' \neq j} x_{j'}\beta + \alpha p_{j'} + \varepsilon_{ij'} \mid x_j, p_j) \\
  &= P ( \varepsilon_{ij'} \leq \varepsilon_{ij} + \delta_j - \delta_{j'} \, \forall j' \neq j).
\end{align}

If we assume that $\varepsilon_{ij}$ is given, we can think of the last term as the cumulative distribution of the T1EV where $F(\varepsilon_{ij}) = e^{-e^{- \varepsilon_{ij}}}$. Since we assumed i.i.d., we can express the last term as the product of the individual cumulative distributions (For brevity, we will now denote the conditional logit choice probability as $P_{ij}$):

$$
  P_{ij} \mid \varepsilon_{ij} = \prod_{j' \neq j} e^{ - e^{-(\varepsilon_{ij} + \delta_j - \delta_{j'})}}.
$$

Since $\varepsilon_{ij}$ is not given, we need o integrate it over density of $\varepsilon_{ij}$:

$$
  P_{ij} = \int \left(\prod_{j' \neq j} e^{ - e^{-(\varepsilon_{ij} + \delta_j - \delta_{j'})}} \right) e^{- \varepsilon_{ij}} e^{-e^{\varepsilon_{ij}}} d \varepsilon_{ij}.
$$

Now let's get this into a closed-form expression:

As a result, we can get the closed-form expression:

$$
  P_{ij} = \frac{e^{\delta_{ij}}}{\sum_{j'} e^{\delta_{ij'}}}
$$

This could be understood as the *predicted share* function given the fixed values of the parameters.

Note that this is a very simple model because we are not assuming any unobserved product demand shifters that could be affected the utility gained from the product. In fact, we are assuming that econometricians can fully observe all the necessary variables that constructs the mean utility. Thus there is not much econometrics involved. You can just get the parameters as follows:

1. Assuming you have the data on market share, you can use it to match it to $P_{ij} \cdot M$ where $M$is the total market size.

2. Then since we will get $J$ equations using $J$ market share, we can do simple algebra to get the mean utility $\delta_j$.

3. Then you can do some nonlinear least squares that minimize the sum of the differences between oberved and predicted shares of all products. This will get you the parameters that best fit the data.

### Adding unobserved demand shifters

We can add the additional unobserved variables $\xi_j$ which can be thought of as some demand shifter for product $j$. This allows the model to be more flexible to incorporate the realistic situation where econometrician might not be able to observe some components that might be affecting the utility of getting some product. Thus most of what we did above does not change much. The only problem would be understanding the nature of this unobserved terms with other main parameters of interest. If there is endogeneity, we would need some IV to estimate the parameter. In this section, we will do both cases (OLS, IV).

### Computation (Following @berry1994)

So how can we retrieve the parameters of interest? Naive way to think about it would be doing some **nonlinear least squares** where you minimize the sum of differences between predicted share and observed shares of all products. The problem is that this directy way is implausible: You would need to know the $\xi_j$. Since this is unobservable, it is problematic.

**This is where @berry1994 comes in.** He introduces this clever two steps estimation process.

**Step 1: Inversion**

Notation: Let $\hat{s}_j (\delta)$ be predicted shares and let $s_j$ be observed shares.[^1]

Then you can use the system of equations from matching actual to predicted shares and invert them to get the mean utility. For this simple case, we can get the following equations:

$$
  \delta_j = \log s_j - \log \hat{s}_0, \quad j = 1, \ldots, J.
$$

So this inversion gets us the value of the mean utility. Then we have the second step.

**Step 2: IV estimation**

By definition, we have $\delta_j = x_j \beta + \alpha p_j + \xi_j$. So we can do the regression to retrieve the parameters. I put IV, but this could be just OLS if you can assume the unobserved term is uncorrelated with the price.

[^1]: You might have already noticed, but I kind of use variables without subscript as the vector of the variables. For example, $\delta$ is just $(\delta_1, \ldots, \delta_J).$

### Coding (with `Julia`)

Finally we will do some coding to get the result we just talked about.

```{julia}
#| echo: true
#| warning: false 
 
using FixedEffectModels, DataFrames, CSV, RegressionTables 

# read in the data
otc = CSV.read("data/otc.csv", DataFrame)

# Run regressions
ols1 = reg(otc, @formula(ln_mkt_share_diff ~ price + promotion)) 
ols2 = reg(otc, @formula(ln_mkt_share_diff ~ price + promotion + fe(product)))
ols3 = reg(otc, @formula(ln_mkt_share_diff ~ price + promotion + fe(product) + fe(store)))
iv1 = reg(otc, @formula(ln_mkt_share_diff ~ (price ~ cost) + promotion))
iv2 = reg(otc, @formula(ln_mkt_share_diff ~ (price ~ cost) + promotion + fe(product)))

regtable(ols1, ols2, ols3, iv1, iv2, order = ["price"], drop = ["(Intercept)"], regression_statistics = [FStatIV, Nobs, R2],
  labels = Dict(
    "price" => "Price",
    "promotion" => "Promotion",
    "ln_mkt_share_diff" => "Log Mkt share difference"
  ))
## Some R codes that I followed

# m1 <- lm(ln_mkt_share_diff ~ price + promotion , data = otc)
# m2 <- lm(ln_mkt_share_diff ~ price + promotion + factor(product), data = otc)
# m3 <- lm(ln_mkt_share_diff ~ price + promotion + factor(product) + factor(store), data = otc)
# m4 <- ivreg(ln_mkt_share_diff ~ price + promotion | . - price + cost, data = otc)
# m5 <- ivreg(ln_mkt_share_diff ~ price + promotion + factor(product) | . - price + cost, data = otc)
# stargazer(m1, m2, m3, m4, m5, 
#           omit = c("product", "store"),
#           type = "text")

```

### Caveats

But we don't usually use this basic setup in IO. This is because the model is bit too simple to fully capture the reality. One of the well known problem is the **Independence of irrelevant alternatives (IIA)**. Basically what this means is that we don't get a realistic demand elasticities. If you want to know more about it, google the famouse ***Red bus, blue bus*** story. 

### Solutions?

There are some ways to alleviate this problem. One of them (which we will not discuss), is using nested logit. Basically we are defining certain group of products where IIA holds within the group but may not hold across the group. So for the case of red bus, blue bus, they would be in a same group.

Another way is to do enhance the random utility model into logit model with random coefficients. In essence, this is sort of introducing preference heterogeneity of consumers into the model. This is done by interacting consumer preferences with product characteristics. The nuisance with this case is that now closed-form expression for choice probability is not obtainable. We need to do some numerical computation.

## Advanced: @blp (Random coefficients logit model)

We again start with the individual utility function. But now something is added (we will now also explicitly denote markets as $t$):

$$
u_{ijt} = x_{jt} \beta_{it} + \alpha p_{jt} + \xi_{jt} + \varepsilon_{ijt}
$$

The difference is that slope coefficients can now vary across individuals $i$. For now, we will assume $\beta_{it}^k = \beta_0^k + \sigma_{kt} v_{it}^k$. We now have $k$ which is the dimension of $\beta$. $\beta_0^k$ are fixed taste for characteristics $k$ and $v_{it}^k$ are random tastes that follow standard normal distribution.

Now we can expand the model:

\begin{align}
  u_{ijt} &= (x_{j1t}, \ldots, x_{jKt}) \cdot (\beta_{0}^1 + \sigma_1 v_{it}^1, \ldots, \beta_{0}^K + \sigma_K v_{it}^K)^T + \alpha p_{jt} + \xi_{jt} + \varepsilon_{ijt}\\
  &= x_{jt}\beta_{it} + \sum_k x_{jkt} \sigma_{k}v_{ikt} + \alpha p_{jt} + \xi_{jt} + \varepsilon_{ijt}\\
  &= \underbrace{x_{jt}\beta_{it} + \alpha p_{jt} + \xi_{jt}}_{\delta_{jt}} + \underbrace{\sum_k x_{jkt} \sigma_{k}v_{ikt}}_{\mu_{ijt}} +  \varepsilon_{ijt}.

\end{align}

We can easily see that this is just an extension of what we did for the basic random utility model. Indirect utility is made up of mean utility $\delta_{jt}$ and random coefficient term $\mu_{ijt} + \varepsilon_{ijt}$.

Now we will make some simplication. We will assume that characteristics dimension of individual is one: $K = 1$. Using this simplication, we can again use the assumption that idiosyncratic shock follows T1EV to get aggregate share:

$$
s_{jt} = \int \frac{\exp(\delta_{jt} + x_{jt} \sigma_t v_{it})}{1 + \sum_j \exp(\delta_{jt} + x_{jt} \sigma_t v_{it})} f(v_i)dv_i
$$

The integral has no analytical solution in the random coefficient model, so we
need to compute the integral by simulation. One way to do it is as follows:

$$
\hat{s}_{jt} = \frac{1}{ns} \sum_{i=1}^{ns} \frac{\exp(\delta_{jt} + x_{jt} \sigma_t v_{it})}{1 + \sum_j \exp(\delta_{jt} + x_{jt} \sigma_t v_{it})}
$$

where $ns$ is number of random draws from $v_i$.

Now we can see the inversion method we did before is not easy to implement. This is because we now have additional parameters that we do not know the values.

So in BLP, we need to do **nested estimation algorithm**.

1. In the **outer loop**, we iterate over different values of the parameters.

2. In the **inner loop**, for a given parameter value, we do the inversion to get the mean utility and estimate the GMM objective function.

3. We keep on doing this iteration until we get the parameters that minimize the GMM function.

Now let's do some coding!

### Another coding (with `Julia`)

This portion of the code is from [here](https://github.com/leima0521/baby_BLP)

```{julia}
#| echo: true
 
###############################################################################
####  BLP fixed-point algorithm, inverting mkt shares to get mean utility  ####
###############################################################################
using CSV
using DataFrames
using GLM
using Statistics
using LinearAlgebra
using Distributions
using NLopt
otc = CSV.read("data/otc.csv", DataFrame)

ns = 500;
nmkt = maximum(otc.mkt);
mkt = unique(otc.mkt);
nprod = maximum(otc.product);

vi = quantile.(Normal(), collect(range(0.5/ns, step = 1/ns, length = ns)));
sigma = 1;

function calc_mkt_share_t(delta_t, sigma_t, x_t, vi_t)
    # Dimension: delta_t 11*1, simga_t 1*1, x_t 11*1
    delta_t = delta_t .* ones(nprod, ns)
    mu_t = x_t*sigma_t*vi_t'
    numerator = exp.(delta_t .+ mu_t)
    denominator = ones(nprod, ns) .+ sum(numerator, dims = 1)
    mkt_share_t = mean(numerator./denominator, dims = 2)
end

function contraction_t(d0, sigma_t, x_t, vi_t, mkt_t, tol = 1e-5, maxiter = 1e5)
    obs_mkt_share_t = mkt_t.mkt_share
    d_old = d0
    normdiff = Inf
    iter = 0
    while normdiff > tol && iter <= maxiter
        model_mkt_share_t = calc_mkt_share_t(d_old, sigma_t, x_t, vi_t)
        d_new = d_old .+ log.(obs_mkt_share_t) .- log.(model_mkt_share_t)
        normdiff = maximum(norm.(d_new .- d_old))
        d_old = d_new
        iter += 1
    end
    return d_old
end

function calc_delta(sigma)
    delta_fp = zeros(nprod, nmkt);
    for t in mkt
        mkt_t = otc[otc.mkt .== t, :];
        x_t = ones(nprod, 1);
        delta_t = zeros(nprod, 1);
        sigma_t = sigma;
        vi_t = vi;
        delta_fp[:, t] = contraction_t(delta_t, sigma_t, x_t, vi_t, mkt_t);
    end
    return vec(delta_fp);
end

@time delta_fp = calc_delta(sigma);
mean(delta_fp)
std(delta_fp)
```

```{julia}
#| echo: true
 
################################################################
#### Estimate beta and sigma using GMM (cost as instrument) ####
################################################################
X = hcat(ones(nprod*nmkt, 1),
         otc.price, otc.promotion,
         otc.product_2, otc.product_3, otc.product_4, otc.product_5,
         otc.product_6, otc.product_7, otc.product_8, otc.product_9,
         otc.product_10, otc.product_11);
z = hcat(X, otc.cost);
Phi = z'*z/1056;
inv_Phi = inv(Phi);

function GMMObjFunc(theta2::Vector, grad::Vector)
    sigma = theta2[1]
    delta = calc_delta(sigma)
    theta1 = inv(X'*z*inv_Phi*z'*X)*X'*z*inv_Phi*z'*delta
    error = delta - X*theta1
    obj = error'*z*inv_Phi*z'*error
    return obj
end

opt = Opt(:LN_COBYLA, 1)
opt.xtol_rel = 1e-4
opt.lower_bounds = [0.00001]
opt.min_objective = GMMObjFunc
@time (minf,minx,ret) = optimize(opt, [1])

@show sigma = minx[1]
delta = calc_delta(sigma[1]);
theta1 = inv(X'*z*inv_Phi*z'*X)*X'*z*inv_Phi*z'*delta
```

## References {.unnumbered}

::: {#refs}
:::