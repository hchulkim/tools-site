{
  "hash": "5f87fa52baa4cc503bd4f7958fd0953d",
  "result": {
    "engine": "julia",
    "markdown": "---\ntitle: \"BLP Demystified: From Basics to Brain-Busting Models!\"\nauthor: \"Hyoungchul Kim\"\nengine: julia\nexecute: \n  freeze: auto\nformat: \n  html:\n    toc: false\n    number-sections: true\n    code-overflow: wrap\nbibliography: references.bib\n---\n\n\n\n\nInstall necessary julia packages for later computation:\n\n\n\n::: {#2 .cell execution_count=1}\n``` {.julia .cell-code}\n# You might need these commented codes to install packages\n# using Pkg\n# Pkg.add([\"DataFrames\", \"CSV\", \"GLM\", \"Statistics\", \"LinearAlgebra\", \"Distributions\", \"NLopt\", \"FixedEffectModels\", \"RegressionTables\"])\n```\n:::\n\n\n\n\n\n\n# BLP\n\nThis exercise estimates the demand-side BLP model.\n\n## Motivation\n\nWhy do this? Demand estimation is very important in IO literature because measuring market power is important in IO. How do we quantify market power? Usually we use markup as the measure. But it is hard to directly calculate markup because it depends on the cost function of the firm which is not observed. But IO theory shows that we can actually get the markup using demand elasticity. Thus estimating demand is important. \n\n## Basic: @mcfadden74 style logit model\n\n### Model setup\n\nWe will first estimate a basic logit model with no unobserved demand shifters and no random coefficents. But let's just talk bit about the background of this discrete choice model. Note that most of it is from @train. \n\nEven before @mcfadden74, there has been a long history of the development of the logit model. But @mcfadden74 provides a complete, well-defined econometric model that is consistent with the utility maximization behavior of individuals.\n\n\nIndividual's ($i$) utility maximizing behavior (indirect utility) can be specified as follows:\n\n$$\nu_{ij} = \\underbrace{x_j \\beta + \\alpha p_j}_{\\delta_j} + \\varepsilon_{ij}\n$$\n\nwhere mean utility of outside option is normalized to zero. Also, idiosyncratic shock (i.i.d) follows Type 1 Extreme Value distribution (T1EV). We also assume there are $0, \\ldots, J$ products (denote $0$ as the outside option) where one option is outside option. We can think of $\\delta_j$ as the mean utility from the product $j$. This is because in this parameterization, $\\delta_j$ does not depend on $i$.\n\nNow let's do some math to derive the logit choice probabilities. One benefit about logit model is that we can get a close-form solution. We are going to compute the probability of individuals choosing product $j$ given $p_j$, and $x_j$.\n\n\\begin{align}\n  P (u_{ij} \\geq \\forall_{j' \\neq j} u_{ij'} \\mid x_j, p_j) &= P (x_j \\beta + \\alpha p_j + \\varepsilon_{ij} \\geq \\forall_{j' \\neq j} x_{j'}\\beta + \\alpha p_{j'} + \\varepsilon_{ij'} \\mid x_j, p_j) \\\\\n  &= P ( \\varepsilon_{ij'} \\leq \\varepsilon_{ij} + \\delta_j - \\delta_{j'} \\, \\forall j' \\neq j).\n\\end{align}\n\nIf we assume that $\\varepsilon_{ij}$ is given, we can think of the last term as the cumulative distribution of the T1EV where $F(\\varepsilon_{ij}) = e^{-e^{- \\varepsilon_{ij}}}$. Since we assumed i.i.d., we can express the last term as the product of the individual cumulative distributions (For brevity, we will now denote the conditional logit choice probability as $P_{ij}$):\n\n$$\n  P_{ij} \\mid \\varepsilon_{ij} = \\prod_{j' \\neq j} e^{ - e^{-(\\varepsilon_{ij} + \\delta_j - \\delta_{j'})}}.\n$$\n\nSince $\\varepsilon_{ij}$ is not given, we need o integrate it over density of $\\varepsilon_{ij}$:\n\n$$\n  P_{ij} = \\int \\left(\\prod_{j' \\neq j} e^{ - e^{-(\\varepsilon_{ij} + \\delta_j - \\delta_{j'})}} \\right) e^{- \\varepsilon_{ij}} e^{-e^{\\varepsilon_{ij}}} d \\varepsilon_{ij}.\n$$\n\nNow let's get this into a closed-form expression:\n\nAs a result, we can get the closed-form expression:\n\n$$\n  P_{ij} = \\frac{e^{\\delta_{ij}}}{\\sum_{j'} e^{\\delta_{ij'}}}\n$$\n\nThis could be understood as the *predicted share* function given the fixed values of the parameters.\n\nNote that this is a very simple model because we are not assuming any unobserved product demand shifters that could be affected the utility gained from the product. In fact, we are assuming that econometricians can fully observe all the necessary variables that constructs the mean utility. Thus there is not much econometrics involved. You can just get the parameters as follows:\n\n1. Assuming you have the data on market share, you can use it to match it to $P_{ij} \\cdot M$ where $M$is the total market size.\n\n2. Then since we will get $J$ equations using $J$ market share, we can do simple algebra to get the mean utility $\\delta_j$.\n\n3. Then you can do some nonlinear least squares that minimize the sum of the differences between oberved and predicted shares of all products. This will get you the parameters that best fit the data.\n\n### Adding unobserved demand shifters\n\nWe can add the additional unobserved variables $\\xi_j$ which can be thought of as some demand shifter for product $j$. This allows the model to be more flexible to incorporate the realistic situation where econometrician might not be able to observe some components that might be affecting the utility of getting some product. Thus most of what we did above does not change much. The only problem would be understanding the nature of this unobserved terms with other main parameters of interest. If there is endogeneity, we would need some IV to estimate the parameter. In this section, we will do both cases (OLS, IV).\n\n### Computation (Following @berry1994)\n\nSo how can we retrieve the parameters of interest? Naive way to think about it would be doing some **nonlinear least squares** where you minimize the sum of differences between predicted share and observed shares of all products. The problem is that this directy way is implausible: You would need to know the $\\xi_j$. Since this is unobservable, it is problematic.\n\n**This is where @berry1994 comes in.** He introduces this clever two steps estimation process.\n\n**Step 1: Inversion**\n\nNotation: Let $\\hat{s}_j (\\delta)$ be predicted shares and let $s_j$ be observed shares.[^1]\n\nThen you can use the system of equations from matching actual to predicted shares and invert them to get the mean utility. For this simple case, we can get the following equations:\n\n$$\n  \\delta_j = \\log s_j - \\log \\hat{s}_0, \\quad j = 1, \\ldots, J.\n$$\n\nSo this inversion gets us the value of the mean utility. Then we have the second step.\n\n**Step 2: IV estimation**\n\nBy definition, we have $\\delta_j = x_j \\beta + \\alpha p_j + \\xi_j$. So we can do the regression to retrieve the parameters. I put IV, but this could be just OLS if you can assume the unobserved term is uncorrelated with the price.\n\n[^1]: You might have already noticed, but I kind of use variables without subscript as the vector of the variables. For example, $\\delta$ is just $(\\delta_1, \\ldots, \\delta_J).$\n\n### Coding (with `Julia`)\n\nFinally we will do some coding to get the result we just talked about.\n\n\n\n\n::: {#4 .cell execution_count=1}\n``` {.julia .cell-code}\nusing FixedEffectModels, DataFrames, CSV, RegressionTables \n\n# read in the data\notc = CSV.read(\"data/otc.csv\", DataFrame)\n\n# Run regressions\nols1 = reg(otc, @formula(ln_mkt_share_diff ~ price + promotion)) \nols2 = reg(otc, @formula(ln_mkt_share_diff ~ price + promotion + fe(product)))\nols3 = reg(otc, @formula(ln_mkt_share_diff ~ price + promotion + fe(product) + fe(store)))\niv1 = reg(otc, @formula(ln_mkt_share_diff ~ (price ~ cost) + promotion))\niv2 = reg(otc, @formula(ln_mkt_share_diff ~ (price ~ cost) + promotion + fe(product)))\n\nregtable(ols1, ols2, ols3, iv1, iv2, order = [\"price\"], drop = [\"(Intercept)\"], regression_statistics = [FStatIV, Nobs, R2],\n  labels = Dict(\n    \"price\" => \"Price\",\n    \"promotion\" => \"Promotion\",\n    \"ln_mkt_share_diff\" => \"Log Mkt share difference\"\n  ))\n## Some R codes that I followed\n\n# m1 <- lm(ln_mkt_share_diff ~ price + promotion , data = otc)\n# m2 <- lm(ln_mkt_share_diff ~ price + promotion + factor(product), data = otc)\n# m3 <- lm(ln_mkt_share_diff ~ price + promotion + factor(product) + factor(store), data = otc)\n# m4 <- ivreg(ln_mkt_share_diff ~ price + promotion | . - price + cost, data = otc)\n# m5 <- ivreg(ln_mkt_share_diff ~ price + promotion + factor(product) | . - price + cost, data = otc)\n# stargazer(m1, m2, m3, m4, m5, \n#           omit = c(\"product\", \"store\"),\n#           type = \"text\")\n\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\n\n-----------------------------------------------------------------------------\n                                        Log Mkt share difference             \n                          ---------------------------------------------------\n                              (1)        (2)       (3)         (4)        (5)\n-----------------------------------------------------------------------------\nPrice                       0.020   -0.189**   -0.145*    0.069***      0.169\n                          (0.014)    (0.059)   (0.059)     (0.015)    (0.115)\nPromotion                   0.121     0.187*   0.201**       0.149   0.308***\n                          (0.093)    (0.074)   (0.073)     (0.093)    (0.082)\n-----------------------------------------------------------------------------\nproduct Fixed Effects                    Yes       Yes                    Yes\nstore Fixed Effects                                Yes                       \n-----------------------------------------------------------------------------\nEstimator                     OLS        OLS       OLS          IV         IV\n-----------------------------------------------------------------------------\nControls                      Yes                              Yes           \n-----------------------------------------------------------------------------\nFirst-stage F statistic                                  8,147.921    394.113\nN                           1,056      1,056     1,056       1,056      1,056\nR2                          0.003      0.440     0.456      -0.008      0.420\n-----------------------------------------------------------------------------\n\n```\n:::\n:::\n\n\n\n\n\n\n### Caveats\n\nBut we don't usually use this basic setup in IO. This is because the model is bit too simple to fully capture the reality. One of the well known problem is the **Independence of irrelevant alternatives (IIA)**. Basically what this means is that we don't get a realistic demand elasticities. If you want to know more about it, google the famouse ***Red bus, blue bus*** story. \n\n### Solutions?\n\nThere are some ways to alleviate this problem. One of them (which we will not discuss), is using nested logit. Basically we are defining certain group of products where IIA holds within the group but may not hold across the group. So for the case of red bus, blue bus, they would be in a same group.\n\nAnother way is to do enhance the random utility model into logit model with random coefficients. In essence, this is sort of introducing preference heterogeneity of consumers into the model. This is done by interacting consumer preferences with product characteristics. The nuisance with this case is that now closed-form expression for choice probability is not obtainable. We need to do some numerical computation.\n\n## Advanced: @blp (Random coefficients logit model)\n\nWe again start with the individual utility function. But now something is added (we will now also explicitly denote markets as $t$):\n\n$$\nu_{ijt} = x_{jt} \\beta_{it} + \\alpha p_{jt} + \\xi_{jt} + \\varepsilon_{ijt}\n$$\n\nThe difference is that slope coefficients can now vary across individuals $i$. For now, we will assume $\\beta_{it}^k = \\beta_0^k + \\sigma_{kt} v_{it}^k$. We now have $k$ which is the dimension of $\\beta$. $\\beta_0^k$ are fixed taste for characteristics $k$ and $v_{it}^k$ are random tastes that follow standard normal distribution.\n\nNow we can expand the model:\n\n\\begin{align}\n  u_{ijt} &= (x_{j1t}, \\ldots, x_{jKt}) \\cdot (\\beta_{0}^1 + \\sigma_1 v_{it}^1, \\ldots, \\beta_{0}^K + \\sigma_K v_{it}^K)^T + \\alpha p_{jt} + \\xi_{jt} + \\varepsilon_{ijt}\\\\\n  &= x_{jt}\\beta_{it} + \\sum_k x_{jkt} \\sigma_{k}v_{ikt} + \\alpha p_{jt} + \\xi_{jt} + \\varepsilon_{ijt}\\\\\n  &= \\underbrace{x_{jt}\\beta_{it} + \\alpha p_{jt} + \\xi_{jt}}_{\\delta_{jt}} + \\underbrace{\\sum_k x_{jkt} \\sigma_{k}v_{ikt}}_{\\mu_{ijt}} +  \\varepsilon_{ijt}.\n\n\\end{align}\n\nWe can easily see that this is just an extension of what we did for the basic random utility model. Indirect utility is made up of mean utility $\\delta_{jt}$ and random coefficient term $\\mu_{ijt} + \\varepsilon_{ijt}$.\n\nNow we will make some simplication. We will assume that characteristics dimension of individual is one: $K = 1$. Using this simplication, we can again use the assumption that idiosyncratic shock follows T1EV to get aggregate share:\n\n$$\ns_{jt} = \\int \\frac{\\exp(\\delta_{jt} + x_{jt} \\sigma_t v_{it})}{1 + \\sum_j \\exp(\\delta_{jt} + x_{jt} \\sigma_t v_{it})} f(v_i)dv_i\n$$\n\nThe integral has no analytical solution in the random coefficient model, so we\nneed to compute the integral by simulation. One way to do it is as follows:\n\n$$\n\\hat{s}_{jt} = \\frac{1}{ns} \\sum_{i=1}^{ns} \\frac{\\exp(\\delta_{jt} + x_{jt} \\sigma_t v_{it})}{1 + \\sum_j \\exp(\\delta_{jt} + x_{jt} \\sigma_t v_{it})}\n$$\n\nwhere $ns$ is number of random draws from $v_i$.\n\nNow we can see the inversion method we did before is not easy to implement. This is because we now have additional parameters that we do not know the values.\n\nSo in BLP, we need to do **nested estimation algorithm**.\n\n1. In the **outer loop**, we iterate over different values of the parameters.\n\n2. In the **inner loop**, for a given parameter value, we do the inversion to get the mean utility and estimate the GMM objective function.\n\n3. We keep on doing this iteration until we get the parameters that minimize the GMM function.\n\nNow let's do some coding!\n\n### Another coding (with `Julia`)\n\n\n\n\n::: {#6 .cell execution_count=1}\n``` {.julia .cell-code}\n###############################################################################\n####  BLP fixed-point algorithm, inverting mkt shares to get mean utility  ####\n###############################################################################\nusing CSV\nusing DataFrames\nusing GLM\nusing Statistics\nusing LinearAlgebra\nusing Distributions\nusing NLopt\notc = CSV.read(\"data/otc.csv\", DataFrame)\n\nns = 500;\nnmkt = maximum(otc.mkt);\nmkt = unique(otc.mkt);\nnprod = maximum(otc.product);\n\nvi = quantile.(Normal(), collect(range(0.5/ns, step = 1/ns, length = ns)));\nsigma = 1;\n\nfunction calc_mkt_share_t(delta_t, sigma_t, x_t, vi_t)\n    # Dimension: delta_t 11*1, simga_t 1*1, x_t 11*1\n    delta_t = delta_t .* ones(nprod, ns)\n    mu_t = x_t*sigma_t*vi_t'\n    numerator = exp.(delta_t .+ mu_t)\n    denominator = ones(nprod, ns) .+ sum(numerator, dims = 1)\n    mkt_share_t = mean(numerator./denominator, dims = 2)\nend\n\nfunction contraction_t(d0, sigma_t, x_t, vi_t, mkt_t, tol = 1e-5, maxiter = 1e5)\n    obs_mkt_share_t = mkt_t.mkt_share\n    d_old = d0\n    normdiff = Inf\n    iter = 0\n    while normdiff > tol && iter <= maxiter\n        model_mkt_share_t = calc_mkt_share_t(d_old, sigma_t, x_t, vi_t)\n        d_new = d_old .+ log.(obs_mkt_share_t) .- log.(model_mkt_share_t)\n        normdiff = maximum(norm.(d_new .- d_old))\n        d_old = d_new\n        iter += 1\n    end\n    return d_old\nend\n\nfunction calc_delta(sigma)\n    delta_fp = zeros(nprod, nmkt);\n    for t in mkt\n        mkt_t = otc[otc.mkt .== t, :];\n        x_t = ones(nprod, 1);\n        delta_t = zeros(nprod, 1);\n        sigma_t = sigma;\n        vi_t = vi;\n        delta_fp[:, t] = contraction_t(delta_t, sigma_t, x_t, vi_t, mkt_t);\n    end\n    return vec(delta_fp);\nend\n\n@time delta_fp = calc_delta(sigma);\nmean(delta_fp)\nstd(delta_fp)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  1.516659 seconds (3.74 M allocations: 364.910 MiB, 2.94% gc time, 95.76% compilation time)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\n0.8537059827877238\n```\n:::\n:::\n\n\n\n::: {#8 .cell execution_count=1}\n``` {.julia .cell-code}\n################################################################\n#### Estimate beta and sigma using GMM (cost as instrument) ####\n################################################################\nX = hcat(ones(nprod*nmkt, 1),\n         otc.price, otc.promotion,\n         otc.product_2, otc.product_3, otc.product_4, otc.product_5,\n         otc.product_6, otc.product_7, otc.product_8, otc.product_9,\n         otc.product_10, otc.product_11);\nz = hcat(X, otc.cost);\nPhi = z'*z/1056;\ninv_Phi = inv(Phi);\n\nfunction GMMObjFunc(theta2::Vector, grad::Vector)\n    sigma = theta2[1]\n    delta = calc_delta(sigma)\n    theta1 = inv(X'*z*inv_Phi*z'*X)*X'*z*inv_Phi*z'*delta\n    error = delta - X*theta1\n    obj = error'*z*inv_Phi*z'*error\n    return obj\nend\n\nopt = Opt(:LN_COBYLA, 1)\nopt.xtol_rel = 1e-4\nopt.lower_bounds = [0.00001]\nopt.min_objective = GMMObjFunc\n@time (minf,minx,ret) = optimize(opt, [1])\n\n@show sigma = minx[1]\ndelta = calc_delta(sigma[1]);\ntheta1 = inv(X'*z*inv_Phi*z'*X)*X'*z*inv_Phi*z'*delta\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n 21.752781 seconds (20.18 M allocations: 109.865 GiB, 15.53% gc time, 0.68% compilation time)\nsigma = minx[1] = 28.720209709193362\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\n13-element Vector{Float64}:\n -71.52085329511755\n  -1.0852292863458644\n   0.22206483339059624\n   1.903952993172239\n   3.990433860485858\n  -0.6194995526125144\n   1.2498418962356002\n   3.9120756908464607\n  -2.12500554825884\n  -1.1737916667530266\n   0.2462099284926299\n  -2.7352849052497947\n   0.011924728064080292\n```\n:::\n:::\n\n\n\n\n\n\n## References {.unnumbered}\n\n::: {#refs}\n:::\n\n",
    "supporting": [
      "blp_files"
    ],
    "filters": [],
    "includes": {}
  }
}