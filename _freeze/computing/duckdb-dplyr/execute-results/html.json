{
  "hash": "62b1be5fa9729d9f72d0dc95a03a8776",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: DuckDB + dplyr (R)\nsubtitle: Use a familiar R frontend\nexecute:\n  freeze: auto\n  cache: true\n  eval: false\n---\n\n\n\n\n\nThis is just a direct copy of the resources from [Grant McDermott](https://grantmcdermott.com/duckdb-polars/duckdb-dplyr.html). Thus, I do not have any credit for it. It is solely for the archive purpose.\n\n## Load libraries\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(duckdb)\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(tidyr, warn.conflicts = FALSE)\n```\n:::\n\n\n\n\n\n## Create a database connection\n\nFor the `d(b)plyr` workflow, the connection step is very similar to the pure SQL\napproach. The only difference is that, after instantiating the database\nconnection, we need to register our parquet dataset as a table in our connection\nvia the `dplyr::tbl()` function. Note that we also assign it to an object (here:\n`nyc`) that can be referenced from R.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Instantiate the in-memory DuckDB connection \ncon = dbConnect(duckdb(), shutdown = TRUE)\n\n## Register our parquet dataset as table in our connection (and that assign it\n## to an object that R understands)\n# nyc = tbl(con, \"nyc-taxi/**/*.parquet\") # works, but safer to use the read_parquet func)\nnyc = tbl(con, \"read_parquet('nyc-taxi/**/*.parquet', hive_partitioning = true)\")\n```\n:::\n\n\n\n\n\n## First example\n\nThis next command runs instantly because all computation is deferred (i.e.,\nlazy eval). In other words, it is just a query object.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nq1 = nyc |>\n  summarize(\n    mean_tip = mean(tip_amount),\n    .by = passenger_count\n  )\n```\n:::\n\n\n\n\n\n:::{.callout-tip}\n## `.by` versus `group_by` \nIn case you weren't aware: `summarize(..., .by = x)` is a shorthand (and\nnon-persistent) version of `group_by(x) |> summarize(...)`. More details\n[here](https://www.tidyverse.org/blog/2023/02/dplyr-1-1-0-per-operation-grouping/).\n:::\n\nWe can see what DuckDB's query tree looks like by asking it to explain\nthe plan\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexplain(q1)\n```\n:::\n\n\n\n\n\nSimilarly, to show the SQL translation that will be implemented on the backend,\nusing `show_query`.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_query(q1)\n```\n:::\n\n\n\n\n\nNote that printing the query object actually does enforce some computation.\nOTOH it's still just a preview of the data (we haven't pulled everything into\nR's memory).\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nq1\n```\n:::\n\n\n\n\n\nTo actually pull all of the result data into R, we must call `collect()`\non the query object\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic = Sys.time()\ndat1 = collect(q1)\ntoc = Sys.time()\n\ndat1\ntoc - tic\n```\n:::\n\n\n\n\n\n\n\n## Aggregation\n\nHere's our earlier filtering example with multiple grouping + aggregation\nvariables...\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nq2 = nyc |>\n  filter(month <= 3) |>\n  summarize(\n    across(c(tip_amount, fare_amount), mean),\n    .by = c(month, passenger_count)\n  )\nq2\n```\n:::\n\n\n\n\n\nAside: note the optimised query includes hash groupings and projection\n(basically: fancy column subsetting, which is a suprisingly effective strategy\nin query optimization)\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexplain(q2)\n```\n:::\n\n\n\n\n\nAnd our high-dimensional aggregation example. We'll create a query for this\nfirst, since I'll reuse it shortly again\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nq3 = nyc |>\n  group_by(passenger_count, trip_distance) |>\n  summarize(\n    across(c(tip_amount, fare_amount), mean),\n  ) \ncollect(q3)\n```\n:::\n\n\n\n\n\n## Pivot (reshape)\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# library(tidyr) ## already loaded\n\nq3 |>\n  pivot_longer(tip_amount:fare_amount) |>\n  collect()\n```\n:::\n\n\n\n\n\n## Joins (merges)\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean_tips  = nyc |> summarise(mean_tips = mean(tip_amount), .by = month)\nmean_fares = nyc |> summarise(mean_fares = mean(fare_amount), .by = month)\n```\n:::\n\n\n\n\n\nAgain, these commands complete instantly because all computation has been\ndeferred until absolutely necessary (i.e.,. lazy eval).\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nleft_join(\n  mean_fares,\n  mean_tips\n  ) |>\n  collect()\n```\n:::\n\n\n\n\n\n## Windowing\n\nIf you recall from the native SQL API, we sampled 1 percent of the data before\ncreating decile bins to reduce the computation burden of sorting the entire\ntable. Unfortunately, this approach doesn't work as well for the **dplyr**\nfrontend because the underlying SQL translation\n[uses](https://dbplyr.tidyverse.org/reference/dbplyr-slice.html) a generic\nsampling approach (rather than DuckDB's optimised `USING SAMPLE` statement.)\n\n## Close connection\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbDisconnect(con)\n```\n:::\n\n\n\n\n\n## Appendix: Related interfaces\n\n### arrow+duckdb\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(arrow)\nlibrary(duckdb) ## Already loaded\nlibrary(dplyr)  ## Already loaded\nlibrary(tidyr)  ## Already loaded\n```\n:::\n\n\n\n\n\nWhen going through the **arrow** intermediary, we don't need to establish a\ndatabase with `DBI::dbConnect` like we did above. Instead, we can create a link\n(pointers) to the dataset on disk directly via the `arrow::open_dataset()`\nconvience function. Here I'll assign it to a new R object called `nyc2`.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnyc2 = open_dataset(\"nyc-taxi\")\n```\n:::\n\n\n\n\n\n:::{.callout-tip}\n## open_dataset() versus read_parquet()\n(For individual parquet files, we could just read then via\n`arrow::read_parquet()`, perhaps efficiently subsetting columns at the same\ntime. But I find the `open_dataset` is generally what I'm looking for.)\n:::\n\nNote that printing our `nyc2` dataset to the R console will just display the\ndata schema. This is a cheap and convenient way to quickly interrogate the basic\nstructure of your data, including column types, etc.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnyc2\n```\n:::\n\n\n\n\n\nThe key step for this \"arrow + duckdb\" **dplyr** workflow is to pass our arrow\ndataset to DuckDB via the `to_duckdb()` function.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nto_duckdb(nyc2)\n```\n:::\n\n\n\n\n\nNote that this transfer from Arrow to DuckDB is very quick (and memory cheap)\nbecause it is a zero copy. We are just passing around pointers instead of\nactually moving any data. See\n[this blog post](https://duckdb.org/2021/12/03/duck-arrow.html)\nfor more details, but the high-level take away is that we are benefitting from\nthe tightly integrated architectures of these two libraries.^[\"Similar\" might be\na better description than \"integrated\", since DuckdB does not use the Arrow\nmemory model. But they are both columnar-orientated (among other things) and so\nthe end result is pretty seamless integration.]\n\nAt this, point all of the regular **dplyr** workflow logic from above should\ncarry over. Just remember to first pass the arrow dataset via the `to_duckdb()`\nfunciton. For example, here's our initial aggregation query again:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnyc2 |>\n  to_duckdb() |> ## <= key step\n  summarise(\n    mean_tip = mean(tip_amount),\n    .by = passenger_count\n  ) |>\n  collect()\n```\n:::\n\n\n\n\n\n:::{.callout-note}\n## Arrow's native acero engine\n\nSome of you may be used to performing computation with the **arrow** package\nwithout going through DuckDB. What's happening here is that arrow provides its\nown computation engine called \"acero\". This Arrow-native engine is actually\npretty performant... albeit not a fast as DuckDB, nor as feature rich. So I\npersonally recommend always passing to DuckDB if you can. Still, if you're\ncurious then you can test yourself by re-trying the code chunk, but commenting\nout the `to_duckdb()` line. For more details, see\n[here](https://youtu.be/LvTX1ZAZy6M?si=7gZYG03ojtAtPGfe).\n:::\n\n### duckplyr\n\nThe new kid on the block is **duckplyr**\n([announcement](https://duckdb.org/2024/04/02/duckplyr.html) /\n[homepage](https://duckdblabs.github.io/duckplyr/)).\nWithout going into too much depth, the promise of **duckplyr** is that it can\nprovide a \"fully native\" dplyr experience that is _directly_ coupled to DuckDB's\nquery engine. So, for example, it won't have to rely on **DBI**'s generic' SQL\ntranslations. Instead, the relevant **dplyr** \"verbs\" are being directly\ntranslated to DuckDB's relational API to construct logical query plans. If\nthat's too much jargon, just know that it should involve less overhead, fewer\ntranslation errors, and better optimization. Moreover, a goal of **duckplyr** is\nfor it to be a drop-in replace for **dplyr** _in general_. In other words, you\ncould just swap out `library(dplyr)` for `library(duckplyr)` and all of your\ndata wrangling operations will come backed by the power of DuckDB. This includes\nfor working on \"regular\" R data frames in memory.\n\nAll of this is exciting and I would urge you stay tuned. Right now, **duckplyr**\nis still marked as experimental and has a few rough edges. But the basics are\nthere. For example:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(duckplyr, warn.conflicts = FALSE)\n\nduckplyr_df_from_parquet(\"nyc-taxi/**/*.parquet\") |>\n  summarise(\n    mean_tip = mean(tip_amount),\n    .by = passenger_count\n  )\n```\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}